[{"content":"前言 相信大家遇到过掉网卡驱动的情况，情况就是网络选项都没有了，WIFI图标都没有了\n救急 既然网络选项没有了，那我们要是急需上网怎么办，可以用数据线连接手机和笔记本,在手机设置里打开个人热点，打开USB共享这样你的笔记本就可以上网了。不过这个方法只适合临时，毕竟每次这样很麻烦。\n重启大法： 无脑重启，偶尔可以解决问题。 重启大法好啊！！！\n驱动重启或重装 打开设备管理器,查看网络设配器，图中我选中的就是无线网卡驱动，如果有个黄色感叹号就是驱动没有正常运行。 右键对应的驱动 可以先更新驱动程序试试 或者直接禁用设备，然后启用设备 如果都不行卸载设备 然后去对应电脑官网下载一个对应的无线网卡驱动 然后重启 注册表 键盘按win+r，弹出运行窗口 输入“redegit”，进入注册表， 删除HKEY_CLASSES_ROOT\\CLSID{3d09c1ca-2bcc-40b7-b9bb-3f3ec143a87b}，然后网卡禁用再启用。 硬件问题 掉网卡有多种可能，有可能是一些其他驱动冲突。有时出现掉网卡的状况可能是因为温度过高，硬件的自我保护机制，强制停止运行。这种情况很有可能是我们的散热系统里面进了灰尘，导致散热系统不能完全发挥性能。也有可能是网卡实体位置不对等等。。。\n如果出现这些问题去线下店看看喽\n线下店的老板告诉我，2022款的拯救者y7000p有这通病，可能是散热不良导致的~~\n","date":"2024-10-16T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/%E8%81%94%E6%83%B3%E6%8E%89%E7%BD%91%E5%8D%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/3168604b7ecc090e38e308737e9e2427_hu4565555821577788711.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/%E8%81%94%E6%83%B3%E6%8E%89%E7%BD%91%E5%8D%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"联想掉网卡解决方案"},{"content":"高级特性 迭代 只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代\n那么，如何判断一个对象是可迭代对象呢？方法是通过collections.abc模块的Iterable类型判断：\n1 2 from collections.abc import Iterable isinstance(\u0026#39;abc\u0026#39;,Iterable) Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身：\n1 2 for i,value in enumerate([\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;]): print(i,value) 列表生成式 1 2 [ x * x for x in range(1,11)] \u0026gt; [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 1 2 [ x * x for x in range(1,11) if x%2 == 0] \u0026gt; [4, 16, 36, 64, 100] 1 2 [ m + b for m in \u0026#39;ABC\u0026#39; for n in \u0026#39;XYZ\u0026#39;] \u0026gt; [\u0026#39;AX\u0026#39;, \u0026#39;AY\u0026#39;, \u0026#39;AZ\u0026#39;, \u0026#39;BX\u0026#39;, \u0026#39;BY\u0026#39;, \u0026#39;BZ\u0026#39;, \u0026#39;CX\u0026#39;, \u0026#39;CY\u0026#39;, \u0026#39;CZ\u0026#39;] 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现：\n1 2 3 import os [d for d in os.listdir(\u0026#39;.\u0026#39;)] #os.listdir可以列出文件和目录 \u0026gt; [\u0026#39;.emacs.d\u0026#39;, \u0026#39;.ssh\u0026#39;, \u0026#39;.Trash\u0026#39;, \u0026#39;Adlm\u0026#39;, \u0026#39;Applications\u0026#39;, \u0026#39;Desktop\u0026#39;, \u0026#39;Documents\u0026#39;, \u0026#39;Downloads\u0026#39;, \u0026#39;Library\u0026#39;, \u0026#39;Movies\u0026#39;, \u0026#39;Music\u0026#39;, \u0026#39;Pictures\u0026#39;, \u0026#39;Public\u0026#39;, \u0026#39;VirtualBox VMs\u0026#39;, \u0026#39;Workspace\u0026#39;, \u0026#39;XCode\u0026#39;] 最后把一个list中所有的字符串变成小写：\n1 2 3 L = [\u0026#39;Hello\u0026#39;, \u0026#39;World\u0026#39;, \u0026#39;IBM\u0026#39;, \u0026#39;Apple\u0026#39;] [s.lower() for s in L] \u0026gt; [\u0026#39;hello\u0026#39;, \u0026#39;world\u0026#39;, \u0026#39;ibm\u0026#39;, \u0026#39;apple\u0026#39;] 1 2 [x if x % 2 == 0 else -x for x in range(1,11)] \u0026gt; [-1, 2, -3, 4, -5, 6, -7, 8, -9, 10] 可见，在一个列表生成式中，for前面的if \u0026hellip; else是表达式，而for后面的if是过滤条件，不能带else。\n生成器 如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。\n要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator：\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; L = [x * x for x in range(10)] \u0026gt;\u0026gt;\u0026gt; L [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] \u0026gt;\u0026gt;\u0026gt; g = (x * x for x in range(10)) \u0026gt;\u0026gt;\u0026gt; g \u0026lt;generator object \u0026lt;genexpr\u0026gt; at 0x1022ef630\u0026gt; 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值\n当然，上面这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象：\n1 2 3 g = (x * x for x in range(10)) for n in g: print(n) 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易：\n1 2 3 4 5 6 7 def fib(max): n,a,b = 0,0,1 while n \u0026lt; max: print(b) a,b = b,a+b n = n + 1 return \u0026#39;done\u0026#39; 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator函数，只需要把print(b)改为yield b就可以了：\n1 2 3 4 5 6 7 def fib(max): n,a,b = 0,0,1 while n \u0026lt; max: yield b a,b = b,a+b n = n+1 return \u0026#39;done\u0026#39; 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator函数，调用一个generator函数将返回一个generator：\n1 2 3 \u0026gt;\u0026gt;\u0026gt; f = fib(6) \u0026gt;\u0026gt;\u0026gt; f \u0026lt;generator object fib at 0x104feaaa0\u0026gt; 这里，最难理解的就是generator函数和普通函数的执行流程不一样。普通函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 举个简单的例子，定义一个generator函数，依次返回数字1，3，5：\n1 2 3 4 5 6 7 def odd(): print(\u0026#39;step 1\u0026#39;) yield 1 print(\u0026#39;step 2\u0026#39;) yield(3) print(\u0026#39;step 3\u0026#39;) yield(5) 调用该generator函数时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;\u0026gt;\u0026gt; o = odd() \u0026gt;\u0026gt;\u0026gt; next(o) step 1 1 \u0026gt;\u0026gt;\u0026gt; next(o) step 2 3 \u0026gt;\u0026gt;\u0026gt; next(o) step 3 5 \u0026gt;\u0026gt;\u0026gt; next(o) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration 可以看到，odd不是普通函数，而是generator函数，在执行过程中，遇到yield就中断，下次又继续执行。执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。\n请务必注意：调用generator函数会创建一个generator对象，多次调用generator函数会创建多个相互独立的generator。\n迭代器 我们已经知道，可以直接作用于for循环的数据类型有以下几种：\n一类是集合数据类型，如list、tuple、dict、set、str等；\n一类是generator，包括生成器和带yield的generator function。\n这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。\n可以使用isinstance()判断一个对象是否是Iterable对象：\n可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator\n生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。\n把list、dict、str等Iterable变成Iterator可以使用iter()函数：\n1 2 3 isinstance(iter([]),Iterator) isinstance(iter(\u0026#39;abc\u0026#39;),Iterator) 这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。\nIterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。\n凡是可作用于for循环的对象都是Iterable类型；\n凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列；\n集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。\nPython的for循环本质上就是通过不断调用next()函数实现的\n函数式编程 函数是Python内建支持的一种封装，我们通过把大段代码拆成函数，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。\n函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。\n函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！\nPython对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。\n高阶函数 变量可以指向函数 abs(-10)是函数调用，而abs是函数本身。\n要获得函数调用结果，我们可以把结果赋值给变量：\n1 2 3 \u0026gt;\u0026gt;\u0026gt; x = abs(-10) \u0026gt;\u0026gt;\u0026gt; x 10 但是，如果把函数本身赋值给变量呢？\n1 2 3 f = abs f \u0026lt;built-in function abs\u0026gt; 函数本身也可以赋值给变量，即：变量可以指向函数。\n函数名也是变量 函数名其实就是指向函数的变量！\n注：由于abs函数实际上是定义在import builtins模块中的，所以要让修改abs变量的指向在其它模块也生效，要用import builtins; builtins.abs = 10。\n传入函数 既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。\nmap/reduce map()函数接收两个参数，一个是函数，一个是Iterable，map将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回。\n1 2 3 4 5 6 def f(x): return x*x r = map(f,[1,2,3,4,5,6,7,8,9]) list(r) [1, 4, 9, 16, 25, 36, 49, 64, 81] reduce把一个函数作用在一个序列[x1, x2, x3, \u0026hellip;]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是： 比方说对一个序列求和，就可以用reduce实现：\n再看reduce的用法。reduce把一个函数作用在一个序列[x1, x2, x3, \u0026hellip;]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，其效果就是：\nreduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)\n1 2 3 4 5 6 from functools import reduce def add(x,y): return x + y reduce(add,[1,3,5,7,9]) \u0026gt; 25 这个例子本身没多大用处，但是，如果考虑到字符串str也是一个序列，对上面的例子稍加改动，配合map()，我们就可以写出把str转换为int的函数\n1 2 3 4 5 6 7 8 9 from functools import reduce def fn(x,y): return x*10+y def char2num(s): digits = {\u0026#39;0\u0026#39;: 0, \u0026#39;1\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;3\u0026#39;: 3, \u0026#39;4\u0026#39;: 4, \u0026#39;5\u0026#39;: 5, \u0026#39;6\u0026#39;: 6, \u0026#39;7\u0026#39;: 7, \u0026#39;8\u0026#39;: 8, \u0026#39;9\u0026#39;: 9} return digits[s] reduce(fn,map(char2sum,\u0026#39;13579\u0026#39;)) 整理成一个str2int的函数就是：\n1 2 3 4 5 6 7 8 from functools import reduce DIGITS = {\u0026#39;0\u0026#39;: 0, \u0026#39;1\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;3\u0026#39;: 3, \u0026#39;4\u0026#39;: 4, \u0026#39;5\u0026#39;: 5, \u0026#39;6\u0026#39;: 6, \u0026#39;7\u0026#39;: 7, \u0026#39;8\u0026#39;: 8, \u0026#39;9\u0026#39;: 9} def str2int(s): def fn(s,y): return x*10+y def char2num(s): return DIGITS[s] return reduce(fn,map(char2num,s)) 还可以用lambda函数进一步简化成\n1 2 3 4 5 6 7 8 9 from functools import reduce DIGITS = {\u0026#39;0\u0026#39;: 0, \u0026#39;1\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;3\u0026#39;: 3, \u0026#39;4\u0026#39;: 4, \u0026#39;5\u0026#39;: 5, \u0026#39;6\u0026#39;: 6, \u0026#39;7\u0026#39;: 7, \u0026#39;8\u0026#39;: 8, \u0026#39;9\u0026#39;: 9} def char2num(s): return DIGITS[s] def str2int(s): return reduce(lambda x,y:x * 10 + y,map(char2num,s)) map用于将一个函数作用于一个序列，以此得到另一个序列；\nreduce用于将一个函数依次作用于上次计算的结果和序列的下一个元素，以此得到最终结果。\nfilter Python内建的filter()函数用于过滤序列。\n和map()类似，filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。\n把一个序列中的空字符串删掉，可以这么写：\n1 2 3 4 5 def not_empty(s): return s and s.strip() list(filter(not_empty,[\u0026#39;A\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;B\u0026#39;, None, \u0026#39;C\u0026#39;, \u0026#39; \u0026#39;])) \u0026gt; 结果: [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;] 注意到filter()函数返回的是一个Iterator，也就是一个惰性序列，所以要强迫filter()完成计算结果，需要用list()函数获得所有结果并返回list。\nfilter()的作用是从一个序列中筛出符合条件的元素。由于filter()使用了惰性计算，所以只有在取filter()结果的时候，才会真正筛选并每次返回下一个筛出的元素。\nsorted sorted()函数也是一个高阶函数，它还可以接收一个key函数来实现自定义的排序，例如按绝对值大小排序：\nsorted([36,5,-12,9,-21],key=abs) [5, 9, -12, -21, 36]\nkey指定的函数将作用于list的每一个元素上，并根据key函数返回的结果进行排序。对比原始的list和经过key=abs处理过的list：\n返回函数 高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。\n我们来实现一个可变参数的求和。通常情况下，求和的函数是这样定义的：\n1 2 3 4 5 def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数：\n1 2 3 4 5 6 7 def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum 当我们调用lazy_sum()时，返回的并不是求和结果，而是求和函数：\n1 2 3 \u0026gt;\u0026gt;\u0026gt; f = lazy_sum(1, 3, 5, 7, 9) \u0026gt;\u0026gt;\u0026gt; f \u0026lt;function lazy_sum.\u0026lt;locals\u0026gt;.sum at 0x101c6ed90\u0026gt; 调用函数f时，才真正计算求和的结果：\nf() 25\n在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。\n请再注意一点，当我们调用lazy_sum()时，每次调用都会返回一个新的函数，即使传入相同的参数：\n闭包 注意到返回的函数在其定义内部引用了局部变量args，所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。\n另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了f()才执行。我们来看一个例子：\n1 2 3 4 5 6 7 8 9 def count(): fs = [] for i in range(1,4): def f(): return i*i fs.append(f) return fs f1,f2,f3 = count() 1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; f1() 9 \u0026gt;\u0026gt;\u0026gt; f2() 9 \u0026gt;\u0026gt;\u0026gt; f3() 9 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量\n如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变：\n1 2 3 4 5 6 7 8 9 def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1,4): fs.append(f(i)) #f(i)立刻被执行，因此i的当前值被传入f() return fs f1, f2, f3 = count() f1() 1 f2() 4 f3() 9\nnonlocal 使用闭包，就是内层函数引用了外层函数的局部变量。如果只是读外层变量的值，我们会发现返回的闭包函数调用一切正常：\n1 2 3 4 5 6 7 8 9 10 def inc(): x = 0 def fn(): # 仅读取x的值 return x + 1 return f = inc() print(f()) print(f()) 但是，如果对外层变量赋值，由于Python解释器会把x当作函数fn()的局部变量，它会报错：\n1 2 3 4 5 6 7 8 9 10 11 def inc(): x = 0 def fn(): # nonlocal x x = x+1 return x return fn f = inc() print(f()) # 1 print(f()) # 2 原因是x作为局部变量并没有初始化，直接计算x+1是不行的。但我们其实是想引用inc()函数内部的x，所以需要在fn()函数内部加一个nonlocal x的声明。加上这个声明后，解释器把fn()的x看作外层函数的局部变量，它已经被初始化了，可以正确计算x+1。\n使用闭包时，对外层变量赋值前，需要先使用nonlocal声明该变量不是当前函数的局部变量。\n一个函数可以返回一个计算结果，也可以返回一个函数。\n返回一个函数时，牢记该函数并未执行，返回函数中不要引用任何可能会变化的变量。\n总结来说，闭包使得 counter 函数能够记住它定义时所在的作用域内的变量 x 的内存位置。当 createCounter 执行完毕后，虽然它的栈帧被销毁了，但是 x 的内存位置仍然被 counter 记住，因此 counter 可以继续访问并修改 x 的值。这种机制保证了 x 的状态能够在多次调用 counter 之间保持一致。\n匿名函数 1 2 list(map(lamdba x:x*x,[1, 2, 3, 4, 5, 6, 7, 8, 9])) \u0026gt; [1, 4, 9, 16, 25, 36, 49, 64, 81] 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数：\n装饰器 由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。\n函数对象有一个__name__属性（注意：是前后各两个下划线），可以拿到函数的名字：\n现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。\n本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下：\n1 2 3 4 5 def log(func): def wrapper(*args,**kw): print(\u0026#39;call %s():\u0026#39; % func.__name__) return func(*args,**kw) return wrapper 把@log放到now()函数的定义处，相当于执行了语句：\nnow = log(now)\n如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本\n1 2 3 4 5 6 7 def log(text): def decorator(func): def wrapper(*args,**kw): print(\u0026#39;%s %s():\u0026#39; %(text,func.__name__)) return fun(*args,**kw) return wrapper return decorator 这个3层嵌套的decorator用法如下：\n1 2 3 @log(\u0026#39;execute\u0026#39;) def now(): print(\u0026#34;2024-6-1\u0026#34;) now() execute now(): 2024-6-1\n和两层嵌套的decorator相比，3层嵌套的效果是这样的\nnow = log(\u0026rsquo;execute\u0026rsquo;)(now)\n我们来剖析上面的语句，首先执行log(\u0026rsquo;execute\u0026rsquo;)，返回的是decorator函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数\n以上两种decorator的定义都没有问题，但还差最后一步。因为我们讲了函数也是对象，它有__name__等属性，但你去看经过decorator装饰之后的函数，它们的__name__已经从原来的\u0026rsquo;now\u0026rsquo;变成了\u0026rsquo;wrapper\u0026rsquo;：\n因为返回的那个wrapper()函数名字就是\u0026rsquo;wrapper\u0026rsquo;，所以，需要把原始函数的__name__等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 不需要编写wrapper.name = func.__name__这样的代码，Python内置的functools.wraps就是干这个事的，所以，一个完整的decorator的写法如下：\n1 2 3 4 5 6 7 8 import functools def long(func): @functools.wraps(func) def wrapper(*args,**kw): print(\u0026#39;call %s():\u0026#39;%func.__name__) return func(*args,**kw) return wrapper 或者针对带参数的decorator：\n1 2 3 4 5 6 7 8 9 10 import functools def log(text): def decorator(func): @functools.wraps(func) def wrapper(*args,**kw): print(\u0026#39;%s %s():\u0026#39;%(text,func.__name__)) return func(*args,**kw) return wrapper return decorator 在面向对象（OOP）的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。\n偏函数 但int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换：\nint(\u0026lsquo;12345\u0026rsquo;,base=8)\nfunctools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2：\n1 2 3 import functools int2 = functools.partial(int,base=2) int2(\u0026#39;2100000\u0026#39;) 所以，简单总结functools.partial的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。\n最后，创建偏函数时，实际上可以接收函数对象、*args和**kw这3个参数，当传入：\n1 int2 = functools.partial(int,base=2) 实际上固定了int()函数的关键字参数base，也就是： int2(\u0026lsquo;10010\u0026rsquo;) 相当于：\nkw = { \u0026lsquo;base\u0026rsquo;: 2 } int(\u0026lsquo;10010\u0026rsquo;, **kw)\n当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。\n模块 我们以内建的sys模块为例，编写一个hello的模块：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # !/usr/bin/env python3 # -*- coding:utf-8 -*- \u0026#39;a test module\u0026#39; __author__ = \u0026#39;Crow\u0026#39; import sys def test(): args = sys.argv if len(args) == 1: print(Hello, world!) if len(args) == 2: print(\u0026#39;Hello,%s!\u0026#39;% args[1]) else: print(\u0026#39;Too many arguments!\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: test() 导入sys模块后，我们就有了变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。\nsys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，例如：\n运行python3 hello.py获得的sys.argv就是[\u0026lsquo;hello.py\u0026rsquo;]；\n运行python3 hello.py Michael获得的sys.argv就是[\u0026lsquo;hello.py\u0026rsquo;, \u0026lsquo;Michael\u0026rsquo;]\n使用模块 作用域 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等；\n类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名；\n类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等；\n外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public\n默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中：\n如果我们要添加自己的搜索目录，有两种方法：\n一是直接修改sys.path，添加要搜索的目录：\nimport sys sys.path.append(\u0026rsquo;/Users/michael/my_py_scripts\u0026rsquo;) 这种方法是在运行时修改，运行结束后失效。\n第二种方法是设置环境变量PYTHONPATH，该环境变量的内容会被自动添加到模块搜索路径中。设置方式与设置Path环境变量类似。注意只需要添加我们自己的搜索路径，Python本身的搜索路径不受影响。\n面向对象编程 OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数\n类和实例 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。\n有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去：\n访问实例 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。\n有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去：\n有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。\n双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量：\n但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。\n总的来说就是，Python本身没有任何机制阻止你干坏事，一切全靠自觉。\n表面上看，外部代码“成功”地设置了__name变量，但实际上这个__name变量和class内部的__name变量不是一个变量！内部的__name变量已经被Python解释器自动改成了_Student__name，而外部代码给bart新增了一个__name变量。不信试试：\n继承和多态 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。\n要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样：\n多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思：\n静态语言vs动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。\n对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了：\n这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。\n动态语言的鸭子类型特点决定了继承不像静态语言那样是必须的。\n获取对象信息 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法：\n仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态：\n一个正确的用法的例子如下：\n1 2 3 4 def readImage(fp): if hasattr(fp,\u0026#39;read\u0026#39;): return readData(fp) return None 假设我们希望从文件流fp中读取图像，我们首先要判断该fp对象是否存在read方法，如果存在，则该对象是一个流，如果不存在，则无法读取。hasattr()就派上了用场。\n请注意，在Python这类动态语言中，根据鸭子类型，有read()方法，不代表该fp对象就是一个文件流，它也可能是网络流，也可能是内存中的一个字节流，但只要read()方法返回的是有效的图像数据，就不影响读取图像的功能。\n实例属性和类属性 由于Python是动态语言，根据类创建的实例可以任意绑定属性。\n给实例绑定属性的方法是通过实例变量，或者通过self变量：\n1 2 3 4 5 6 class Student(object): def __init__(self,name): self.name = name s = Student(\u0026#39;Bob\u0026#39;) s.score = 90 但是，如果Student类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归Student类所有：\n1 2 class Student(object): name = \u0026#39;Student\u0026#39; 实例属性属于各个实例所有，互不干扰；\n类属性属于类所有，所有实例共享一个属性；\n不要对实例属性和类属性使用相同的名字，否则将产生难以发现的错误。\n面向对象高级编程 使用__slot__ 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class实例能添加的属性：\n1 2 class Student(object): __slot__ = (\u0026#39;name\u0026#39;,\u0026#39;age\u0026#39;) # 用tuple定义允许绑定的属性名称 使用__slots__要注意，slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： 除非在子类中也定义__slots，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。\n使用@property 还记得装饰器（decorator）可以给函数动态加上功能吗？对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的：\n1 2 3 4 5 6 7 8 9 10 11 12 class Student(object): @property def score(self): return self._score @score.setter def score(self,value): if not isinstance(value,int): raise ValueError(\u0026#39;score must be an integer!\u0026#39;) if value \u0026lt; 0 or value \u0026gt; 100: raise ValueError(\u0026#39;score must between 0 ~ 100!\u0026#39;) self._score = value @property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作：\n注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。\n还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性：\n1 2 3 4 5 6 7 8 9 10 11 12 class Student(object): @propery def birth(self): return self._birth @birth.setter def birth(self,value): self._birth = value @propery def age(self): return 2015 - self.birth 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。\n要特别注意：属性的方法名不要和实例变量重名。\n属性方法名和实例变量重名，会造成递归调用，导致栈溢出报错！\n多重继承 MixIn 在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。\n为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn，让某个动物同时拥有好几个MixIn：\n1 2 class Dog(Mammal,RunnableMixIn,CarnivorousMixIn): pass MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。\nPython自带的很多库也使用了MixIn。举个例子，Python自带了TCPServer和UDPServer这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由ForkingMixIn和ThreadingMixIn提供。通过组合，我们就可以创造出合适的服务来。\n比如，编写一个多进程模式的TCP服务，定义如下：\n1 2 class MyTCPServer(TCPServer, ForkingMixIn): pass 编写一个多线程模式的UDP服务，定义如下：\n1 2 class MyUDPServer(UDPServer, ThreadingMixIn): pass 如果你打算搞一个更先进的协程模型，可以编写一个CoroutineMixIn：\n1 2 class MyTCPServer(TCPServer, CoroutineMixIn): pass 由于Python允许使用多重继承，因此，MixIn就是一种常见的设计。\n只允许单一继承的语言（如Java）不能使用MixIn的设计。\n定制类 看到类似__slots__这种形如__xxx__的变量或者函数名就要注意，这些在Python中是有特殊用途的。\n__slots__我们已经知道怎么用了，len()方法我们也知道是为了能让class作用于len()函数。\n除此之外，Python的class中还有许多这样有特殊用途的函数，可以帮助我们定制类\nstr 这是因为直接显示变量调用的不是__str__()，而是__repr__()，两者的区别是__str__()返回用户看到的字符串，而__repr__()返回程序开发者看到的字符串，也就是说，repr()是为调试服务的。\n解决办法是再定义一个__repr__()。但是通常__str__()和__repr__()代码都是一样的，所以，有个偷懒的写法：\n1 2 3 4 5 6 class Student(object): def __init__(self, name): self.name = name def __str__(self): return \u0026#39;Student object (name=%s)\u0026#39; % self.name __repr__ = __str__ iter 如果一个类想被用于for \u0026hellip; in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。\n我们以斐波那契数列为例，写一个Fib类，可以作用于for循环：\n1 2 3 4 5 6 7 8 9 10 11 12 class Fib(object): def __init__(self): self.a,self.b = 0,1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a,self.b = self.b,self.a + self.b # 计算下一个值 if self.a \u0026gt; 100000: raise StopIteration return self.a # 返回下一个值 getitem 要表现得像list那样按照下标取出元素，需要实现__getitem__()方法：\n1 2 3 4 5 6 class Fib(object): def __getitem__(self,n): a,b = 1,1 for x in range(n): a,b = b, a+b return a 但是list有个神奇的切片方法：\nlist(range(100))[5:10] [5, 6, 7, 8, 9] 对于Fib却报错。原因是__getitem__()传入的参数可能是一个int，也可能是一个切片对象slice，所以要做判断：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Fib(object): def __getitem__(self,n): if isinstance(n,int): # n是索引 a,b = 1,1 for x in range(n): a,b = b,a+b return a if isinstance(n.slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a,b = 1,1 L = [] for x in range(stop): if x \u0026gt;= start: L.append(a) a,b = b,a+b return L 此外，如果把对象看成dict，getitem()的参数也可能是一个可以作key的object，例如str。\n与之对应的是__setitem__()方法，把对象视作list或dict来对集合赋值。最后，还有一个__delitem__()方法，用于删除某个元素。\n总之，通过上面的方法，我们自己定义的类表现得和Python自带的list、tuple、dict没什么区别，这完全归功于动态语言的“鸭子类型”，不需要强制继承某个接口。\ngetattr 要避免这个错误，除了可以加上一个score属性外，Python还有另一个机制，那就是写一个__getattr__()方法，动态返回一个属性。修改如下：\n1 2 3 4 5 6 7 class Student(object): def __init__(self): self.name = \u0026#39;Michael\u0026#39; def __getattr__(self, attr): if attr==\u0026#39;score\u0026#39;: return 99 s = Student() s.name \u0026lsquo;Michael\u0026rsquo; s.score 99\n返回函数也是完全可以的：\n1 2 3 4 class Student(object): def __getattr__(self, attr): if attr==\u0026#39;age\u0026#39;: return lambda: 25 只是调用方式要变为：\ns.age() 25\n注意，只有在没有找到属性的情况下，才调用__getattr__，已有的属性，比如name，不会在__getattr__中查找。\n此外，注意到任意调用如s.abc都会返回None，这是因为我们定义的__getattr__默认返回就是None。要让class只响应特定的几个属性，我们就要按照约定，抛出AttributeError的错误：\n这实际上可以把一个类的所有属性和方法调用全部动态化处理了，不需要任何特殊手段。\n这种完全动态调用的特性有什么实际作用呢？作用就是，可以针对完全动态的情况作调用。\n举个例子：\n现在很多网站都搞REST API，比如新浪微博、豆瓣啥的，调用API的URL类似：\nhttp://api.server/user/friends http://api.server/user/timeline/list 如果要写SDK，给每个URL对应的API都写一个方法，那得累死，而且，API一旦改动，SDK也要改。\n利用完全动态的__getattr__，我们可以写出一个链式调用：\n1 2 3 4 5 6 7 8 9 10 11 class Chain(object): def __init__(self,path=\u0026#39;\u0026#39;) self._path = path def __getattr__(self,path): return Chain(\u0026#39;%s%s\u0026#39; %(self._path,path)) def __str__(self): return self._path __repr__ = __str__ 这样，无论API怎么变，SDK都可以根据URL实现完全动态的调用，而且，不随API的增加而改变！\n还有些REST API会把参数放到URL中，比如GitHub的API：\nGET /users/:user/repos 调用时，需要把:user替换为实际用户名。如果我们能写出这样的链式调用：\nChain().users(\u0026lsquo;michael\u0026rsquo;).repos 就可以非常方便地调用API了。有兴趣的童鞋可以试试写出来。\ncall 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？在Python中，答案是肯定的。\n任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例\n1 2 3 4 5 6 class Student(object): def __init__(self,name): self.name = name def __call__(self): print(\u0026#39;My name is %s.\u0026#39;%self.name) 调用方式如下：\ns = Student(\u0026lsquo;Michael\u0026rsquo;) s() # self参数不要传入 My name is Michael.\ncall()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别。\n如果你把对象看成函数，那么函数本身其实也可以在运行期动态创建出来，因为类的实例都是运行期创建出来的，这么一来，我们就模糊了对象和函数的界限。\n那么，怎么判断一个变量是对象还是函数呢？其实，更多的时候，我们需要判断一个对象是否能被调用，能被调用的对象就是一个Callable对象，比如函数和我们上面定义的带有__call__()的类实例：\ncallable(Student()) True callable(max) True callable([1, 2, 3]) False callable(None) False callable(\u0026lsquo;str\u0026rsquo;) False\n通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。\n使用枚举类 更好的方法是为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能\n1 2 3 from enum import Enum Month = Enum(\u0026#39;Month\u0026#39;,(\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;, \u0026#39;Apr\u0026#39;, \u0026#39;May\u0026#39;, \u0026#39;Jun\u0026#39;, \u0026#39;Jul\u0026#39;, \u0026#39;Aug\u0026#39;, \u0026#39;Sep\u0026#39;, \u0026#39;Oct\u0026#39;, \u0026#39;Nov\u0026#39;, \u0026#39;Dec\u0026#39;)) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员：\n1 2 for name,member in Month.__members__.items(): print(name,\u0026#39;=\u0026gt;\u0026#39;,member,\u0026#39;,\u0026#39;,member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。\n如果需要更精确地控制枚举类型，可以从Enum派生出自定义类：\n1 2 3 4 5 6 7 8 9 10 11 from enum import Enum,unique @unique class Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。\n访问这些枚举类型可以有若干种方法：\nday1 = Weekday.Mon print(day1) Weekday.Mon print(Weekday.Tue) Weekday.Tue print(Weekday[\u0026lsquo;Tue\u0026rsquo;]) Weekday.Tue print(Weekday.Tue.value) 2 print(day1 == Weekday.Mon) True print(day1 == Weekday.Tue) False print(Weekday(1)) Weekday.Mon print(day1 == Weekday(1)) True Weekday(7) Traceback (most recent call last): \u0026hellip; ValueError: 7 is not a valid Weekday for name, member in Weekday.members.items(): \u0026hellip; print(name, \u0026lsquo;=\u0026gt;\u0026rsquo;, member) \u0026hellip; Sun =\u0026gt; Weekday.Sun Mon =\u0026gt; Weekday.Mon Tue =\u0026gt; Weekday.Tue Wed =\u0026gt; Weekday.Wed Thu =\u0026gt; Weekday.Thu Fri =\u0026gt; Weekday.Fri Sat =\u0026gt; Weekday.Sat\n可见，既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from enum import Enum,unique class Gender(Enum): Male = 0 Female = 1 class Student(object): def __init__(self,name,gender): self.name = name self.gender = gender # 测试 bart = Student(\u0026#39;Bart\u0026#39;,Gender.Male) if bart.gender == Gender.Male: print(\u0026#39;测试通过!\u0026#39;) else: print(\u0026#39;测试失败!\u0026#39;) Enum可以把一组相关常量定义在一个class中，且class不可变，而且成员可以直接\n使用元类 动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。\ntype() 比方说我们要定义一个Hello的class，就写一个hello.py模块：\n1 2 3 class Hello(object): def Hello(self,name=\u0026#39;world\u0026#39;): print(\u0026#39;Hello,%s.\u0026#39;%name) 当Python解释器载入hello模块时，就会依次执行该模块的所有语句，执行结果就是动态创建出一个Hello的class对象，\n我们说class的定义是运行时动态创建的，而创建class的方法就是使用type()函数。\ntype()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)\u0026hellip;的定义\n要创建一个class对象，type()函数依次传入3个参数：\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt;\u0026gt;\u0026gt; def fn(self, name=\u0026#39;world\u0026#39;): # 先定义函数 ... print(\u0026#39;Hello, %s.\u0026#39; % name) ... \u0026gt;\u0026gt;\u0026gt; Hello = type(\u0026#39;Hello\u0026#39;, (object,), dict(hello=fn)) # 创建Hello class \u0026gt;\u0026gt;\u0026gt; h = Hello() \u0026gt;\u0026gt;\u0026gt; h.hello() Hello, world. \u0026gt;\u0026gt;\u0026gt; print(type(Hello)) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; print(type(h)) \u0026lt;class \u0026#39;__main__.Hello\u0026#39;\u0026gt; class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。\n通过type()函数创建的类和直接写class是完全一样的，因为Python解释器遇到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。\n正常情况下，我们都用class Xxx\u0026hellip;来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。\nmetaclass 除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass。\nmetaclass，直译为元类，简单的解释就是：\n当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。\n但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。\n连接起来就是：先定义metaclass，就可以创建类，最后创建实例。\n所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。\nmetaclass是Python面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用metaclass的情况，所以，以下内容看不懂也没关系，因为基本上你不会用到\n我们先看一个简单的例子，这个metaclass可以给我们自定义的MyList增加一个add方法：\n定义ListMetaclass，按照默认习惯，metaclass的类名总是以Metaclass结尾，以便清楚地表示这是一个metaclass：\n1 2 3 4 5 # metaclass 是类的模板，所以必须从`type`类型派生 class ListMetaclass(type): def __new__(cls,name,bases,attrs): attrs[\u0026#39;add\u0026#39;] = lambda self,value:self.append(value) return type.__new__(cls,name,bases,attrs) 有了ListMetaclass，我们在定义类的时候还要指示使用ListMetaclass来定制类，传入关键字参数metaclass：\n1 2 class MyList(list,metaclass=ListMetabclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示Python解释器在创建MyList时，要通过ListMetaclass.new()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。\nnew()方法接收到的参数依次是：\n当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。\n动态修改有什么意义？直接在MyList定义中写上add()方法不是更简单吗？正常情况下，确实应该直接写，通过metaclass修改纯属变态。\n但是，总会遇到需要通过metaclass修改类定义的。ORM就是一个典型的例子。\nORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，写代码更简单，不用直接操作SQL语句。\n要编写一个ORM框架，所有的类都只能动态定义，因为只有使用者才能根据表的结构定义出对应的类来。\n让我们来尝试编写一个ORM框架。\n编写底层模块的第一步，就是先把调用接口写出来。比如，使用者如果使用这个ORM框架，想定义一个User类来操作对应的数据库表User，我们期待他写出这样的代码：\n1 2 3 4 5 6 7 8 9 10 11 class User(Model): # 定义类的属性到列的映F射 id = IntegerField(\u0026#39;id\u0026#39;) name = StringField(\u0026#39;username\u0026#39;) email = StringField(\u0026#39;email\u0026#39;) password = StringField(\u0026#39;password\u0026#39;) # 创建一个实例 u = User(id=12345,name=\u0026#39;Michael\u0026#39;,email=\u0026#39;test@orm.org\u0026#39;,password=\u0026#39;my-pwd\u0026#39;) # 保存到数据库 u.save() 其中，父类Model和属性类型StringField、IntegerField是由ORM框架提供的，剩下的魔术方法比如save()全部由父类Model自动完成。虽然metaclass的编写会比较复杂，但ORM的使用者用起来却异常简单。\n现在，我们就按上面的接口来实现该ORM。\n首先来定义Field类，它负责保存数据库表的字段名和字段类型：\n1 2 3 4 5 6 7 8 class Field(object): def __init__(self,name,column_type): self.name = name self.column_type = column_type def __str__(self): return \u0026#39;\u0026lt;%s:%s\u0026gt;\u0026#39; % (self.__class__.__name__,self.name) 在Field的基础上，进一步定义各种类型的Field，比如StringField，IntegerField等等：\n1 class StringField(Field) ","date":"2024-10-05T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/python%E7%AC%94%E8%AE%B0%E5%BB%96%E9%9B%AA%E5%B3%B0/39ddbb84c4a998da1962a60addcfed6c201d30f8_hu3761392623032128462.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/python%E7%AC%94%E8%AE%B0%E5%BB%96%E9%9B%AA%E5%B3%B0/","title":"Python笔记(廖雪峰)"},{"content":"概述 Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。\nRedis（Remote Dictionary Server）是一个开源的内存数据库，遵守 BSD 协议，它提供了一个高性能的键值（key-value）存储系统，常用于缓存、消息队列、会话存储等应用场景。\n数据结构 string：字符串 hash：散列 list：列表 set：集合 sorted set:有序集合 redis配置 Redis 的配置文件位于 Redis 安装目录下，文件名为 redis.conf 你可以通过修改 redis.conf 文件或使用 CONFIG set 命令来修改配置。\n参数说明 port 6379:指定 Redis 监听端口，默认端口为 6379 databases 16:设置数据库的数量，默认数据库为0，可以使用SELECT 命令在连接上指定数据库id dbfilename dump.rdb:指定本地数据库文件名，默认值为 dump.rdb dir ./:指定本地数据库存放目录 requirepass foobared:设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH 命令提供密码，默认关闭 数据类型 string string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据，比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。\n常用命令 SET key value：设置键的值。 GET key：获取键的值 INCR key：将键的值加 1 DECR key：将键的值减 1 APPEND key value：将值追加到键的值之后 Hash Redis hash 是一个键值(key=\u0026gt;value)对集合，类似于一个小型的 NoSQL 数据库。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 每个哈希最多可以存储 2^32 - 1 个键值对。\n常用命令 HSET key field value：设置哈希表中字段的值。 HGET key field：获取哈希表中字段的值。 HGETALL key：获取哈希表中所有字段和值。 HDEL key field：删除哈希表中的一个或多个字段。 List Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 列表最多可以存储 2^32 - 1 个元素。\n常用命令 LPUSH key value：将值插入到列表头部。 RPUSH key value：将值插入到列表尾部 LPOP key：移出并获取列表的第一个元素 RPOP key：移出并获取列表的最后一个元素。 LRANGE key start stop：获取列表在指定范围内的元素。 Set Redis 的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。\n常用命令 SADD key value：向集合添加一个或多个成员。 SREM key value：移除集合中的一个或多个成员。 SMEMBERS key：返回集合中的所有成员。 SISMEMBER key value：判断值是否是集合的成员。 zset(sorted set:有序集合) Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。\n常用命令 ZADD key score value：向有序集合添加一个或多个成员，或更新已存在成员的分数。 ZRANGE key start stop [WITHSCORES]：返回指定范围内的成员。 ZREM key value：移除有序集合中的一个或多个成员。 ZSCORE key value：返回有序集合中，成员的分数值。 zadd命令 添加元素到集合，元素在集合中存在则更新对应score\nzadd key score member\n其他高级数据类似 HyperLogLog 用于基数估计算法的数据结构。 常用于统计唯一值的近似值。\nBitmaps 位数组，可以对字符串进行位操作。 常用于实现布隆过滤器等位操作。\nGeospatial Indexes 处理地理空间数据，支持地理空间索引和半径查询。\nStreams 日志数据类型，支持时间序列数据。 用于消息队列和实时数据处理\nredis命令 Redis 命令用于在 redis 服务上执行操作 要在 redis 服务上执行命令需要一个 redis 客户端。Redis 客户端在我们之前下载的的 redis 的安装包中\n语法 Redis 客户端的基本语法为：\n$ redis-cli\n实例 以下实例讲解了如何启动 redis 客户端： 启动 redis 服务器，打开终端并输入命令 redis-cli，该命令会连接本地的 redis 服务。\n$ redis-cli redis 127.0.0.1:6379\u0026gt; redis 127.0.0.1:6379\u0026gt; PING\nPONG\n在远程服务上执行命令 如果需要在远程 redis 服务上执行命令，同样我们使用的也是 redis-cli 命令。\n语法 $ redis-cli -h host -p port -a password\n实例 以下实例演示了如何连接到主机为 127.0.0.1，端口为 6379 ，密码为 mypass 的 redis 服务上\n$redis-cli -h 127.0.0.1 -p 6379 -a \u0026ldquo;mypass\u0026rdquo; redis 127.0.0.1:6379\u0026gt; redis 127.0.0.1:6379\u0026gt; PING\nPONG\nredis键(key) Redis 键命令用于管理 redis 的键。\n语法 Redis 键命令的基本语法如下:\nredis 127.0.0.1:6379\u0026gt; COMMAND KEY_NAME\n实例 redis 127.0.0.1:6379\u0026gt; SET runoobkey redis OK redis 127.0.0.1:6379\u0026gt; DEL runoobkey (integer) 1\n在以上实例中 DEL 是一个命令， runoobkey 是一个键。 如果键被删除成功，命令执行后输出 (integer) 1，否则将输出 (integer) 0\nredis keys命令 del key：该命令用于在 key 存在时删除 key。 dump key:序列化给定 key ，并返回被序列化的值。 exists key:检查给定 key 是否存在。 expire key seconds：检查给定 key 是否存在。 expire key timestamp:EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 pexpire key milliseconds：设置 key 的过期时间以毫秒计。 pexpireat key milliseconds-timestamp：设置 key 过期时间的时间戳(unix timestamp) 以毫秒计 keys pattern：查找所有符合给定模式( pattern)的 key 。 move key db：将当前数据库的 key 移动到给定的数据库 db 当中。 persist key：移除 key 的过期时间，key 将持久保持。 pttl key：以毫秒为单位返回 key 的剩余的过期时间。 ttl key：以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 randomkey：从当前数据库中随机返回一个 key 。 rename key newkey：仅当 newkey 不存在时，将 key 改名为 newkey 。 renamenx key newkey：迭代数据库中的数据库键。 scan cursor [match pattern][count count]： type key:返回 key 所储存的值的类型。 redis 字符串 语法 redis 127.0.0.1:6379\u0026gt; COMMAND KEY_NAME\n实例 redis 127.0.0.1:6379\u0026gt; SET runoobkey redis OK redis 127.0.0.1:6379\u0026gt; GET runoobkey \u0026ldquo;redis\u0026rdquo;\n常用命令 set key value:设置指定key的值 get key:获取指定 key 的值。 getrange key start end：返回 key 中字符串值的子字符 getset key value:将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 getbit key offset:对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 mget key1[key2]:获取所有(一个或多个)给定 key 的值。 setbit key offset value:对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 setex key seconds value:将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位)。 setnx key value:只有在 key 不存在时设置 key 的值。 setrange key offset value:用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始。 strlen key:返回 key 所储存的字符串值的长度。 mset key value [key value]：同时设置一个或多个 key-value 对。 msetnx key value [key value]:同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 psetex key milliseconds value:这个命令和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，而不是像 SETEX 命令那样，以秒为单位 incr key:将 key 中储存的数字值增一 incrby key increment:将 key 所储存的值加上给定的增量值（increment）。 incrbyfloat key increment:将 key 所储存的值加上给定的浮点增量值（increment） decr key:将 key 中储存的数字值减一。 decrby key decrement:key 所储存的值减去给定的减量值（decrement） 。 append key value:如果 key 已经存在并且是一个字符串， APPEND 命令将指定的 value 追加到该 key 原来值（value）的末尾。 redis哈希 Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。 Redis 中每个 hash 可以存储 232 - 1 键值对（40多亿）。\n实例 127.0.0.1:6379\u0026gt; HMSET runoobkey name \u0026ldquo;redis tutorial\u0026rdquo; description \u0026ldquo;redis basic commands for caching\u0026rdquo; likes 20 visitors 23000 OK 127.0.0.1:6379\u0026gt; HGETALL runoobkey\n\u0026ldquo;name\u0026rdquo; \u0026ldquo;redis tutorial\u0026rdquo; \u0026ldquo;description\u0026rdquo; \u0026ldquo;redis basic commands for caching\u0026rdquo; \u0026ldquo;likes\u0026rdquo; \u0026ldquo;20\u0026rdquo; \u0026ldquo;visitors\u0026rdquo; \u0026ldquo;23000\u0026rdquo; 在以上实例中，我们设置了 redis 的一些描述信息(name, description, likes, visitors) 到哈希表的 runoobkey 中。\n命令 hdel key field1[field2]：删除一个或多个哈希表字段 hexists key field:查看哈希表 key 中，指定的字段是否存在 hget key field:获取存储在哈希表中指定字段的值。 hgetall key:获取在哈希表中指定 key 的所有字段和值 hincrby key field increment:为哈希表 key 中的指定字段的整数值加上增量 increment 。 hincrbyfloat key field increment:为哈希表 key 中的指定字段的浮点数值加上增量 increment 。 hkeys key:获取哈希表中的所有字段 hlen key:获取哈希表中字段的数量 hmget key field1[field2]:获取所有给定字段的值 hmset key field1 value1[field1 value2]:同时将多个 field-value (域-值)对设置到哈希表 key 中。 hset key field value:将哈希表 key 中的字段 field 的值设为 value 。 hsetnx key field value:只有在字段 field 不存在时，设置哈希表字段的值。 hvals key:获取哈希表中所有值 hscan key cursor[matchb pattern][count count]:迭代哈希表中的键值对。 redis列表 Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过40亿个元素)。\n命令 blpop key1[key2] timeout:移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpop key1[key2] timeout:移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpoplpush source destination timeout：从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 lindex key index:通过索引获取列表中的元素 linsert key before|after pivot value:在列表的元素前或者后插入元素 lien key:获取列表长度 lpop key:移出并获取列表的第一个元素 lpush key value1[value2]：将一个或多个值插入到列表头部 lpushx key value:将一个值插入到已存在的列表头部 lrange key start stop:获取列表指定范围内的元素 lrem key count value：移除列表元素 lset key index value:通过索引设置列表元素的值 ltrim key start stop:对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除 rpop key:移除列表的最后一个元素，返回值为移除的元素 rpoplpush source destination:移除列表的最后一个元素，并将该元素添加到另一个列表并返回 rpush key value1[value2]:在列表中添加一个或多个值到列表尾部 rpushx key value:为已存在的列表添加值 redis 集合 Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 集合对象的编码可以是 intset 或者 hashtable。 Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。\n命令 sadd key member1：向集合添加一个或多个成员 scard key:获取集合的成员数 sdiff key1:返回第一个集合与其他集合之间的差异。 sdiffstore destination key1:返回给定所有集合的差集并存储在 destination 中 sinter key1:返回给定所有集合的交集 sinterstore destination key1:返回给定所有集合的交集并存储在 destination 中 sismember key member:判断 member 元素是否是集合 key 的成员 smembers key：返回集合中的所有成员 smove source destination member：将 member 元素从 source 集合移动到 destination 集合 spop key:移除并返回集合中的一个随机元素 srandmember key[count]:返回集合中一个或多个随机数 srem key member1[member2]:移除集合中一个或多个成员 sunion key1:返回所有给定集合的并集 sunionstore deswwtination key1:所有给定集合的并集存储在 destination 集合中 sscan key cursor [match pattern][count count]:迭代集合中的元素 redis 有序集合（sorted set） Redis 有序集合和集合一样也是 string 类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。 有序集合的成员是唯一的,但分数(score)却可以重复。\n实例 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 1 redis (integer) 1 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 2 mongodb (integer) 1 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 3 mysql (integer) 1 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 3 mysql (integer) 0 redis 127.0.0.1:6379\u0026gt; ZADD runoobkey 4 mysql (integer) 0 redis 127.0.0.1:6379\u0026gt; ZRANGE runoobkey 0 10 WITHSCORES\n\u0026ldquo;redis\u0026rdquo; \u0026ldquo;1\u0026rdquo; \u0026ldquo;mongodb\u0026rdquo; \u0026ldquo;2\u0026rdquo; \u0026ldquo;mysql\u0026rdquo; \u0026ldquo;4\u0026rdquo; 命令 ZADD key score1 member1 [score2 member2]:向有序集合添加一个或多个成员，或者更新已存在成员的分数 ZCARD key:获取有序集合的成员数 ZCOUNT key min max:计算在有序集合中指定区间分数的成员数 ZINCRBY key increment member:有序集合中对指定成员的分数加上增量 increment ZINTERSTORE destination numkeys key [key \u0026hellip;]:计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 destination 中 ZLEXCOUNT key min max:在有序集合中计算指定字典区间内成员数量 ZRANGE key start stop [WITHSCORES]:通过索引区间返回有序集合指定区间内的成员 ZRANGEBYLEX key min max [LIMIT offset count]:通过字典区间返回有序集合的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT]:通过分数返回有序集合指定区间内的成员 ZRANK key member:返回有序集合中指定成员的索引 ZREM key member [member \u0026hellip;]:移除有序集合中的一个或多个成员 ZREMRANGEBYLEX key min max:移除有序集合中给定的字典区间的所有成员 ZREMRANGEBYRANK key start stop:移除有序集合中给定的排名区间的所有成员 ZREMRANGEBYSCORE key min max:移除有序集合中给定的分数区间的所有成员 ZREVRANGE key start stop [WITHSCORES]:返回有序集中指定区间内的成员，通过索引，分数从高到低 ZREVRANGEBYSCORE key max min [WITHSCORES]:返回有序集中指定分数区间内的成员，分数从高到低排序 ZREVRANK key member:返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 ZSCORE key member:返回有序集中，成员的分数值 ZUNIONSTORE destination numkeys key [key \u0026hellip;]:计算给定的一个或多个有序集的并集，并存储在新的 key 中 ZSCAN key cursor [MATCH pattern] [COUNT count]:迭代有序集合中的元素（包括元素成员和元素分值） redis hyperloglog Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\n什么是基数 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。\n命令 PFADD key element [element \u0026hellip;]:添加指定元素到 HyperLogLog 中。 PFCOUNT key [key \u0026hellip;]:返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey \u0026hellip;]:将多个 HyperLogLog 合并为一个 HyperLogLog redis 发布订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。\n实例 以下实例演示了发布订阅是如何工作的，需要开启两个 redis-cli 客户端。 在我们实例中我们创建了订阅频道名为 runoobChat:\nredis 127.0.0.1:6379\u0026gt; SUBSCRIBE runoobChat\nReading messages\u0026hellip; (press Ctrl-C to quit)\n\u0026ldquo;subscribe\u0026rdquo; \u0026ldquo;runoobChat\u0026rdquo; (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。\nredis 127.0.0.1:6379\u0026gt; PUBLISH runoobChat \u0026ldquo;Redis PUBLISH test\u0026rdquo;\n(integer) 1\nredis 127.0.0.1:6379\u0026gt; PUBLISH runoobChat \u0026ldquo;Learn redis by runoob.com\u0026rdquo;\n(integer) 1\n#订阅者的客户端会显示如下消息\n\u0026ldquo;message\u0026rdquo; \u0026ldquo;runoobChat\u0026rdquo; \u0026ldquo;Redis PUBLISH test\u0026rdquo; \u0026ldquo;message\u0026rdquo; \u0026ldquo;runoobChat\u0026rdquo; \u0026ldquo;Learn redis by runoob.com\u0026rdquo; 命令 PSUBSCRIBE pattern [pattern \u0026hellip;]：订阅一个或多个符合给定模式的频道。 PUBSUB subcommand [argument [argument \u0026hellip;]]：查看订阅与发布系统状态。 PUBLISH channel message：将信息发送到指定的频道。 PUNSUBSCRIBE [pattern [pattern \u0026hellip;]]：退订所有给定模式的频道。 SUBSCRIBE channel [channel \u0026hellip;]：订阅给定的一个或多个频道的信息。 UNSUBSCRIBE [channel [channel \u0026hellip;]]：指退订给定的频道 redis事务 Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证：\n批量操作在发送 EXEC 命令前被放入队列缓存 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 一个事务从开始到执行会经历以下三个阶段\n开始事务 命令入队 执行事务 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。\n命令 DISCARD:取消事务，放弃执行事务块内的所有命令 EXEC:执行所有事务块内的命令。 MULTI:标记一个事务块的开始 UNWATCH:取消 WATCH 命令对所有 key 的监视。 WATCH key [key \u0026hellip;]:监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断 redis脚本 redis连接 Redis 连接命令主要是用于连接 redis 服务。\n实例 以下实例演示了客户端如何通过密码验证连接到 redis 服务，并检测服务是否在运行：\nredis 127.0.0.1:6379\u0026gt; AUTH \u0026ldquo;password\u0026rdquo; OK redis 127.0.0.1:6379\u0026gt; PING PONG\n命令 AUTH password：验证密码是否正确 ECHO message：打印字符串 PING：查看服务是否运行 QUIT：关闭当前连接 SELECT index：切换到指定的数据库 redis服务器 Redis 服务器命令主要是用于管理 redis 服务。\nredis GEO reids stream Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅 (pub/sub) 来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。 简单来说发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。 而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失 Redis Stream 的结构如下所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容\n每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。\nredis客户端连接 Redis 通过监听一个 TCP 端口或者 Unix socket 的方式来接收来自客户端的连接，当一个连接建立后，Redis 内部会进行以下一些操作：\n首先，客户端 socket 会被设置为非阻塞模式，因为 Redis 在网络事件处理上采用的是非阻塞多路复用模型。 然后为这个 socket 设置 TCP_NODELAY 属性，禁用 Nagle 算法 然后创建一个可读的文件事件用于监听这个客户端 socket 的数据发送 最大连接数、 在 Redis2.4 中，最大连接数是被直接硬编码在代码里面的，而在2.6版本中这个值变成可配置的。 maxclients 的默认值是 10000，你也可以在 redis.conf 中对这个值进行修改。\n命令 CLIENT LIST：返回连接到 redis 服务的客户端列表 CLIENT SETNAME：设置当前连接的名称 CLIENT GETNAME：获取通过 CLIENT SETNAME 命令设置的服务名称 CLIENT PAUSE：挂起客户端连接，指定挂起的时间以毫秒计 CLIENT KILL：关闭客户端连接 ","date":"2024-06-20T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/redis%E7%AC%94%E8%AE%B0/e0f12bb2ec86a77c779b904b79e29ca5_hu6629852201849345440.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/redis%E7%AC%94%E8%AE%B0/","title":"Redis笔记"},{"content":"调试 VS Code 的内置调试器有助于加速编辑、编译和调试循环。\n调试器扩展 VS Code 具有对 Node.js 运行时的内置调试支持，可以调试 JavaScript、TypeScript 或转译为 JavaScript 的任何其他语言。\n若要调试其他语言和运行时（包括 PHP、Ruby、Go、C#、Python、C++、PowerShell 等），请在 VS Code Marketplace 中查找扩展，或在顶级“运行”菜单中选择“安装其他调试器”。\nLaunch.json属性 在为属性指定值后，可以使用 IntelliSense （Ctrl+Space） 查看可用属性的列表。launch.jsontype\n以下属性对于每个启动配置都是必需的：\ntype: 用于此启动配置的调试器类型。每个已安装的调试扩展都引入了一种类型：例如，用于内置 Node 调试器，或者用于 PHP 和 Go 扩展。 request: 此启动配置的请求类型。目前，并受支持。launch attach name:\u0026ldquo;Python: Current File\u0026rdquo;: 这个字段指定了配置的名称，它将在 VS Code 的调试启动配置下拉菜单中显示。这里的名称是 \u0026ldquo;Python: Current File\u0026rdquo;，意味着这个配置是用来调试当前打开的 Python 文件 ","date":"2024-05-26T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/vscode%E7%AE%80%E5%8D%95%E8%B0%83%E8%AF%95/f5a3e854330c8e49bae0774f3de85ba3_hu8425483847487405866.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/vscode%E7%AE%80%E5%8D%95%E8%B0%83%E8%AF%95/","title":"Vscode简单调试"},{"content":"介绍 YAML 是 \u0026ldquo;YAML Ain\u0026rsquo;t a Markup Language\u0026rdquo;（YAML 不是一种标记语言）的递归缩写。在开发的这种语言时，YAML 的意思其实是：\u0026ldquo;Yet Another Markup Language\u0026rdquo;（仍是一种标记语言）。\nYAML 的语法和其他高级语言类似，并且可以简单表达清单、散列表，标量等数据形态。它使用空白符号缩进和大量依赖外观的特色，特别适合用来表达或编辑数据结构、各种配置文件、倾印调试内容、文件大纲（例如：许多电子邮件标题格式和YAML非常接近）。\nYAML 的配置文件后缀为 .yml，如：runoob.yml 。\n基本语法 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格 缩进的空格数不重要，只要相同层级的元素左对齐即可 #表示注释 数据类型 对象：键值对的机会，又称为映射/哈希/字典 数组：一组按次序排列的值，又称为序列/表 纯量：单个的，不可再分的值 YAML对象 对象键值对使用冒号结构表示key：value，冒号后面再加一个空格 也可以使用key：{key1：value1,key2：value2,\u0026hellip;} 还可以用缩进表示层级关系\nkey: child-key: value child-key2: value2\n较为复杂的对象格式，可以使用问号加一个空格代表一个复杂的 key，配合一个冒号加一个空格代表一个 value\n?\n- complexkey1 - complexkey2 : - complexvalue1 - complexvalue2\n意思即对象的属性是一个数组[complexkey1,complexkey2],对应的值也是一个数组 [complexvalue1,complexvalue2]\nYAML数组 以 - 开头的行表示构成一个属组\nA B C 数据结构的子成员是一个数据，则可以在该项下面缩进一个空格 A B C 一个相对复杂的例子： companies: - id: 1 name: company1 price: 200W - id: 2 name: company2 price: 500W\n意思是 companies 属性是一个数组，每一个数组元素又是由 id、name、price 三个属性构成。\n数组也可以使用流式(flow)的方式表示：\ncompanies: [{id: 1,name: company1,price: 200W},{id: 2,name: company2,price: 500W}] 复合结构 数组和对象可以构成复合结构 languages:\nRuby Perl Python websites: YAML: yaml.org Ruby: ruby-lang.org Python: python.org Perl: use.perl.org 转换为json为： { languages: [ \u0026lsquo;Ruby\u0026rsquo;, \u0026lsquo;Perl\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;], websites: { YAML: \u0026lsquo;yaml.org\u0026rsquo;, Ruby: \u0026lsquo;ruby-lang.org\u0026rsquo;, Python: \u0026lsquo;python.org\u0026rsquo;, Perl: \u0026lsquo;use.perl.org\u0026rsquo; } }\n纯量 纯量是最基本的，不可再分的值，包括\n字符串 布尔值 整数 浮点数 Null 时间 日期 使用一个例子来快速了解纯量的基本使用： boolean: - TRUE #true,True都可以 - FALSE #false，False都可以 float: - 3.14 - 6.8523015e+5 #可以使用科学计数法 int: - 123 - 0b1010_0111_0100_1010_1110 #二进制表示 null: nodeName: \u0026rsquo;node\u0026rsquo; parent: ~ #使用~表示null string: - 哈哈 - \u0026lsquo;Hello world\u0026rsquo; #可以使用双引号或者单引号包裹特殊字符 - newline newline2 #字符串可以拆成多行，每一行会被转化成一个空格 date: - 2018-02-17 #日期必须使用ISO 8601格式，即yyyy-MM-dd datetime: - 2018-02-17T15:02:31+08:00 #时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n引用 \u0026amp; 锚点和 * 别名，可以用来引用 defaults: \u0026amp;defaults adapter: postgres host: localhost\ndevelopment: database: myapp_development \u0026laquo;: *defaults\ntest: database: myapp_test \u0026laquo;: *defaults\n相当于： defaults: adapter: postgres host: localhost\ndevelopment: database: myapp_development adapter: postgres host: localhost\ntest: database: myapp_test adapter: postgres host: localhost\n\u0026amp; 用来建立锚点，\u0026laquo; 表示合并到当前数据，*用来引用锚点\n下面是另一个例子:\n\u0026amp;showell Steve Clark Brian Oren *showell 转为 JavaScript 代码如下: [ \u0026lsquo;Steve\u0026rsquo;, \u0026lsquo;Clark\u0026rsquo;, \u0026lsquo;Brian\u0026rsquo;, \u0026lsquo;Oren\u0026rsquo;, \u0026lsquo;Steve\u0026rsquo; ]\n","date":"2024-05-26T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/yaml%E7%AC%94%E8%AE%B0/8d65db2f2003a7a63c85ae9e4edb4f2d_hu14422259510554334276.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/yaml%E7%AC%94%E8%AE%B0/","title":"Yaml笔记"},{"content":"概述 SQLAlchemy SQL 工具包和对象关系映射器是一套用于处理数据库和 Python 的综合工具\nSQLAlchemy 最重要的两个面向用户的部分是对象关系映射器 (ORM)和Core。\nCore 包含了 SQLAlchemy 的 SQL 和数据库集成以及描述服务的广度，其中最突出的是SQL 表达式语言。\nSQL 表达式语言是一个独立于 ORM 包的工具包，它提供了一个由可组合对象表示的 SQL 表达式构建系统，然后可以在特定事务的范围内针对目标数据库“执行”这些表达式，返回结果集。通过传递表示这些语句的 SQL 表达式对象以及表示要与每个语句一起使用的参数的字典，可以实现插入、更新和删除（即 DML）。\n统一教程 SQLAlchemy 被呈现为两个不同的 API，一个建立在另一个之上。这些 API 被称为Core 和ORM。\nSQLAlchemy Core 是 SQLAlchemy 作为“数据库工具包”的基础架构。该库提供了管理数据库连接、与数据库查询和结果交互以及以编程方式构建 SQL 语句的工具。\n主要仅限 Core 的部分不会引用 ORM。这些部分中使用的 SQLAlchemy 结构将从 sqlalchemy 命名空间导入。作为主题分类的额外指示，它们还将在右侧包含一个深蓝色边框。在使用 ORM 时，这些概念仍然在起作用，但在用户代码中不那么显式。ORM 用户应该阅读这些部分，但不要期望直接使用这些 API 来编写以 ORM 为中心的代码。\nSQLAlchemy ORM 建立在 Core 之上，提供可选的对象关系映射功能。ORM 提供了一个额外的配置层，允许用户定义的 Python 类被映射到数据库表和其他结构，以及一种称为会话的对象持久化机制。然后，它扩展了 Core 级别的 SQL 表达式语言，允许以用户定义的对象来编写和调用 SQL 查询。\n建立连接 - 引擎 任何 SQLAlchemy 应用程序的开始都是一个称为 Engine 的对象。此对象充当连接到特定数据库的中心源，既提供一个工厂，又提供一个称为 连接池 的存储空间，用于这些数据库连接。引擎通常是仅为特定数据库服务器创建一次的全局对象，并且使用 URL 字符串进行配置，该 URL 字符串将描述它如何连接到数据库主机或后端\n1 2 from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite+pysqlite:///:memory:\u0026#34;, echo=True) 使用事务和DBAPI 获取连接 从用户角度来看，Engine 对象的唯一目的是提供一个连接到数据库的连接单元，称为 Connection。在直接使用 Core 时，Connection 对象是所有与数据库交互的方式。由于 Connection 代表一个针对数据库的开放资源，我们希望始终将对该对象的使用的范围限制在特定上下文中，而最好的方法是使用 Python 上下文管理器形式，也称为 with 语句\n1 2 3 4 5 from sqlalchemy import text with engine.connect() as conn: result = conn.execute(text(\u0026#34;select \u0026#39;hello world\u0026#39;\u0026#34;)) print(result.all()) 提交更改 我们发出了两个通常是事务性的 SQL 语句，一个“CREATE TABLE”语句 [1] 和一个参数化的“INSERT”语句（上面的参数化语法将在下面几节中讨论，在 发送多个参数 中）。由于我们希望在块内提交所做的工作，因此我们调用 Connection.commit() 方法来提交事务。在我们在块内调用此方法后，我们可以继续运行更多 SQL 语句，如果我们选择，我们可以再次调用 Connection.commit() 来处理后续语句。SQLAlchemy 将这种风格称为边走边提交。\n还有一种提交数据的方式，即我们可以预先将“连接”块声明为事务块。对于这种操作模式，我们使用 Engine.begin() 方法获取连接，而不是使用 Engine.connect() 方法。此方法将同时管理 Connection 的范围，并将所有内容包含在一个事务中，并在成功执行块后执行 COMMIT，或在出现异常时执行 ROLLBACK。这种方式被称为 一次开始\n使用数据库元数据 在完成引擎和 SQL 执行后，我们准备开始使用 SQLAlchemy。SQLAlchemy Core 和 ORM 的核心元素是 SQL 表达式语言，它允许以流畅、可组合的方式构建 SQL 查询。这些查询的基础是表示数据库概念（如表和列）的 Python 对象。这些对象统称为 数据库元数据。\nSQLAlchemy 中最常见的数据库元数据基础对象是 MetaData、Table 和 Column\n使用Table对象设置MetaData 当我们使用关系型数据库时，数据库中用于存储数据的基本结构称为表。在 SQLAlchemy 中，数据库“表”最终由一个名为 Table 的 Python 对象表示。\n要开始使用 SQLAlchemy 表达式语言，我们需要构建表示我们想要操作的所有数据库表的 Table 对象。Table 是以编程方式构建的，可以通过直接使用 Table 构造函数，也可以通过使用 ORM Mapped 类（稍后在 使用 ORM 声明式表单定义表元数据 中描述）间接构建。还可以选择从现有数据库加载部分或全部表信息，称为 反射。\n无论使用哪种方法，我们总是从一个集合开始，这个集合将是我们放置称为 MetaData 对象的表的集合。这个对象本质上是一个围绕 Python 字典的 facade，它存储一系列以字符串名称为键的 Table 对象。虽然 ORM 提供了一些关于从哪里获取这个集合的选项，但我们始终可以选择直接创建一个，它看起来像\n1 2 from sqlalchemy import MetaData metadata_obj = MetaData() 一旦我们有了 MetaData 对象，我们就可以声明一些 Table 对象\n对于整个应用程序使用单个 MetaData 对象是最常见的情况，它被表示为应用程序中某个位置的模块级变量，通常位于“models”或“dbschema”类型的包中。 MetaData 通常通过以 ORM 为中心的 registry 或 Declarative Base 基类访问，因此同一个 MetaData 在 ORM 和 Core 声明的 Table 对象之间共享。\n声明简单约束 将DDL发射到数据库 我们构建了一个对象结构，它表示数据库中的两个数据库表，从根 MetaData 对象开始，然后进入两个 Table 对象，每个对象都包含一个 Column 和 Constraint 对象的集合。这个对象结构将成为我们今后使用 Core 和 ORM 执行的大多数操作的核心。\n使用ORM声明式表单定义表元数据 Table 对象，它是在构建 SQL 表达式时 SQLAlchemy 最终引用数据库表的底层机制。如前所述，SQLAlchemy ORM 为 Table 声明过程提供了一个外观，称为 声明式表。声明式表过程实现了与上一节中相同的目标，即构建 Table 对象，但在这个过程中，它还为我们提供了其他东西，称为 ORM 映射类，或简称为“映射类”。映射类是使用 ORM 时 SQL 最常见的基石，在现代 SQLAlchemy 中，它也可以非常有效地与以 Core 为中心的用法结合使用\n使用 ORM 时，声明 Table 元数据的过程通常与声明 映射 类相结合。映射类是我们想要创建的任何 Python 类，它将具有与数据库表中的列关联的属性。虽然实现这一点的方式有很多种，但最常见的风格被称为 声明式，它允许我们同时声明用户定义的类和 Table 元数据。\n建立声明式基类 当使用 ORM 时，MetaData 集合仍然存在，但它本身与一个仅限 ORM 的结构相关联，通常被称为 声明式基类。获取新的声明式基类的最便捷方式是创建一个新的类，该类是 SQLAlchemy DeclarativeBase 类的子类。\n1 2 3 from sqlalchemy.orm import DeclarativeBase class Base(DeclarativeBase): pass 上面，Base 类是我们所说的声明式基类。当我们创建新的类作为 Base 的子类时，结合适当的类级别指令，它们将在类创建时被建立为新的 ORM 映射类，每个类通常（但并非总是）引用一个特定的 Table 对象。\n声明式基类引用一个 MetaData 集合，该集合会自动为我们创建，假设我们没有从外部提供。这个 MetaData 集合可以通过 DeclarativeBase.metadata 类级别属性访问。当我们创建新的映射类时，它们将分别引用此 MetaData 集合中的一个 Table。\n声明式基类也指代一个名为 registry 的集合，它是 SQLAlchemy ORM 中的中心“映射配置”单元。虽然很少直接访问，但此对象是映射配置过程的核心，因为一组 ORM 映射类将通过此注册表相互协调。与 MetaData 一样，我们的声明式基类也为我们创建了一个 registry（同样可以传递我们自己的 registry），我们可以通过 DeclarativeBase.registry 类变量访问它。\n声明映射类 在建立了 Base 类之后，我们现在可以根据新的类 User 和 Address，为 user_account 和 address 表定义 ORM 映射类。我们将在下面说明最现代的声明式形式，它由 PEP 484 类型注解驱动，使用特殊类型 Mapped，它指示将属性映射为特定类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from typing import List from typing import Optional from sqlalchemy.orm import Mapped from sqlalchemy.orm import mapped_column from sqlalchemy.orm import relationship class User(Base): __tablename__ = \u0026#34;user_account\u0026#34; id: Mapped[int] = mapped_column(primary_key=True) name: Mapped[str] = mapped_column(String(30)) fullname: Mapped[Optional[str]] addresses: Mapped[List[\u0026#34;Address\u0026#34;]] = relationship(back_populates=\u0026#34;user\u0026#34;) def __repr__(self) -\u0026gt; str: return f\u0026#34;User(id={self.id!r}, name={self.name!r}, fullname={self.fullname!r})\u0026#34; class Address(Base): __tablename__ = \u0026#34;address\u0026#34; id: Mapped[int] = mapped_column(primary_key=True) email_address: Mapped[str] user_id = mapped_column(ForeignKey(\u0026#34;user_account.id\u0026#34;)) user: Mapped[User] = relationship(back_populates=\u0026#34;addresses\u0026#34;) def __repr__(self) -\u0026gt; str: return f\u0026#34;Address(id={self.id!r}, email_address={self.email_address!r})\u0026#34; 每个类都引用一个 Table 对象，该对象是在声明式映射过程中生成的，其名称是通过将字符串分配给 DeclarativeBase.tablename 属性来确定的。创建类后，此生成的 Table 可从 DeclarativeBase.table 属性获得。\n如前所述，这种形式被称为 声明式表配置。几种替代声明风格中的一种将让我们直接构建 Table 对象，并将其 分配 给 DeclarativeBase.table。这种风格被称为 带有命令式表的声明式。\n为了在 Table 中指示列，我们使用 mapped_column() 结构，并结合基于 Mapped 类型的类型注解。此对象将生成 Column 对象，这些对象将应用于 Table 的构建。\n对于具有简单数据类型且没有其他选项的列，我们可以仅指示 Mapped 类型注解，使用简单的 Python 类型，如 int 和 str 来表示 Integer 和 String。在声明式映射过程中如何解释 Python 类型是开放式的；有关背景信息，请参阅部分 使用带注解的声明式表（mapped_column() 的类型注解形式） 和 自定义类型映射。\n根据是否存在 Optional[] 类型注解（或其等效形式， | None 或 Union[, None]），可以将列声明为“可为空”或“不可为空”。mapped_column.nullable 参数也可以显式使用（并且不必与注解的可选性匹配）。\n使用显式类型注解是完全可选的。我们也可以使用 mapped_column() 而不使用注解。使用这种形式时，我们将使用更明确的类型对象，例如 Integer 和 String，以及根据需要在每个 mapped_column() 结构中使用 nullable=False。\n另外两个属性，User.addresses 和 Address.user，定义了一种不同类型的属性，称为 relationship()，它具有与所示类似的注解感知配置样式。relationship() 结构将在 使用 ORM 相关对象 中更详细地讨论。\n如果我们没有声明自己的 init() 方法，这些类将自动获得一个 init() 方法。此方法的默认形式接受所有属性名称作为可选关键字参数。\nsandy = User(name=\u0026ldquo;sandy\u0026rdquo;, fullname=\u0026ldquo;Sandy Cheeks\u0026rdquo;) 为了自动生成一个功能齐全的 init() 方法，该方法提供位置参数以及具有默认关键字值的参数，可以在 声明式数据类映射 中介绍的数据类功能中使用。当然，始终可以选择使用显式的 init() 方法。\n添加了 repr() 方法，以便我们获得可读的字符串输出；这些方法不需要在这里。与 init() 一样，可以使用 数据类 功能自动生成 repr() 方法。\n表反射 表反射是指通过读取数据库的当前状态来生成Table和相关对象的过程\n作为反射的示例，我们将创建一个新的Table对象，它代表我们在本文档前面部分手动创建的some_table对象。执行此操作的方式有很多种，但最基本的方式是构造一个Table对象，给出表的名称和它将所属的MetaData集合，然后不是指定单个Column和Constraint对象，而是使用Engine传递目标Table.autoload_with参数。\n在整个过程中，some_table对象现在包含有关表中存在的Column对象的信息，并且该对象的使用方式与我们显式声明的Table完全相同。\n使用数据 使用insert 使用select 使用update 使用delete ","date":"2024-05-05T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/sqlalchemy%E7%AC%94%E8%AE%B0/bebc542adb28a4465a71050225954ca26cb14e96_hu13291367772106908814.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/sqlalchemy%E7%AC%94%E8%AE%B0/","title":"SQLAlchemy笔记"},{"content":"快速开始 基于Ruia快速实现一个以Hacker News为目标的爬虫 本文主要通过对hacker News的爬取示例来展示如何使用ruia。\n第一步：定义item item的目的是定义目标网站你需要爬取的数据，此时，爬虫的目标数据就是页面中的title和url，怎么提取数据，ruia的field类提供了以下三种方式提取目标数据\nxpath re css selector 本教程爬虫例子都默认使用css selector的规则来提取目标数据\n规则确定后，就可以用 item来实现一个针对目标数据的 orm，创建文件 items.py\n1 2 3 4 5 6 from ruia import AttrField,TextField,Item class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;,attr=\u0026#39;href\u0026#39;) 这段代码含义是：针对我们提取的目标html，我们定义了一个HackerNewsItem类，其包含了两个field\ntitle：直接从文本提取 url：从属性提取 而 target_item是什么。对于一个 Item类来说，当其定义好网页目标数据后，ruia提供两种方式进行获取 Item\nget_item：获取网页的单目标，比如目标网页的标题，此时无需定义target_item get_items：获取网页的多目标，比如当前目标网页Hacker News中的title和url一共有30个，这时就必须定义target_item来寻找多个目标块；target_item的作用就是针对这样的工作而诞生的，开发者只要定义好这个属性（此时Ruia会自动获取网页中30个target_item），然后每个target_item里面包含的title和url就会被提取出来 第二步：测试Item Ruia为了方便扩展以及自由地组合使用，本身各个模块之间耦合度是极低的，每个模块都可以在你的项目中单独使用；你甚至只使用ruia.Item、Ruia.TextField和ruia.AttrField来编写一个简单的爬虫\n第三步：编写Spider Ruia.Spider是Ruia框架里面的核心控制类\n控制目标网页的请求 Ruia.Request和 Ruia.Response 可加载自定义钩子,插件以及相关配置等,让开发效率更高 开发者实现 HackerNewsSpider 必须是 Spider的子类，代码出现的两个方法 Spider内置：\nparse：此方法是Spider的入口，每一个start_urls的响应必然会被parse方法捕捉并执行； process_item:此方法作用是抽离出对Item提取结果的处理过程，比如这里会接受自定义Item类作为输入，然后进行处理持久化到文件。 第四步：运行Start 如果你想在异步函数里面调用，执行**await HackerNewsSpider.start()**即可\n第五步：扩展 Middleware的目的是对每次请求前后进行一番处理 Ruia已经专门编写了一个名为 ruia-ua的插件来为开发者提升效率。示例代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from ruia import AttrField, TextField, Item, Spider from ruia_ua import middleware class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#39;tr.athing\u0026#39;) title = TextField(css_select=\u0026#39;a.storylink\u0026#39;) url = AttrField(css_select=\u0026#39;a.storylink\u0026#39;, attr=\u0026#39;href\u0026#39;) class HackerNewsSpider(Spider): start_urls = [\u0026#39;https://news.ycombinator.com/news?p=1\u0026#39;, \u0026#39;https://news.ycombinator.com/news?p=2\u0026#39;] async def parse(self, response): # Do something... print(response.url) if __name__ == \u0026#39;__main__\u0026#39;: HackerNewsSpider.start(middleware=middleware) MongoDB 数据持久化，如果想将数据持久化到数据库（MongoDB）中，该怎么做？此时就到了凸显Ruia插件优势的时候了，你只需要安装 ruia-motor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from ruia_motor import RuiaMotorInsert, init_spider from ruia import AttrField, Item, Spider, TextField class HackerNewsItem(Item): target_item = TextField(css_select=\u0026#34;tr.athing\u0026#34;) title = TextField(css_select=\u0026#34;a.storylink\u0026#34;) url = AttrField(css_select=\u0026#34;a.storylink\u0026#34;, attr=\u0026#34;href\u0026#34;) class HackerNewsSpider(Spider): start_urls = [f\u0026#34;https://news.ycombinator.com/news?p={index}\u0026#34; for index in range(3)] concurrency = 3 # aiohttp_kwargs = {\u0026#34;proxy\u0026#34;: \u0026#34;http://0.0.0.0:1087\u0026#34;} async def parse(self, response): async for item in HackerNewsItem.get_items(html=await response.text()): yield RuiaMotorInsert(collection=\u0026#34;news\u0026#34;, data=item.results) async def init_plugins_after_start(spider_ins): spider_ins.mongodb_config = {\u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 27017, \u0026#34;db\u0026#34;: \u0026#34;ruia_motor\u0026#34;} init_spider(spider_ins=spider_ins) if __name__ == \u0026#34;__main__\u0026#34;: HackerNewsSpider.start(after_start=init_plugins_after_start) 入门指南 概览 Ruia是一个基于 asyncio和 aiohttp的异步爬虫框架，理念：\n更少的代码：能通用的功能就插件化，让开发者直接引用即可 更快的速度：由异步驱动 安装 定义Item 运行 Spider 基础概念 Request Request的主要作用是方便处理网络请求，最终返回一个 Request对象\n主要提供的方法有\nRequest().fetch:请求一个网页资源，可以单独使用 Request().fetch_callback:为 Spider类提供的核心方法 核心参数 url:请求的资源连接 method:请求的方法 callback:回调函数 headers:请求头 load_js:目标网页是否需要加载js metadata:跨请求传递的一些数据 request_config:请求配置 request_session: aiohttp的请求 session aiohttp-kwargs:请求目标资源可定义的其他参数 Request通过对 aiohttp和 pyppeteer的封装来实现对网页资源的异步请求\nResponse Respoonse的目的是返回一个统一切友好的响应对象\nurl:请求的资源连接 metadata：跨请求传递的一些数据 html：源网站返回的资源数据 cookies：网站cookie history：访问历史 headers：请求头 status：请求状态码 Item Item的主要作用是定义以及通过一定的规则提取网页中的目标数据，它主要提供了\nget_item:针对页面单目标数据进行提取 get_items:针对页面多目标数据进行提取 Core arguments html:网页源码 url：网页连接 html_etree：etree._Element对象 不论是源网站链接或者网站HTML源码，甚至是经过lxml处理过的etree._Element对象，Item能接收这三种类型的输入并进行处理\n有时你会遇见这样一种情况，例如爬取Github的Issue时，你会发现一个Issue可能对应多个Tag。 这时，将Tag作为一个独立的Item来提取是不划算的， 我们可以使用Field字段的many=True参数，使这个字段返回一个列表。\n最终 Item类会将输入最终转化为etree._Element对象进行处理，然后利用元类的思想将每一个 Field构造的属性计算为原网页上对应的真实数据\nSelector Selector通过Field类实现，为开发者提供了CSS Selector和XPath两种方式提取目标数据，具体由下面两个类实现：\nAttrField(BaseField):提取网页标签的属性数据 TextField(BaseField):提取网页标签的text属性 核心参数 所有 Field共有的参数\ndefault：str；设置默认值，建议定义，否则找不到字段时会报错 many：bool，返回值将是一个列表 AttrField TextField HtmlField共有参数\ncss_select:str，利用 CSS Select提取目标数据 xpath_select：str，利用 XPath提取目标数据 AttrField需要一个额外的参数\nattr：目标标签属性 RegexField需要一个额外的参数\nre_select:str,正则表达式字符串 定好CSS Selector或XPath规则，然后利用lxml实现对目标html进行目标数据的提取\nSpider Spider是爬虫程序的入口，它将Item,Middleware,Request,等模块组合在一起，从而为你构造一个稳健的爬虫程序。你只需要关注以下两个函数：\nSpider.start：爬虫的启动函数 parse：爬虫的第一层解析函数，继承 Spider的子类必须实现这个函数 核心参数： Spider.start的参数\nafter_start:爬虫启动后的钩子函数 before_stop:爬虫启动前的钩子函数 middleware:中间件类，可以是一个中间件 **Middleware()**实例，也可以是一组 **Middleware()**实例组成的列表 loop:事件循环 Spider会自动读取start_urls列表里面的请求链接，然后维护一个异步队列，使用生产消费者模式进行爬取，爬虫程序一直循环直到没有调用函数为止\nMiddleware Middleware的主要作用是在进行一个请求的前后进行一些处理，比如监听请求或者响应：\nMiddleware().request:在请求前处理一些事情 Middleware().response:在请求后处理一些事情 使用中间件有两点需要注意，一个是处理函数需要带上特定的参数，第二个是不需要返回值，具体使用如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from ruia import Middleware middleware = Middleware() @middleware.request async def print_on_request(spider_ins, request): \u0026#34;\u0026#34;\u0026#34; 每次请求前都会调用此函数 request: Request类的实例对象 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;request: print when a request is received\u0026#34;) @middleware.response async def print_on_response(spider_ins, request, response): \u0026#34;\u0026#34;\u0026#34; 每次请求后都会调用此函数 request: Request类的实例对象 response: Response类的实例对象 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;response: print when a response is received\u0026#34;) Middleware通过装饰器来实现对函数的回调，从而让开发者可以优雅的实现中间件功能，Middleware类中的两个属性request_middleware和response_middleware分别维护着一个队列来处理开发者定义的处理函数\n","date":"2024-04-30T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/ruia%E7%AC%94%E8%AE%B0/f5fe3c72432cf9bace81d3d424f1a6a5_hu16972317326397920307.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/ruia%E7%AC%94%E8%AE%B0/","title":"Ruia笔记"},{"content":"Nginx Nginx（发音为“engine X”）是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP代理服务器。它因其稳定性、丰富的功能集、简单的配置以及低资源消耗而广受欢迎。Nginx 的主要作用包括但不限于以下几个方面：\nWeb服务器：Nginx可以作为静态网页的服务器，直接处理并返回用户请求的静态文件（如HTML、CSS、JavaScript、图片等）。由于Nginx对静态文件的处理非常高效，因此它非常适合作为静态资源的服务器。\n反向代理服务器：Nginx最常用的功能之一是作为反向代理服务器。反向代理服务器接收来自客户端的请求，然后将这些请求转发给后端服务器（如Apache、Tomcat等），并将从后端服务器得到的响应返回给客户端。这种方式可以隐藏后端服务器的IP地址，增强安全性，同时实现负载均衡，将请求分发到多个后端服务器上，提高网站的并发处理能力。\n负载均衡器：Nginx支持多种负载均衡算法（如轮询、最少连接、IP哈希等），可以根据后端服务器的负载情况、响应时间等因素，智能地将请求分发到不同的服务器上，以实现资源的合理利用和负载均衡。\nHTTP缓存：Nginx支持HTTP缓存，可以缓存静态内容，减少服务器的请求处理次数和响应时间，提高网站的访问速度。\n支持SSL/TLS协议：Nginx支持SSL/TLS协议，可以提供安全的HTTPS服务，保护用户数据在传输过程中的安全。\n模块化设计：Nginx采用模块化设计，具有高度的可扩展性。用户可以根据需要安装不同的模块，以实现特定的功能，如HTTP/2支持、WebSocket支持、邮件代理服务等。\n高并发处理能力：Nginx采用异步非阻塞的事件驱动模型，能够处理大量的并发连接，非常适合作为高并发网站的服务器。\n资源消耗低：Nginx在资源消耗方面表现优秀，即使在处理大量并发连接时，也能保持较低的CPU和内存使用率。\n综上所述，Nginx因其高性能、稳定性、丰富的功能集和易用性，在Web服务器、反向代理、负载均衡、HTTP缓存等方面发挥着重要作用，是现代互联网架构中不可或缺的一部分。\nnginx 有一个主进程（Master）和几个工作进程（Worker）。主进程的主要目的是读取和评估配置，并维护工作进程。工作进程对请求进行处理。nginx 采用了基于事件模型和依赖于操作系统的机制来有效地在工作进程之间分配请求。工作进程的数量可在配置文件中定义，并且可以针对给定的配置进行修改，或者自动调整到可用 CPU 内核的数量\n配置文件决定了 nginx 及其模块的工作方式。默认情况下，配置文件名为 nginx.conf，并放在目录 /usr/local/nginx/conf，/etc/nginx 或 /usr/local/etc/nginx 中。\n启动，停止和重新加载配置 要启动 nginx，需要运行可执行文件。nginx 启动之后，可以通过调用可执行文件附带 -s 参数 来控制它。 使用以下语法：\nnginx -s 信号\n信号可能是以下之一：\nstop - 立即关闭\nquit - 正常关闭\nreload - 重新加载配置文件\nreopen - 重新打开日志文件\n例如，要等待工作进程处理完当前的请求才停止 nginx 进程，可以执行以下命令：\nnginx -s quit\n这个命令的执行用户应该是与启动nginx用户是一致的\n在将重新加载配置的命令发送到 nginx 或重新启动之前，配置文件所做的内容更改将不会生效。要重新加载配置，请执行：\nnginx -s reload\n一旦主进程（Master）收到要重新加载配置（reload）的信号，它将检查新配置文件的语法有效性，并尝试应用其中提供的配置。如果成功，主进程将启动新的工作进程（Worker），并向旧工作进程发送消息，请求它们关闭。否则，主进程回滚更改，并继续使用旧配置。旧工作进程接收到关闭命令后，停止接受新的请求连接，并继续维护当前请求，直到这些请求都被处理完成之后，旧工作进程将退出\n可以借助 Unix 工具（如 kill 工具）将信号发送到 nginx 进程，信号直接发送到指定进程 ID 的进程。默认情况下，nginx 主进程的进程 ID 是写入在 /usr/local/nginx/logs 或 /var/run 中的 nginx.pid 文件中。例如，如果主进程 ID 为 1628，则发送 QUIT 信号让 nginx 正常平滑关闭，可执行\nkill -s QUIT 1628\n获取所有正在运行的 nginx 进程列表，可以使用 ps 命令，如下\nps -ax | grep nginx\n配置文件结构 nginx 是由配置文件中指定的指令控制模块组成。指令可分为简单指令和块指令。一个简单的指令是由空格分隔的名称和参数组成，并以分号 ; 结尾。块指令具有与简单指令相同的结构，但不是以分号结尾，而是以大括号{}包围的一组附加指令结尾。如果块指令的大括号内部可以有其它指令，则称这个块指令为上下文（例如：events，http，server 和 location）。\n配置文件中被放置在任何上下文之外的指令都被认为是主上下文 main。events 和 http 指令在主 main 上下文中，server 在 http 中，location 又在 server 中。\n井号 # 之后的行的内容被视为注释。\n提供静态内容服务 Web 服务器的一个重要任务是提供文件（比如图片或者静态 HTML 页面）服务。您将实现一个示例，根据请求，将提供来自不同的本地目录的文件： /data/www（可能包含 HTML 文件）和 /data/images（包含图片）。这需要编辑配置文件，在 http 中配置一个包含两个 location 块的 server 块指令。\n首先，创建 /data/www 目录将包含任何文本内容的 index.html 文件放入其中，创建 /data/images 目录然后放一些图片进去。\n其次，打开这个配置文件， 默认配置文件已经包含几个服务器块示例，大部分是已经注释掉的。现在注释掉这些块并且启动一个新的 server 块。\nhttp { server { } } 通常，配置文件可以包含几个由监听 listen 端口和服务器域名 server names 区分的 server 块指令 distinguished。一旦 nginx 决定由哪个 server 来处理请求，它会根据 server 块中定义的 location 指令的参数来检验请求头中指定的URI\n添加如下 location 块指令到 server 块指令中\nlocation / { root /data/www; } 该 location 块指令指定 / 前缀与请求中的 URI 相比较。对于匹配的请求，URI 将被添加到根指令 root 中指定的路径，即 /data/ www，以形成本地文件系统上所请求文件的路径。如果有几个匹配上的 location 块指令，nginx 将选择具有最长前缀的 location 块。上面的位置块提供最短的前缀，长度为 1，因此只有当所有其它 location 块不能匹配时，才会使用该块。\n接下来，添加第二个 location 指令快：\nlocation /images/ { root /data; } 以 /images/ 为开头的请求将会被匹配上（虽然 location / 也能匹配上此请求，但是它的前缀更短）\n最后，server 块指令应如下所示：\nserver { location / { root /data/www; } location /images/ { root /data; } } 这已经是一个监听标准 80 端口并且可以在本地机器上通过 http://localhost/ 地址来访问的有效配置。响应以 /images/ 开头的URI请求，服务器将从 /data/images 目录发送文件。例如，响应http://localhost/images/example.png 请求，nginx 将发送 /data/images/example.png 文件。如果此文件不存在，nginx 将发送一个404错误响应。不以 / images/ 开头的 URI 的请求将映射到 /data/www 目录。例如，响应 http://localhost/some/example.html 请求，nginx 将发送 /data/www/some/example.html 文件。\n如果运行的效果没有在预期之中，您可以尝试从 /usr/local/nginx/logs 或 /var/log/ nginx 中的 access.log 和 error.log 日志文件中查找原因。\n设置一个简单的代理服务器 nginx 的一个常见用途是作为一个代理服务器，作用是接收请求并转发给被代理的服务器，从中取得响应，并将其发送回客户端。\n我们将配置一个基本的代理服务器，它为图片请求提供的文件来自本地目录，并将所有其它请求发送给代理的服务器。在此示例中，两个服务器在单个 nginx 实例上定义。\n首先，通过向 nginx 的配置文件添加一个 server 块来定义代理服务器，其中包含以下内容\nserver { listen 8080; root /data/up1; location / { } } 这是一个监听 8080 端口的简单服务器（以前，由于使用了标准 80 端口，所以没有指定 listen 指令），并将所有请求映射到本地文件系统上的 /data/up1 目录。创建此目录并将 index.html 文件放入其中。请注意，root 指令位于 server 上下文中。当选择用于处理请求的 location 块自身不包含 root 指令时，将使用此 root 指令。\n接下来，在上一节中的服务器配置基础上进行修改，使其成为代理服务器配置。在第一个 location 块中，使用参数指定的代理服务器的协议，域名和端口（在本例中为 http://localhost:8080）放置在 proxy_pass 指令处：\nserver { location / { proxy_pass http://localhost:8080; } location /images/ { root /data; } } 我们将修改使用了 /images/ 前缀将请求映射到 /data/images 目录下的文件的第二个 location 块，使其与附带常见的图片文件扩展名的请求相匹配。修改后的 location 块如下所示：\nlocation ~ \\.(gif|jpg|png)$ { root /data/images; } 该参数是一个正则表达式，匹配所有以.gif，.jpg 或 .png 结尾的 URI。正则表达式之前应该是 ~。相应的请求将映射到 /data/images 目录。\n当 nginx 选择一个 location 块来提供请求时，它首先检查指定前缀的 location 指令，记住具有最长前缀的 location，然后检查正则表达式。如果与正则表达式匹配，nginx 会选择此 location，否则选择更早之前记住的那一个。\n代理服务器的最终配置如下：\nserver { location / { proxy_pass http://localhost:8080/; }\nlocation ~ \\.(gif|jpg|png)$ { root /data/images; } }\n此 server 将过滤以 .gif，.jpg 或 .png 结尾的请求，并将它们映射到 /data/images 目录（通过向 root 指令的参数添加 URI），并将所有其它请求传递到上面配置的代理服务器。\n设置FastCGI代理 nginx 可被用于将请求路由到运行了使用各种框架和 PHP 等编程语言构建的应用程序的 FastCGI 服务器。\n与 FastCGI 服务器协同工作的最基本的 nginx 配置是使用 fastcgi_pass 指令而不是 proxy_pass 指令，以及 fastcgi_param 指令来设置传递给 FastCGI 服务器的参数。假设 FastCGI 服务器可以在 localhost:9000 上访问。以上一节的代理配置为基础，用 fastcgi_pass 指令替换 proxy_pass 指令，并将参数更改为 localhost:9000。在 PHP 中，SCRIPT_FILENAME 参数用于确定脚本名称，QUERY_STRING 参数用于传递请求参数。最终的配置将是：\nserver { location / { fastcgi_pass localhost:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; }\nlocation ~ \\.(gif|jpg|png)$ { root /data/images; } }\n这里设置一个 server，将除了静态图片请求之外的所有请求路由到通过 FastCGI 协议在 localhost:9000 上运行的代理服务器。\n","date":"2024-04-26T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/nginx%E7%AC%94%E8%AE%B0/f48d72dbe89cb374fafa2450e23b0dc409e89c86_hu7420129695037075187.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/nginx%E7%AC%94%E8%AE%B0/","title":"Nginx笔记"},{"content":"Python模块和包管理 模块和包 在 Python 中每一个 .py 都可以视作一个模块（module），而每一个包含 init.py 的目录则可以视作包（packge）。\n如下所示，packs 因为包含 init.py 而可以看做是一个包，而 packs/one.py 则是一个模块\n├── main.py ├── packs │ ├── __init__.py │ └── one.py └── readme.md 在 one.py 中简单定义了一个函数 One：\ndef One(): print(\u0026quot;module one\u0026quot;) def OneOne(): print(\u0026quot;module one/one\u0026quot;) 如果我们想要在main.py中使用的话，那么可以直接import，这种方式也称之为“模块导入”\nimport packs packs.one.One() 这里使用的同时加上了模块名称，当然可以省略模块名，而这种方式称之为“成员导入”，这种方式比之前一种方式要快一些\nfrom packs.one import One One() 当我们运行后就会打印 module one，另外也可以看到在 packs 目录下生成一个 pycache 的新目录，这是编译后中间文件，可以提升模块载入速度。\n如果当前模块有多个 One 名称导入的话，可以使用别名进行区分。如果有多个导入名称的话可以使用逗号进行分隔。\nfrom packs.one import One as Alias, OneOne Alias() OneOne() 如果要在一行导入的名称过多，也可以分行写\nfrom packs.one import One from packs.one import OneOne One() OneOne() 当然也可以全部导入到当前模块，不过注意这种方式可能存在命名冲突，并且在一些 linter 工具下会提示必须使用全部的导入名称。\nfrom packs.one import * One() OneOne() 除了这种全局导入外，还可以在局部作用域导入。\ndef main(): from packs.one import One One() main() init文件 这里的__init__.py文件每当导入当前包的模块的时候就会运行一次\n现在在 packs/init.py 中加入一行 print(\u0026ldquo;packs one imported\u0026rdquo;) 的语句，然后运行，可以发现 init.py 是首先运行的\n根据这个特点，我们可以再 init.py 输入导出的模块，外部使用的就不需要很长的导入路径了。\n修改 packs/__init__py 为如下所示：\nfrom .one import One print(\u0026quot;packs one imported\u0026quot;) 那么在 main.py 中就可以这样使用：\nfrom packs import One One() 或者\nimport packs packs.One() 在深入一些，我们在 packs 文件夹下新建一个 two 包，然后修改 main.py 并导入这个包，类如下面\n$ tree . . ├── main.py ├── packs │ ├── __init__.py │ ├── one.py │ └── two │ ├── __init__.py │ └── two.py └── readme.md 2 directories, 6 files $ cat packs/two/__init__.py print(\u0026quot;packs two imported\u0026quot;) $ cat packs/two/two.py def Two(): print(\u0026quot;module two\u0026quot;) $ cat main.py from packs.two.two import Two Two() 可以看到两个 init 都被运行了，但是我们还不能使用 packs.one.One 这个函数，因为在 main.py 并没有导入这个名称。\n相对路径和绝对路径引用 上面使用类似这种带有相对路径的导入路径from .XXX，这种代表从当前的xx模块中导入名称，如果想要在packs/two/two.py 中使用上一层的 packs/one.py，就可以使用from ..one import One的方式\n# packs/two/__init__.py from ..one import One One() 以此类推，那么 \u0026hellip; 代表更上一级。\n但是这种方式还是有问题的，如果项目深度太大就容易写太多 .，还有一种方式就是绝对路径引用，这里的绝对路径是指相对项目根目录而言的，比如上述例子，那么就要修改为：\n# packs/two/__init__.py from packs.one import One One() 那引用当前目录的模块必须使用相对路径了，比如上述例子:\n# packs/two/__init__.py from packs.one import One from .two import Two One() 注意，这里不能是 from two import Two 的形式！这个也好理解，因为绝对路径不是 . 开始的，如果相对路径不使用 . 开始，那么就得从项目根目录开始找了。\n当然绝对路径的模块，就有一个 base 路径，所有的文件都是相对此 base 目录，如果在 IDE 中直接打开这里的模块，模块的根目录就是当前模块，显然就会提示找不到了对应的模块了。\n模块搜索顺序 自己写的包名肯定可能和第三方或者标准库同名，不过这种同名通常没有问题。因为python会优先在当前目录搜索然后在环境变量的搜索路径，之后才是标准库和第三方包\n这个和linux$PATH的环境变量一样，按照顺序来搜索，一旦导入每个模块就有全局的命名空间，第二层再次加载就会使用缓存\n这个路径搜索方式和 nodejs 有些区别，nodejs 是一旦同名，优先标准库，如果自定义一个 http 模块，那么永远不会被加载。\npip包管理工具 Python之所以受欢迎不光是因为它简单易学，更重要的是它有成千上万的宝藏库。这些库相当于是已经集成好的工具，只要安装就能在Python里使用。它们可以处理各式各样的问题，无需你再造轮子，而且随着社区的不断更新维护，有些库越来越强大，几乎能媲美企业级应用。那么这些工具库怎么下载安装呢？它们被放在一个统一的“仓库”里，名叫PyPi（Python Package Index），所有的库安装都是从这里调度。有了仓库之后，还需要有管理员，pip就是这样一个角色。\npip是一个工具，用它可以来管理 Python 标准库中其他的包，允许你安装和管理不属于 Python 标准库的其它软件包，其提供了对 Python 包的查找、下载、安装、卸载等功能。总的来说，pip的Python第三方库的大管家，搞懂它，会让你省很多事。从Python 3 \u0026gt;= Python 3.4 、Python2 \u0026gt;= Python2.7.9 版本开始，pip默认包含在Python的安装程序中，在安装Python时将会自动被安装，省事方便。\n*作者：独泪了无痕 链接\n包管理 # 安装 # 最新版本 pip install Django # 指定版本号 pip install Django==2.0.0 # 最小版本 pip install 'Django\u0026gt;=2.0.0' # 升级包 pip install --upgrade Django # 卸载包 pip uninstall SomePackage # 搜索包 pip search SomePackage # 显示安装包信息 pip show # 查看指定包的详细信息 pip show -f SomePackage # 列出已安装的包 pip list # 查看可升级的包 pip list -o 包依赖项 pip freeze \u0026gt; requirement.txt # 锁版本 pip install -r requirement.txt # 指定安装版本 pip install --user install black # 安装到用户级目录 使用镜像 pip install -r requirements.txt \\\n\u0026ndash;index-url=https://mirrors.aliyun.com/pypi/simple/ \\\n\u0026ndash;extra-index-url https://pypi.mirrors.ustc.edu.cn/simple/\n这条命令的意思是从阿里云的镜像源和中国科技大学的镜像源下载 requirements.txt 文件中列出的 Python 包，并安装到当前的 Python 环境中。这种方式可以提高包下载的速度，特别是在网络环境不稳定或者网络延迟较大的情况下。\n配置pip镜像 对于windows系统，在C:\\Users\\文件夹下的用户目录（例如如果当前用户是Administrator则是C:\\Users\\Administrator）下创建pip文件夹，然后再在此文件夹下创建pip.ini文件，在文件中写入一下内容：\n[global] # 添加默认下载地址，以阿里云源为例 index-url = https://mirrors.aliyun.com/pypi/simple/ [install] # 此参数是为了避免麻烦，否则使用的时候可能会提示不受信任 trusted-host = mirrors.aliyun.com 配置完成后在通过 pip config list 查看pip配置。\n升级pip pip install -U pip 或者sudo easy_install --upgrade pip Pyenv 我们经常会遇到这样的开发需求，比如你手头有多个开发项目，其中项目A要求用python3.7，项目B需要用python3.6，有要求项目A和项目B依赖包相互独立，互不干扰。为了满足这样的开发需求，我们需要在自己的电脑上安装多个Python版本，并且项目之间进行环境隔离。因此，我们要想运行这些项目，在工作电脑上就要安装不同版本的Python。pyenv 是Python版本管理工具，通过系统修改环境变量来实现Python不同版本的切换，利用它可以在同一台电脑上安装多个版本的Python，设置目录级别的Python，还能创建和管理vitual python enviroments。而且所有的设置都是用户级别的操作，不需要sudo命令。\n安装或升级pyenv 在 Windows 系统下安装 pyenv 需要借助 pyenv-win。pyenv-win 是 pyenv 的一个移植版本，专门针对 Windows 平台。下面是详细的安装步骤：\n使用 Git 安装 克隆 pyenv-win 仓库：git clone https://github.com/pyenv-win/pyenv-win.git %USERPROFILE%.pyenv\n配置环境变量 打开系统的环境变量设置界面： 右键点击“此电脑”或“我的电脑”，选择“属性”。 点击“高级系统设置”。 在“系统属性”窗口中，点击“环境变量”。 在“系统变量”部分，找到并编辑 Path 变量，添加以下路径\n%USERPROFILE%\\.pyenv\\pyenv-win\\bin %USERPROFILE%\\.pyenv\\pyenv-win\\shims 保存更改并关闭所有窗口。\n验证安装 pyenv \u0026ndash;version\n使用pyenv安装python 列出所有可用的 Python 版本 pyenv install \u0026ndash;list\n安装指定版本的 Python，例如 3.9.7： pyenv install 3.9.7\n设置全局默认的 Python 版本 pyenv global 3.9.7\n验证 Python 版本 python \u0026ndash;version\nPipenv Pipenv 是 Python 官方推荐的包管理工具，它综合了 virtualenv、pip 和 pyenv 三者的功能，你可以使用 pipenv 这一个工具来安装、卸载、跟踪和记录依赖性，并创建、使用和组织你的虚拟环境。\n安装和升级pipenv pip install pipenv pip install --upgrade pipenv 为项目建立虚拟环境 进入到项目目录中，通过下面的指令为项目创建虚拟环境 $ mkdir pipenv_demo $ cd pipenv_demo $ pipenv \u0026ndash;python 3.9.9 Creating a virtualenv for this project… Pipfile: /Users/dllwh/work/pipenv_demo/Pipfile Using /Users/dllwh/.pyenv/versions/3.9.9/bin/python3 (3.9.9) to create virtualenv… ⠙ Creating virtual environment\u0026hellip;Using base prefix \u0026lsquo;/Users/dllwh/.pyenv/versions/3.7.7\u0026rsquo; New python executable in /Users/dllwh/.local/share/virtualenvs/pipenv_demo-RYMSREda/bin/python3 Also creating executable in /Users/dllwh/.local/share/virtualenvs/pipenv_demo-RYMSREda/bin/python Installing setuptools, pip, wheel\u0026hellip; done. Running virtualenv with interpreter /Users/dllwh/.pyenv/versions/3.7.7/bin/python3\n✔ Successfully created virtual environment! Virtualenv location: /Users/dllwh/.local/share/virtualenvs/pipenv_demo-RYMSREda 上面的操作，给pipenv_demo这个项目初始化了一个 Python 3.9.9 的虚拟环境，并在项目录下生成一个项目依赖包文件 Pipefile。如果系统中没有 3.9.8 版本的Python，pipenv 会调用 pyenv 来安装对应的 Python 的版本。默认地，虚拟环境会创建在 ~/.local/share/virtualenvs目录里面。我们也可以通过 pipenv \u0026ndash;venv查看项目的虚拟环境目录。可以通过 pipenv \u0026ndash;rm 删除虚拟环境。\n用Pipenv管理依赖包 pipenv使用 Pipfile 和 Pipfile.lock 来管理依赖包，并且在使用pipenv添加或删除包时，自动维护 Pipfile 文件，同时生成 Pipfile.lock 来锁定安装包的版本和依赖信息。相比pip需要手动维护requirements.txt 中的安装包和版本，具有很大的进步。\n安装依赖包 为项目安装依赖包到虚拟环境中，使每个项目拥有相互独立的依赖包，是非常不错的Python的开发实践。安装依赖包到虚拟环境中的方法： pipenv install pytest\n执行完上面的命令后，检查一下是否安装成功： pipenv graph\n观察项目的根目录下，又多了一个 Pipfile.lock 文件。这两个文件记录了此项目的依赖包，这两个文件的区别是 Pipfile 中安装的包不包含包的具体版本号，而Pipfile.lock 是包含包的具体的版本号的。如果不想产生 Pipfile.lock 文件，在安装依赖包的时候，加上 –skip-lock 选项即可。 在使用pipenv的时候，常常会安装过程比较慢，这个是因为pipenv创建的 Pipfile 中默认的Pypi源是python官方的 pypi.python.org/simple。\n删除依赖包 如果是要删除虚拟环境中的第三方包，执行： pipenv uninstall pytest\n安装项目所有的依赖包 用git管理项目时候，要把Pipfile和Pipfile.lock加入版本跟踪。这样clone了这个项目的同学，只需要执行：\npipenv install 就可以安装所有的Pipfile中 [packages]部分列出来的包了，并且自动为项目在自己电脑上创建了虚拟环境。\n安装pipefile.lock中的依赖包 上面的方法都是安装Pipfile中列出来的第三方包的最新版本，如果是想安装Pipfile.lock中固定版本的第三方依赖包，需要执行：\npipenv install --ignore-pipfile 安装requirements.txt里面的依赖包 如果项目之前使用requirements.txt来管理依赖的，那么使用pipenv安装所有依赖可以采用类似pip的方法\npipenv install -r requirements.txt\n使用虚拟环境开发 虚拟环境创建好了之后，就可以在里面进行开发了。如果在命令行下开发，则在项目目录下执行 pipenv shell，就进入到了虚拟环境中，在这个环境中，已经包含安装过的所有依赖包了，接下来就可以利用这些依赖包进行开发工作了。如果是用Pycharm进行开发，就更简单了，直接用Pycharm打开项目即可。可以从Pycharm中的左侧导航栏里面看到External Libraries显示的是虚拟环境中的Python解释器了。 在虚拟环境中执行开发好的程序，有两种方式，一种是前面提到的先执行pipenv shell进入到虚拟环境后，再执行python程序；另一种方式，则是执行pyenv run，比如在虚拟环境中执行基于pytest框架编写的测试用例，只需要执行下面的命令即可：\npipenv run py.test ","date":"2024-04-12T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/python%E6%A8%A1%E5%9D%97%E5%92%8C%E5%8C%85%E7%AE%A1%E7%90%86/2e39b9e527ae73e37e51f2051e2a53bc_hu3055649958681251570.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/python%E6%A8%A1%E5%9D%97%E5%92%8C%E5%8C%85%E7%AE%A1%E7%90%86/","title":"Python模块和包管理"},{"content":"环境管理 使用Anaconda和Pipenv共同管理Python项目环境\nAnaconda包含了许多常用的Python库,工具和环境,使得安装和管理这些工具变得更加容易.它包括了Python解释器本身以及许多流行的科学计算库,比如NumPy,SciPy,Pandas,Matplotlib等.此外,Anaconda还提供了一个名为Conda的包管理器,用于安装,更新和管理这些库,以及创建和管理不同的Python环境.\n对于Python新手来说,Anaconda是一个很好的选择,因为它提供了一个完整的Python环境,无需手动安装和配置每个库.你只需安装Anaconda,就可以开始编写Python代码并使用各种科学计算工具了.\nPipenv 是 Python 项目的包管理器和虚拟环境管理器.它旨在提供一种简单的方式来创建和管理项目的依赖项,并确保这些依赖项在不同环境中的一致性\n下面是 Pipenv 的一些主要功能:\n虚拟环境管理:Pipenv 可以为每个项目创建独立的虚拟环境,这意味着每个项目都有自己的 Python 解释器和依赖项,而不会干扰系统中的其他项目.\n依赖项管理:Pipenv 使用 Pipfile 和 Pipfile.lock 文件来管理项目的依赖项.Pipfile 是项目的声明性文件,用于指定所需的包和版本,而 Pipfile.lock 文件则记录了确切的依赖项版本,以确保在不同环境中的一致性.\n自动环境激活:当你进入项目目录时,Pipenv 会自动激活该项目的虚拟环境,这意味着你可以立即开始使用项目的依赖项而无需手动激活虚拟环境.\n命令行工具:Pipenv 提供了一组简单的命令行工具,用于安装,更新和删除依赖项,以及管理虚拟环境.\n胎教版:\n当你开始一个新的 Python 项目时,你需要安装各种各样的库来帮助你完成任务.Pipenv 就像一个保姆,帮助你照顾这些库,确保它们安装正确且不会互相冲突. 想象一下你在一个大杂货店里购物,每个库就像是你要买的商品.Pipenv 就是你的购物清单和购物车.你列出你要买的东西(依赖库),然后 Pipenv 帮你把它们一个一个加到你的购物车里(项目环境). 同时,Pipenv 还能确保你购物车里的东西都是正确的版本,就像是每个商品上都有一个标签标明它的准确型号和规格一样.这样,当你需要重建你的购物车时,你就可以确保每个东西都是一样的. 最后,Pipenv 还能帮你管理购物车的整个过程.它会告诉你什么时候需要买新的东西(更新库),或者什么时候需要把一些东西扔掉(删除库). 总的来说,Pipenv 就像是你的 Python 项目的好帮手,确保你所需的库都安装正确,没有任何混乱 师兄画的图 编码标准 对于编码标准,推荐跟着社区的PEP8走\nPEP8 是 Python Enhancement Proposal 8 的缩写,它是 Python 社区中的一项规范,用于指导 Python 代码的编写风格和格式.PEP8 旨在提高代码的可读性,并促进代码在不同项目和团队之间的一致性.\n以下是 PEP8 涵盖的一些主要方面:\n缩进:使用 4 个空格作为缩进的标准,而不是制表符. 行长度:每行代码应尽量控制在 79 个字符以内,但可以允许最多 99 个字符. 命名约定:变量名应该采用小写字母,单词之间使用下划线分隔(例如,my_variable),而类名应采用驼峰命名法(例如,MyClass). 空格:在逗号,冒号,分号等符号后应添加一个空格,但在圆括号内部不需要添加空格. 导入:每个导入应该独立一行,避免使用通配符导入(例如,from module import *). 注释:使用适当的注释来解释代码的作用和目的,注释应该清晰,简洁且易于理解. 代码布局:遵循统一的代码布局规范,包括适当的缩进,空行和代码段分隔. pep8链接\n规范化 black:代码格式化 Black 是一个自动化代码格式化工具,它可以帮助你自动将 Python 代码按照一致的规范进行格式化,从而使得代码风格更加统一,减少手动调整格式的工作量.\npylint:代码检查 Pylint 是 Python 中一种静态代码分析工具,它的主要作用是检查 Python 代码中的语法错误,代码风格问题以及潜在的逻辑错误,并提供相应的建议和警告,以帮助开发者编写更加规范,可靠和易于维护的代码.\n语法错误检查:Pylint 可以检测出代码中的语法错误,并指出出错的位置和可能的原因. 代码风格检查:Pylint 可以根据 PEP8 等 Python 代码规范,检查代码中的格式问题,如缩进,空格,命名规范等,并提供相应的建议. 代码质量检查:Pylint 可以评估代码的质量,并提供一些度量指标,如代码的复杂度,重复代码等,帮助开发者识别出代码中的潜在问题. 错误和警告提示:Pylint 可以检测出代码中的潜在逻辑错误,不安全的操作,未使用的变量等,并给出相应的警告或错误提示,帮助开发者尽早发现并修复问题. isort:导包规划化 isort 的作用就是帮助开发者自动将导入语句按照指定的规范进行排序和格式化,以保持代码的清晰易读,并且符合约定俗成的编码规范.\n导入语句排序:isort 可以自动将导入语句按照特定的顺序排序,例如将标准库模块,第三方库模块和本地模块分组,并按字母顺序排列. 分组和对齐:isort 可以将导入语句分组,并且对齐每个组内的导入语句,使得代码结构更加清晰易读. 移除重复导入:isort 可以检测并移除重复的导入语句,避免在代码中出现不必要的重复导入. 自定义配置:isort 提供了丰富的配置选项,可以根据项目的需要进行定制,包括导入顺序,分组规则,对齐方式等. flake8(mccabe):代码复杂度检查 flake8是一个Python工具,用于检查你的代码是否符合PEP 8样式指南 Flake8 是 Python 中常用的代码质量检查工具之一,而 mccabe 则是 Flake8 的一个插件,用于检测代码中的复杂度.\n使用方法: 终端激活虚拟环境 activate 环境名 进入所需项目 cd 路径 进行pipenv初始化 进行对应的工具处理\n文件作用 setup.cfg setup.cfg文件是一个配置文件,用于存储与项目相关的各种设置\nPipfile.lock 是由Pipenv生成的一个文件,用于锁定项目的依赖包的精确版本.这个文件确保了你的项目在不同的环境和时间点上都能使用相同版本的依赖,从而避免了因为依赖版本不一致导致的问题\nPipfile 是由Pipenv工具使用的一个文件,用于管理Python项目的依赖.它类似于requirements.txt文件,但提供了更强大的功能,例如锁定依赖的精确版本,区分开发和生产环境的依赖,以及自动管理虚拟环境.\n[[source]]:这个部分定义了包的源.在这个例子中,包的源被设置为PyPI.\n[packages]:这个部分列出了项目的主要依赖.在这个例子中,项目依赖了requests,lxml,black,isort,pylint,flake8这几个包.\n[dev-packages]:这个部分列出了项目的开发依赖.这些依赖只在开发环境中需要,例如测试工具,代码格式化工具等.在这个例子中,项目的开发依赖包括black,pylint,isort,pytest.\n[requires]这个部分定义了项目需要的Python版本.在这个例子中,项目需要Python 3.10.\n[scripts]:在Pipfile中用于定义一些自定义的命令.这些命令可以是任何你想要的shell命令,它们可以帮助你自动化一些常见的任务,例如运行测试,构建项目,或者启动开发服务器\n当你运行pipenv install命令时,Pipenv会查看Pipfile,然后安装列出的所有依赖.如果你运行pipenv install \u0026ndash;dev,Pipenv还会安装[dev-packages]中列出的依赖.\n[packages]和[dev-packages]两个部分都是用来列出项目的依赖的,但它们的用途有所不同.\n[packages]部分列出的是项目运行时需要的依赖.这些依赖是项目的核心部分,无论是在开发环境还是在生产环境,都需要这些依赖.例如,如果你的项目是一个Web应用,那么你可能需要像Flask或Django这样的Web框架,这些依赖应该被列在[packages]部分.\n[dev-packages]部分列出的是只在开发环境中需要的依赖.这些依赖在生产环境中通常是不需要的.例如,你可能需要像pytest这样的测试框架来运行你的测试,或者需要像flake8或black这样的工具来检查你的代码格式,这些依赖应该被列在[dev-packages]部分.\n当你运行pipenv install命令时,Pipenv会安装[packages]部分列出的所有依赖.如果你运行pipenv install \u0026ndash;dev命令,Pipenv会额外安装[dev-packages]部分列出的依赖.这样,你可以在开发环境中安装所有的依赖,而在生产环境中只安装必要的依赖,从而减少生产环境的复杂性和安全风险.*\nDockerfile 这个文件是一个Dockerfile,它是一种文本文档,用于自动化Docker镜像的构建过程.Dockerfile中的每一行都代表在镜像中执行的一条命令.\nDocker作用:\nDocker是一个开源的应用容器引擎,它允许开发者将应用及其依赖打包到一个可移植的容器中,然后发布到任何流行的Linux或Windows机器上,也可以实现虚拟化.容器是完全使用沙箱机制,相互之间不会有任何接口.\n以下是Docker的一些主要用途:\n提供一致的环境:Docker可以确保在不同的环境中(开发,测试,生产等)运行相同的软件和配置.\n简化配置:Docker使得配置应用环境变得更加简单.你只需要编写一个Dockerfile或者docker-compose.yml文件,就可以定义你的整个环境.\n隔离应用:每个Docker容器都在其自己的环境中运行,这意味着你可以在同一台机器上运行多个不同版本的同一软件,而不会有冲突.\n微服务架构:Docker非常适合微服务架构,因为它允许你为每个服务创建一个单独的容器.\n持续集成/持续部署(CI/CD):Docker可以与CI/CD工具(如Jenkins,GitLab CI等)集成,使得自动化构建,测试和部署变得更加简单.\n快速部署:Docker容器比传统的虚拟机启动得更快,这使得它非常适合需要快速扩展的应用.\n总的来说,Docker是一种使得软件开发,测试和部署变得更加简单和一致的工具.\n.pylintrc 这个文件是一个.pylintrc文件,它是Pylint工具的配置文件.Pylint是一个Python代码静态分析工具,它可以检查代码中的错误,查找不符合PEP 8编码规范的代码,以及查找代码中的其他问题\n总的来说,这个.pylintrc文件定义了Pylint如何检查你的Python代码.你可以根据你的需要修改这个文件,以定制Pylint的行为\n.gitinore 这个文件是一个.gitignore文件,它告诉Git哪些文件或目录不应该被版本控制系统跟踪 在这个.gitignore文件中,每一行都代表一个忽略规则,Git将忽略与这些规则匹配的文件和目录\npycache/和*.py[cod]规则告诉Git忽略Python编译的字节码文件和优化的字节码文件.\n*.so规则告诉Git忽略C扩展的共享对象文件.\nbuild/,dist/,*.egg-info/等规则告诉Git忽略Python打包和分发过程中生成的文件和目录.\n.manifest和.spec规则告诉Git忽略PyInstaller生成的文件.\nhtmlcov/,.tox/,.coverage等规则告诉Git忽略单元测试和覆盖率报告生成的文件和目录.\n总的来说,.gitignore文件帮助你管理你的Git仓库,确保只有需要跟踪的文件被包含在版本控制中.\n.gitattributes 这个文件是一个.gitattributes文件,它用于设置Git仓库的特定行为\n在这个.gitattributes文件中,Dockerfile linguist-language=Python这行告诉GitHub的Linguist工具将Dockerfile文件识别为Python语言.Linguist是GitHub用来在仓库中进行语言检测的工具,它决定了在GitHub仓库概览中显示的语言比例,以及在\u0026quot;搜索\u0026quot;功能中如何对代码进行语法高亮\n通常,Linguist会自动识别文件的语言,但有时你可能希望覆盖这个行为.在这个例子中,即使Dockerfile实际上是Docker的语言,但这行设置让Linguist将其识别为Python语言\n总的来说,.gitattributes文件允许你自定义Git在你的仓库中的行为\n.env 这个文件是一个.env文件,它用于设置环境变量.在这个文件中,PYTHONPATH=${PYTHONPATH}:${PWD}这行代码将当前工作目录(${PWD})添加到PYTHONPATH环境变量中.\nPYTHONPATH是一个环境变量,它告诉Python解释器在哪里查找模块.当你尝试导入一个模块时,Python解释器会在PYTHONPATH中列出的目录中查找这个模块.\n在这个例子中,${PWD}是一个环境变量,它表示当前工作目录.所以,这行代码的效果是将当前工作目录添加到PYTHONPATH中.这意味着你可以直接导入当前工作目录中的Python模块,而不需要将它们移动到Python的安装目录或其他位置.\n总的来说,.env文件允许你设置环境变量,这可以帮助你配置你的应用的运行环境.\n.dockerignore 这个文件是一个.dockerignore文件,它的作用类似于.gitignore文件,但是它是用于Docker的..dockerignore文件告诉Docker在构建镜像时应该忽略哪些文件和目录.\n在这个.dockerignore文件中:\n.idea/:这个规则告诉Docker忽略.idea/目录.这个目录通常包含由JetBrains IDE(如PyCharm)生成的项目设置和配置文件.\n.git/:这个规则告诉Docker忽略.git/目录.这个目录包含Git的版本控制信息.\n.vscode/:这个规则告诉Docker忽略.vscode/目录.这个目录通常包含由Visual Studio Code生成的项目设置和配置文件.\n总的来说,.dockerignore文件帮助你管理你的Docker镜像,确保只有需要的文件被包含在镜像中,从而减小镜像的大小并提高构建速度.\ntest文件夹 在Python项目中,test文件夹通常用于存放所有的测试代码.这些测试代码用于验证你的应用的功能和行为,确保它们按照预期工作.\n以下是test文件夹的一些主要用途:\n单元测试:这些是针对单个函数或类的测试,用于验证它们的行为.例如,你可能有一个测试用例来验证你的函数是否正确地处理边界条件.\n集成测试:这些是针对多个组件或模块的测试,用于验证它们是否能够正确地一起工作.\n回归测试:当你修改或添加新的代码时,你可以运行你的测试套件来确保你没有引入新的错误.\n性能测试:这些测试用于检查你的代码的性能,例如,检查一个函数是否在给定的时间内完成.\n在Python中,你可以使用unittest库来编写测试用例,并使用pytest或nose等工具来运行你的测试.\n总的来说,test文件夹是你的项目的重要组成部分,它帮助你保持你的代码的质量和可靠性.\nsrc文件夹 在Python项目中,src文件夹通常用于存放项目的源代码.这个目录通常包含一个__init__.py文件,这个文件告诉Python这个目录应该被视为一个包\n以下是src文件夹的一些主要用途:\n代码组织:src文件夹提供了一个地方,你可以在其中组织你的Python模块和包.你可以在src文件夹中创建子目录来进一步组织你的代码.\n导入模块:当你的Python文件在src文件夹中时,你可以使用相对导入来导入其他模块.例如,如果你有一个在src/my_package目录中的my_module.py文件,你可以在src/my_package目录中的其他Python文件中使用from . import my_module来导入它.\n测试:将你的源代码放在src文件夹中可以使得测试更加容易.你可以在一个单独的tests文件夹中编写测试,然后在测试中导入src文件夹中的模块.\n总的来说,src文件夹是你的项目的重要组成部分,它帮助你组织和管理你的源代码.\ndocs文件夹 在Python项目中,docs文件夹通常用于存放项目的文档.这些文档可能包括:\n项目说明:对项目的概述,包括它的目的,主要功能,以及如何使用它.\nAPI文档:对项目中的类,函数,方法和模块的详细描述.\n开发者指南:对如何为项目贡献代码的指南,包括代码风格指南,测试策略,以及代码提交流程.\n用户手册:对如何使用项目的详细指南,包括安装指南,教程,以及常见问题解答.\n这些文档可以使用Markdown,reStructuredText,或其他标记语言编写,然后使用工具如Sphinx生成HTML或PDF格式的文档.\n总的来说,docs文件夹是你的项目的重要组成部分,它帮助用户和开发者理解和使用你的项目.\n.vscode文件夹 .vscode文件夹是Visual Studio Code(VS Code)编辑器为特定项目创建的一个配置文件夹.这个文件夹通常包含两个文件:settings.json和launch.json.\nsettings.json:这个文件包含了VS Code的工作区设置.这些设置会覆盖用户级别和全局的VS Code设置.例如,你可以在这个文件中设置特定的Python解释器,或者设置代码格式化工具的参数.\nlaunch.json:这个文件定义了调试配置.例如,你可以在这个文件中设置启动和调试Python程序的参数.\n.vscode文件夹通常会被添加到.gitignore文件中,因为这个文件夹通常包含特定用户或特定环境的设置,这些设置可能不适用于其他用户或环境.\n.pytestcache文件夹 .pytest_cache文件夹是由pytest测试框架自动创建的.pytest在运行测试时使用这个文件夹来存储一些中间数据和测试结果,以便在后续的测试运行中重用.\n以下是.pytest_cache文件夹中可能包含的一些文件和目录:\nv/cache/lastfailed:这个文件包含了上次测试失败的测试用例的信息.pytest可以使用这个信息来首先运行失败的测试用例.\nv/cache/nodeids:这个文件包含了所有测试用例的节点ID.pytest使用节点ID来唯一标识每个测试用例.\nd/:这个目录包含了一些由pytest插件生成的数据.\n通常,你不需要手动修改.pytest_cache文件夹中的内容.这个文件夹通常会被添加到.gitignore文件中,因为它包含的是特定于本地环境的数据,不应该被添加到版本控制系统中\nCACHEDIR.TAG 这个文件名为CACHEDIR.TAG,它是一个缓存目录标签文件.这种文件通常用于标记一个目录被用作缓存,也就是说,该目录中的文件是可以被安全删除的,因为它们可以被重新生成.\n在这个特定的情况下,CACHEDIR.TAG文件是由pytest测试框架创建的,用于标记.pytest_cache目录为缓存目录.\n文件中的Signature: 8a477f597d28d172789f06886806bc55是一个固定的签名,用于标识这个文件是一个缓存目录标签.\n总的来说,CACHEDIR.TAG文件的存在告诉其他工具和用户,.pytest_cache目录中的文件是临时的,可以被安全删除.\n_pycache_文件 __pycache__是Python自动创建的一个目录,用于存储编译后的Python代码,也就是字节码文件.当你运行一个Python程序时,Python解释器会首先将源代码(.py文件)编译成字节码(.pyc文件),然后执行这个字节码.\n字节码文件的主要目的是加速程序的启动.当Python解释器再次运行同一个程序时,如果源代码没有改变,解释器可以直接加载字节码文件,而不需要再次编译源代码.\n__pycache__目录中的文件通常有如下的命名格式:module.version.pyc,其中module是源代码文件的名称,version是Python解释器的版本.\n通常,你不需要手动管理__pycache__目录或其中的文件.Python解释器会自动创建和更新这些文件.这个目录通常会被添加到.gitignore文件中,因为字节码文件是特定于Python解释器的,不应该被添加到版本控制系统中.\n","date":"2024-04-12T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/24366b5b414b2df862acef0a71d899d6d74df8ac_hu6648505284054060823.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/","title":"Python项目管理"},{"content":"Ajax Asynchronous javascript and xml AJAX 不是新的编程语言，而是一种使用现有标准的新方法。 AJAX 最大的优点是在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。 AJAX 不需要任何浏览器插件，但需要用户允许 JavaScript 在浏览器上执行。 XMLHttpRequest 只是实现 Ajax 的一种方式。\n应用：\n运用 XHTML+CSS 来表达资讯； 运用 JavaScript 操作 DOM（Document Object Model）来执行动态效果； 运用 XML 和 XSLT 操作资料; 运用 XMLHttpRequest 或新的 Fetch API 与网页服务器进行异步资料交换； 简介 AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。\nAjax是基于现有的internet标准 AJAX是基于现有的Internet标准，并且联合使用它们：\nXMLHttpRequest对象(异步的与服务器交换数据) JavaScript/DOM(信息显示/交互) CSS (给数据定义样式) XML (作为转换数据的格式) Google Suggest 使用 AJAX 创造出动态性极强的 web 界面：当您在谷歌的搜索框输入关键字时，JavaScript 会把这些字符发送到服务器，然后服务器会返回一个搜索建议的列表。\nXHR创建对象 AJAX-创建XMLHttpRequest 对象 XMLHttpRequest 是 AJAX 的基础。\nXMLHttpRequest 对象 所有现代浏览器均支持 XMLHttpRequest 对象,XMLHttpRequest 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。\nXHR请求 AJAX - 向服务器发送请求 XMLHttpRequest 对象用于和服务器交换数据。\n向服务器发送请求 如需将请求发送到服务器，我们使用 XMLHttpRequest 对象的 open() 和 send() 方法：\n对于 web 开发人员来说，发送异步请求是一个巨大的进步。很多在服务器执行的任务都相当费时。AJAX 出现之前，这可能会引起应用程序挂起或停止。\n通过 AJAX，JavaScript 无需等待服务器的响应，而是：\n在等待服务器响应时执行其他脚本 当响应就绪后对响应进行处理 XHR响应 AJAX - 服务器响应 如需获得来自服务器的响应，请使用 XMLHttpRequest 对象的 responseText 或 responseXML 属性。\nXHR readyState onreadystatechange 事件 当请求被发送到服务器时，我们需要执行一些基于响应的任务。 每当 readyState 改变时，就会触发 onreadystatechange 事件。 readyState 属性存有 XMLHttpRequest 的状态信息。\n使用回调函数 回调函数是一种以参数形式传递给另一个函数的函数 如果您的网站上存在多个 AJAX 任务，那么您应该为创建 XMLHttpRequest 对象编写一个标准的函数，并为每个 AJAX 任务调用该函数。 该函数调用应该包含 URL 以及发生 onreadystatechange 事件时执行的任务（每次调用可能不尽相同）\nAJAX数据库 AJAX 可用来与数据库进行动态通信。\nAJAX XML实例 ","date":"2024-04-10T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/ajax/1_hu12900211400174317745.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/ajax/","title":"Ajax"},{"content":"爬虫基础 “可见即可爬~”\n爬虫概念 如果互联网是一张大的蜘蛛网，那一台计算机上的数据便是蜘蛛网上的一个猎物，而爬虫程序就是一只小蜘蛛，沿着蜘蛛网抓取自己想要的猎物/数据\n网络爬虫也叫网络蜘蛛，特指一类自动批量下载网络资源的程序 网络爬虫是伪装成客户端与服务端进行数据交互的程序\n爬虫的应用 1.数据采集 2.搜索引擎 3.模拟操作 4.软件测试 5.网络安全\n爬虫的分类（略） 爬虫的一般开发流程： 1.最简单的单一页面数据的爬取：\nurl-\u0026gt;发送请求，获取响应-\u0026gt;提取数据-\u0026gt;保存数据\n2.多页面数据的爬取\n发送请求，获取响应——\u0026gt;提取url地址，继续请求\n爬虫开发的重难点 1.数据的获取（反爬） 2.采集的速度\nHTTP和HTTPS 大多数商业应用采用的架构： 1.c/s client server 2.b/s broser(浏览器) server 3.m/s mobile server\n以上统称为客户端与服务端\nHTTP协议（超文本传输协议） 爬取想要的数据前，一定要明确其使用的是什么协议\nHTTP是基于TCP/IP通信协议来传输数据的\nTCP/IP通信的三次握手与四次挥手 三次握手建立连接 client:嘿，服务端girl！我想和你建立连接 server：好呀，嘻嘻 client：真好，那我们开始（数据）交互吧！\n（进行数据交互ing）\n四次挥手断开连接 client：我已经和你交互（数据）完了，我们断开连接吧! server：你确定要断开连接吗？ server：那你断开连接吧 client：欧克，那我断开连接了\nHTTP请求流程： 我们日常用浏览器搜索东西，输入的是URL,浏览器会将其自动转换为HTTP协议\n一次http请求的基本流程，有客户端向服务端发起一次请求（request），而服务器在接收到以后返回给客户端一个响应（response）.所以一次完成的http请求包含请求和响应两部分\n浏览器发送http请求的过程：\n1.域名解析 2.发起TCP的3次握手 3.建立TCP连接后发起HTTP请求 4.服务器响应http请求，浏览器得到html代码 5.浏览器解析html代码，并请求html代码中的资源（js，css，图片等） 6.浏览器对页面进行渲染呈现给用户 tip： 在网页的右键检查network-\u0026gt;name-\u0026gt;request headers view parsed下的connection:keep-alive保持常连接，就不用频繁三次握手和四次挥手了\n浏览器获取的内容（elements的内容）包含：url地址对应的响应+js+css+picture 爬虫会获取：url地址对应的响应\nurl（浏览器搜索框里的内容） 发送http请求时，通过url对网络资源进行定位\nurl：统一资源定位符。用来标识某一处资源的地址，也叫网址\n组成：协议+域名（端口默认80）+路径+参数 http协议的端口号默认为80可以不写，http协议的端口号默认为443可以不写，（域名可以确定是哪一台电脑，而端口号是为了确定是哪台电脑的哪一个应用）\n域名通常是IP地址的映射\nhttp请求格式 客户端发送一个HTTP请求到服务器的请求消息包括一下部分：请求行,请求头，空行和请求数据\n请求方法 分类：\nOPTIONS PUT DELETE TRACE CONNECT 常用方法是GET 和 POST\nGET:负责从服务器获取数据 POST:负责向服务器提交数据\n请求头 http请求正文 请求正文通常是使用POST请求中表单数据，而对于GET请求，请求体则为空\n在爬虫中，如果构造POST，需要正确的content-type，并了解各种请求库的各个参数设置时使用的是哪种content-type，不然可能会导致post提交后无法正常响应\nHTTP响应格式 由四个部分组成，分别是： 状态行（响应行） 消息报头 空行 响应正文\nhttp协议的特点 HTTP是无连接的 HTTP是媒体独立的 HTTP是无状态的\n保持http连接状态的技术是会话和Cookies\nhttps协议，安全版的HTTP http是基于tcp/ip协议的，而https是在http协议的基础之上，再加一层SSL/TLS协议，数据在传输过程中是加密的\nhttp是明文传输的而https是密文传输，所以较安全但性能低\n会话技术 会话在服务端，就是网站的服务器，用来保存用户的会话信息；cookies在客户端，也可以理解为浏览器端\nCookie 指某些网站为了辨别用户身份，进行session跟踪而存储在用户本地终端上的数据（通常经过加密）\nCookie可以理解为一个凭证 1.实际是由服务器发给客户端的特殊信息 2.这些信息以文本文件的方式存放在客户端 3.客户端每次向服务器发送请求的时候都会带上这些特殊信息 4.服务器在接收到Cookie以后，会验证cookie的信息，以此来辨别用户的身份\n爬虫为什么要使用cookie 好处： 能够访问登陆页面 有一定的反爬作用 坏处： 请求太频繁有可能被识别为爬虫 一般使用多账号解决 Session 一个浏览器窗口从打开到关闭这个期间\n在一个客户从打开浏览器到关闭浏览器这个期间，发起的所以请求都可以被识别为同一个用户，session是基于cookie的\n简单的爬虫程序 TODO:\n我的爬虫思路 TODO:\n","date":"2024-04-02T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/0fb747771069b0a5bcb1e8a58d048259_hu9374234730506142729.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/","title":"爬虫笔记"},{"content":"绪论 通常将优化一个模型的过程称为训练或学习 检验模型效果的过程称为测试 对不同参数下的模型表现基涅检验来选择模型参数是开发或验证 损失函数 损失函数（Loss Function）是机器学习和深度学习中用于评估模型预测值与实际值之间差异程度的函数。通过最小化损失函数，我们可以训练模型以更准确地预测结果。不同的学习任务和模型类型会使用不同的损失函数。以下是一些常见的损失函数：\n拟合和过拟合 拟合（Fitting）和过拟合（Overfitting）是机器学习和统计建模中常见的概念，它们描述了模型在训练数据上的表现与在新数据（即测试数据）上表现之间的关系。\n拟合（Fitting） 拟合是指模型在训练数据上学习的过程，目的是使模型能够准确地描述或预测训练数据中的关系或模式。在理想情况下，我们希望模型能够学习到数据的内在规律，而不仅仅是记住训练数据的具体细节。然而，在实际应用中，由于数据的复杂性、噪声以及模型的表达能力等因素，完全准确地拟合所有训练数据往往是不现实的。\n过拟合（Overfitting） 过拟合是指模型在训练数据上表现得过于复杂，以至于它学习了训练数据中的噪声和随机波动，而不是数据背后的真实关系。因此，当模型被应用到新的、未见过的数据时，它的表现会显著下降。过拟合通常发生在模型具有非常高的表达能力（例如，具有大量参数的非线性模型）而训练数据又相对较少时。\n如何识别过拟合 训练集和验证集/测试集的性能差异：如果模型在训练集上的性能（如准确率、损失值）远好于在验证集或测试集上的性能，那么很可能是出现了过拟合。 学习曲线：通过观察训练集和验证集上的损失随训练迭代次数变化的曲线，可以判断是否存在过拟合。如果验证集上的损失在训练过程中开始增加，而训练集上的损失仍在下降，那么很可能是过拟合。 防止过拟合的方法 增加数据量：更多的训练数据可以帮助模型学习到更加一般化的特征，减少过拟合的风险。 简化模型：减少模型的复杂度，例如减少神经网络的层数或神经元数量，可以降低过拟合的风险。 正则化：通过在损失函数中添加正则化项（如L1、L2正则化）来惩罚复杂的模型，从而防止过拟合。 早停法（Early Stopping）：在验证集性能开始下降时停止训练，以避免过拟合。 数据增强：通过生成额外的训练样本来增加数据量，例如通过旋转、缩放、裁剪等方式处理图像数据。 Dropout：在神经网络训练过程中随机丢弃一部分神经元，以减少神经元之间的共适应性，从而防止过拟合。 LDA主题模型简介 LDA（Latent Dirichlet Allocation）主题模型，也称为潜在狄利克雷分布模型，是一种文档主题生成模型，同时也是一种非监督机器学习技术。它基于三层贝叶斯概率模型，包含词、主题和文档三层结构。LDA模型的主要目的是识别文档中的主题，将文档-词汇矩阵转换成文档-主题矩阵（分布）和主题-词汇矩阵（分布）。\nLDA模型的基本思想 LDA模型认为，一篇文档是由多个主题混合而成的，而每个主题又是由多个词汇按照一定概率分布组成的。文档的生成过程可以看作是，首先以一定的概率选择某个主题，然后在这个主题下以一定的概率选择某个词，不断重复这个过程，直到生成整篇文档。LDA的使用则是这个过程的逆过程，即根据一篇得到的文档，去推断出这篇文档的主题以及这些主题所对应的词。\nLDA模型的数学表达 LDA模型通过概率图模型来表示上述的生成过程。具体来说，LDA假设文档到主题的分布服从多项式分布，而主题到词的分布也服从多项式分布。这两个分布的参数（即主题分布和词分布）是LDA模型需要学习的。\nLDA模型的算法流程 LDA模型的算法流程大致如下：\n初始化：随机给文档中的每个词分配一个主题编号。 统计：统计每个主题下每个词出现的次数，以及每个文档中每个主题下词出现的次数。 迭代更新：对于文档中的每个词，根据当前的主题分布和词分布，重新计算该词属于各个主题的概率，并根据这个概率更新该词的主题编号。这个过程会不断重复，直到主题分布和词分布收敛。 LDA模型的参数 LDA模型中有几个重要的参数需要设置，包括：\n主题数量：这是需要预先设定的参数，表示文档集合中将被识别的主题个数。主题数量的选择对模型的效果有很大影响，通常需要通过实验来确定。 迭代次数：表示算法迭代的次数，迭代次数越多，模型可能越稳定，但计算量也会增加。 超参数：包括α（文档-主题分布的先验参数）和β（主题-词分布的先验参数）。这些参数可以通过先验知识来设定，也可以通过模型训练来自动调整。 LDA模型的应用 LDA模型在文本挖掘、信息检索、自然语言处理等领域有广泛的应用。它可以用于文档的聚类、主题识别、特征提取等任务。通过LDA模型，我们可以从大量的文本数据中挖掘出潜在的主题信息，为后续的文本分析提供有力的支持。\nLDA模型的实现 LDA模型的实现通常可以使用一些开源的库和工具，如Gensim、scikit-learn等。这些库提供了LDA模型的实现和训练接口，用户只需要提供文档集合和相应的参数，就可以进行模型的训练和主题识别。\n总的来说，LDA主题模型是一种非常有效的文本挖掘工具，它可以帮助我们从大量的文本数据中挖掘出潜在的主题信息，为后续的文本分析提供有力的支持。\n词干化 词干化（Stemming）是自然语言处理（NLP）中的一种常见文本预处理技术，其目的是将单词缩减为其基本形式或词干，以减少词汇的复杂性并提高文本分析的准确性。以下是关于词干化的详细解释：\n定义 词干化是一种基于规则的文本处理技术，它尝试通过去除单词的后缀来将单词还原到它们的词干或根形式。这通常涉及到简单的字符串操作，如去除常见的后缀（如-ing、-ed、-s等）。词干化可以使不同形式的单词被视为相同的单词，从而简化文本分析。\n原理 词干化技术常常利用词缀规则来确定单词的词干。它不考虑单词的词法和语法，仅仅基于一系列预定义的规则来截断单词，从而得到其词干。这种方法的优点是简单快速，但可能不够精确，因为有时候去除后缀后得到的词干可能不是实际存在的单词。\n适用场景 词干化通常用于快速文本处理，例如信息检索或文档分类。在这些场景中，目标是快速地将不同形式的单词映射到它们的共同词干，以减少不同形式的单词的数量，提高处理效率。\n常用算法 Porter词干化算法：是最早和最常用的词干化算法之一。它通过一系列规则和模式匹配来截断单词的后缀，得到其词干。这个算法在许多自然语言处理任务中广泛使用，尤其是信息检索领域。 Snowball词干化算法（Porter2）：是Porter词干化算法的改进版本，提供了更准确的词干化，同时支持多种语言。它修复了Porter算法中的一些问题，使得词干化结果更加准确。 Lancaster词干化算法：是另一种基于规则的词干化算法，它比Porter算法更加激进，更倾向于将单词截断至更短的形式。它适用于某些任务，但可能会导致一些不常见的单词被切割过度。 示例 以单词“running”为例，经过词干化处理后，其词干为“run”。类似地，“ran”和“runs”等单词也会被还原为词干“run”。\n总结 词干化是自然语言处理中一种重要的文本预处理技术，它通过去除单词的后缀来得到其词干，从而简化文本分析并减少词汇的多样性。虽然词干化技术简单快速，但在某些情况下可能不够精确。因此，在选择是否使用词干化技术时，需要根据具体任务的需求和精确性要求来进行权衡。\n抽取词袋 抽取词袋（Bag-of-Words，简称BoW）是自然语言处理（NLP）中常用的一种文本特征提取方法，用于将文本数据转换为数值表示，从而便于机器学习算法的处理。以下是抽取词袋的基本步骤和要点：\n一、定义与原理 词袋模型的基本思想是将文本看作是由单词构成的“袋子”（即无序集合），然后统计每个单词在文本中出现的频次或使用其他权重方式来表示单词的重要性。这样，每个文本都可以用一个向量表示，其中向量的每个维度对应于一个单词，并记录了该单词在文本中的出现次数或权重。\n二、抽取步骤 文本预处理：\n分词：将文本分割成单词或词语。这通常依赖于特定的分词工具或算法，如正则表达式、机器学习模型等。 去除停用词：停用词是指那些在文本中频繁出现但对文本内容理解帮助不大的词汇，如“的”、“是”等。去除停用词可以减少词袋向量的维度，提高处理效率。 词干提取/词形还原：将单词还原为其基本形式（词干或词根），以减少词汇的多样性。这可以通过词干化算法（如Porter算法）或词形还原算法来实现。 构建词汇表：\n创建一个包含文本数据集中所有唯一词汇的词汇表。这个词汇表包括文本数据集中出现的所有单词，不重复，无顺序。 文本向量化：\n对于每个文本文档，将文档中的每个词汇映射到词汇表中的词汇。这通常涉及将文档中的每个词汇替换为其在词汇表中的索引。 统计每个词汇在文档中的出现次数（词频，TF），或者使用更高级的方法，如TF-IDF（Term Frequency-Inverse Document Frequency）来衡量词汇的重要性。TF-IDF考虑了词汇在文档中的出现频率以及在整个文本数据集中的分布情况，能够更准确地反映词汇对于文档的重要性。 每个文本文档都被表示为一个向量，其中向量的维度等于词汇表的大小。向量的每个元素对应于词汇表中的一个词汇，其值表示相应词汇在文档中的出现次数或其他相关信息（如TF或TF-IDF值）。 三、注意事项 忽略词汇顺序：词袋模型忽略了文档中词汇的语法和语义顺序，因此对于同一组词汇，无论它们出现的顺序如何，都会生成相同的文档向量。这在一定程度上限制了词袋模型在需要理解文本结构和语义关系任务中的应用。 维度灾难：当词汇表非常大时，词袋向量的维度也会非常高，这可能导致“维度灾难”问题，即随着维度的增加，计算复杂度和所需的存储空间急剧增加，同时模型的性能可能会下降。为了缓解这个问题，可以采用特征选择或降维技术来减少向量的维度。 扩展性：词袋模型可以很容易地扩展到大规模文本数据集上，但需要注意处理效率和存储成本的问题。 四、应用场景 词袋模型在文本分类、聚类、信息检索等任务中有广泛的应用。通过将文本数据转换为数值表示，词袋模型为这些任务提供了有效的输入特征，从而支持机器学习算法的训练和预测。\n五、总结 抽取词袋是自然语言处理中一种简单而有效的文本特征提取方法。通过文本预处理、构建词汇表和文本向量化等步骤，可以将文本数据转换为数值表示，为后续的机器学习算法提供输入特征。然而，词袋模型也存在一些局限性，如忽略词汇顺序和可能导致的维度灾难问题。在实际应用中，需要根据具体任务的需求和数据的特性来选择合适的文本表示方法。\n文本向量化 文本向量化是自然语言处理（NLP）中的一项关键技术，它旨在将文本数据（如单词、句子、文档等）转换为数值向量表示，以便计算机能够处理和分析。这种转换过程有助于捕捉文本中的语义信息，提高文本处理任务的效率和准确性。以下是对文本向量化的详细解析：\n一、定义与原理 文本向量化是将文本信息转换为向量表示的过程，这些向量能够表达文本的语义特征。通过向量化，文本数据可以被转化为计算机可处理的数值型数据，进而应用于各种NLP任务中，如文本分类、聚类、信息检索、情感分析等。\n二、主要方法 文本向量化的方法多种多样，以下是一些常见的方法：\n独热编码（One-Hot Encoding）\n独热编码是一种简单的文本向量化方法，它首先构建一个包含所有可能词汇的词典，然后为每个词汇分配一个唯一的索引。对于文本中的每个词汇，将其表示为一个与词典长度相同的向量，其中该词汇对应索引位置上的值为1，其余位置上的值为0。 优点：实现简单，易于理解。 缺点：当词典很大时，向量维度会非常高，导致“维度灾难”问题；且无法表示词汇之间的语义关系。 词袋模型（Bag-of-Words, BoW）\n词袋模型忽略了文本中词汇的顺序和语法结构，仅考虑词汇在文本中出现的频次。它首先将文本分割成词汇，然后统计每个词汇在文本中出现的次数，最后将这些频次作为向量的元素。 优点：实现简单，能够处理可变长度的文本。 缺点：同样存在维度灾难问题；且无法考虑词汇之间的语义关系和上下文信息。 TF-IDF（Term Frequency-Inverse Document Frequency）\nTF-IDF是一种改进的词袋模型，它结合了词频（TF）和逆文档频率（IDF）两个因素来评估词汇在文本中的重要性。TF表示词汇在文本中出现的频次，而IDF则表示词汇在文档集合中的普遍重要性。通过计算TF和IDF的乘积，可以得到词汇的TF-IDF值，从而更准确地表示词汇在文本中的权重。 优点：能够考虑词汇在文档集合中的普遍重要性，减少常见词汇的权重。 缺点：仍然无法考虑词汇之间的语义关系和上下文信息。 词嵌入（Word Embedding）\n词嵌入是一种将词汇映射到低维向量空间中的方法，这些向量能够捕捉词汇之间的语义关系。常见的词嵌入方法包括Word2Vec、GloVe、BERT等。这些方法通常基于大量的文本数据，通过训练神经网络模型来学习词汇的向量表示。 优点：能够捕捉词汇之间的语义关系，提高文本处理任务的准确性和效率。 缺点：需要大量的训练数据和计算资源；且模型的可解释性相对较差。 三、应用场景 文本向量化在NLP领域有着广泛的应用场景，包括但不限于：\n文本分类：将文本数据转换为向量表示后，可以使用分类算法对文本进行分类。 文本聚类：通过计算文本向量之间的相似度，可以将相似的文本聚集成簇。 信息检索：利用文本向量化技术，可以构建高效的搜索引擎，根据用户查询返回相关的文本结果。 情感分析：将文本数据转换为向量表示后，可以使用机器学习算法对文本的情感倾向进行分析。 推荐系统：通过计算用户兴趣和项目内容的向量表示，可以为用户推荐相关的项目或内容。 四、未来趋势 随着NLP技术的不断发展，文本向量化技术也在不断进步。未来，文本向量化技术可能会更加注重以下几个方面：\n语义理解能力：提高文本向量在捕捉语义信息方面的能力，以更好地支持复杂的NLP任务。 跨语言处理能力：开发能够处理多种语言的文本向量化技术，以满足全球化背景下的NLP需求。 高效性与可扩展性：优化文本向量化的算法和模型，以提高处理速度和可扩展性，支持大规模文本数据的处理和分析。 综上所述，文本向量化是NLP领域中的一项重要技术，它通过将文本数据转换为数值向量表示，为各种NLP任务提供了有力的支持。随着技术的不断发展，文本向量化技术将在更多领域发挥重要作用。\n决策树 决策树（Decision Tree）是一种在机器学习和决策分析领域广泛使用的技术，它通过树形结构来表示决策过程，并帮助解决分类、回归等问题。以下是对决策树的详细解析：\n一、定义与原理 决策树是一种通过树形图来表达决策过程中不同方案及其可能结果的图解法。在机器学习中，决策树是一个预测模型，它表示对象属性与对象值之间的一种映射关系。决策树由根节点、内部节点（决策节点）、分支和叶节点组成，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别或决策结果。\n二、基本组成部分 根节点：决策树最上边的节点，表示整个决策过程的起点。 内部节点（决策节点）：代表一个属性或特征的测试，用于根据属性值将数据集分割成不同的子集。 分支：连接节点之间的线，表示不同的决策路径。 叶节点：决策树的末端节点，表示一个类别或决策结果。 三、构建过程 决策树的构建过程通常包括以下几个步骤：\n数据准备：对原始数据进行清洗、变换和相关性分析，以确保数据的质量和适用性。 选择最佳属性：在构建决策树时，需要选择一个属性作为当前节点的测试属性。这通常通过计算信息增益、信息增益率或其他指标来实现。 分割数据集：根据选定的测试属性将数据集分割成不同的子集。 递归构建子树：对每个子集重复上述过程，直到满足停止条件（如子集属于同一类别、没有更多属性可供选择等）。 剪枝：为了避免过拟合，通常需要对生成的决策树进行剪枝处理，即去除一些不必要的分支。 四、经典算法 ID3算法：由J.R.Quinlan在1979-1986年间提出，使用信息增益来选择测试属性。 C4.5算法：对ID3算法的改进，使用信息增益率来选择测试属性，并支持对连续属性的离散化处理和不完整数据的处理。 CART算法：另一种常用的决策树算法，它既可以用于分类也可以用于回归，使用基尼系数作为选择测试属性的标准。 五、优点与缺点 优点 直观易懂：决策树以树形图的形式表示决策过程，易于理解和解释。 计算量小：相对于其他机器学习算法，决策树的计算量较小。 能够处理非线性关系：决策树能够处理复杂的非线性关系。 能够处理缺失值：一些决策树算法（如C4.5）能够处理数据中的缺失值。 缺点 容易过拟合：当决策树过于复杂时，可能会对数据中的噪声进行拟合，导致过拟合现象。 对连续变量处理不佳：传统的决策树算法对连续变量的处理相对较弱。 类别不平衡问题：当数据集中各类别样本数量差异较大时，决策树的性能可能会受到影响。 六、应用场景 决策树算法在多个领域都有广泛的应用，包括但不限于：\n分类问题：如文本分类、图像分类等。 回归问题：预测连续值的问题，如房价预测、股票价格预测等。 信用评估：根据客户的个人信息和贷款申请信息，判断客户的信用等级。 医学诊断：根据患者的症状和检测结果，判断患者可能患有的疾病。 推荐系统：根据用户的历史行为和喜好，预测用户可能感兴趣的物品或内容。 综上所述，决策树是一种功能强大且易于理解的机器学习算法，它通过树形结构来表示决策过程，并广泛应用于分类、回归等多个领域。\n剪枝 预剪枝和后剪枝是决策树算法中常用的两种剪枝技术，用于避免决策树模型的过拟合，提高模型的泛化能力。以下是对这两种剪枝技术的详细解析：\n一、预剪枝（Pre-pruning） 定义与原理： 预剪枝是在决策树生成过程中，对每个结点在划分前进行估计，如果当前结点的划分不能带来决策树模型泛化性能的提升（即验证集精度未提升），则不对当前结点进行划分，并且将当前结点标记为叶结点。\n核心思想： 在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。\n优点：\n降低过拟合风险：预剪枝通过提前停止树的生长，减少了不必要的分支，从而降低了过拟合的风险。 减少训练时间和测试时间：由于决策树的部分分支被提前剪除，因此训练时间和测试时间都会显著减少。 缺点：\n可能导致欠拟合：预剪枝是基于“贪心”策略的，它可能禁止了一些当前划分不能提升泛化性能但后续划分可能显著提升性能的分支的展开，从而带来欠拟合的风险。 参数选择困难：预剪枝需要设定一些停止划分的阈值（如熵减小的阈值），这些阈值的选择往往依赖于经验或交叉验证，具有一定的主观性。 二、后剪枝（Post-pruning） 定义与原理： 后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对树中的所有非叶节点进行逐一考察，如果将该结点对应的子树换为叶结点能够带来泛化性能的提升（即验证集精度提升），则把该子树替换为叶结点。\n核心思想： 在决策树完全生成后，通过比较剪枝前后的分类精度来决定是否进行剪枝。\n优点：\n泛化性能通常优于预剪枝：后剪枝决策树通常保留了更多的分支，能够更充分地利用数据中的信息，因此其泛化性能往往优于预剪枝决策树。 欠拟合风险小：后剪枝是在决策树完全生成后进行的，因此不会因为提前停止树的生长而导致欠拟合。 缺点：\n训练时间开销大：后剪枝需要生成完整的决策树后再进行剪枝操作，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大很多。 剪枝过程复杂：后剪枝需要自底向上地对树中的所有非叶节点进行逐一考察，并计算剪枝前后的分类精度，因此其剪枝过程相对复杂。 三、总结 预剪枝和后剪枝都是决策树算法中重要的剪枝技术，它们各有优缺点。在实际应用中，可以根据具体问题的特点和数据集的特性来选择合适的剪枝方法。例如，如果数据集较小或模型的训练时间受限，可以考虑使用预剪枝；如果数据集较大且对模型的泛化性能要求较高，可以考虑使用后剪枝。\n支持向量机 支持向量机（Support Vector Machine, SVM）是一种基于统计学习理论的监督学习模型，主要用于分类和回归分析。以下是对支持向量机的详细解析：\n一、基本概念 定义：支持向量机是一类按监督学习(supervised learning)方式对数据进行二元分类的广义线性分类器(generalized linear classifier)，其决策边界是对学习样本求解的最大边距超平面(maximum-margin hyperplane)。\n提出者：V.N. Vapnik, A.Y. Chervonenkis, C. Cortes等。\n提出时间：1964年。\n二、工作原理 超平面分割：SVM通过寻找一个超平面来对样本进行分割，分割的原则是间隔最大化。这个超平面由支持向量确定，支持向量是离超平面最近的样本点。 间隔最大化：SVM的目标就是最大化这个间隔值，这样可以使得分类器对于新的、未见过的样本有更好的泛化能力。 核方法：当遇到线性不可分的样例时，通常的做法是将样例特征映射到高维空间中去。尽管这样做可能会导致维度变得非常高，但通过使用核函数，可以在低维空间进行计算，而将实质上的分类效果表现在高维空间，从而避免了直接在高维空间中的复杂计算。 三、核函数 核函数是SVM中的关键部分，它决定了数据从低维空间映射到高维空间的方式。常见的核函数包括：\n线性核函数：简单，求解快，可解释性强。 高斯核函数（RBF核）：可以映射到无限维，决策边界更多样，只有一个参数，更容易选择，特征多时会选用。但可解释性差，容易过拟合，计算速度较慢。 多项式核函数：可解决非线性问题，参数较多，对大数量级特征不适用。 Sigmoid核函数：主要用于神经网络。 四、应用领域 支持向量机在许多领域都有广泛的应用，如：\n文本分类：如垃圾邮件过滤、情感分析、主题分类等。 图像识别：如手写数字识别、人脸识别、物体检测等。 生物信息学：如基因表达数据分析、蛋白质结构预测、药物设计等。 金融预测：如股票价格预测、信用评分、风险评估等。 五、优缺点 优点：\n具有非常完善的数学理论。 对于非线性问题具有较好的处理能力。 鲁棒性较好，对噪声数据具有较强的抗干扰能力。 缺点：\n对于大规模数据集，训练时间较长。 对参数和核函数的选择敏感，不同的参数和核函数可能导致模型性能差异较大。 六、总结 支持向量机是一种强大的机器学习算法，它通过寻找最大间隔超平面对数据进行分类，并通过核函数处理非线性问题。在实际应用中，需要根据具体问题选择合适的核函数和参数，以达到最佳的预测性能。\n核函数 核函数（Kernel Function）在支持向量机（SVM）中扮演着至关重要的角色。它们允许SVM算法有效地处理非线性分类问题，通过将输入空间（通常是低维的）映射到一个更高维的特征空间，在这个空间中，原本非线性可分的数据变得线性可分\n随机森林 随机森林（Random Forest）是一种集成学习方法，它通过构建多个决策树并将它们的结果进行投票或平均来得到最终的预测结果。以下是关于随机森林的详细解析：\n一、定义与原理 定义：随机森林是指利用多棵决策树对样本数据进行训练、分类并预测的一种方法。它不仅可以用于分类问题，还可以用于回归问题。在分类问题中，通过多棵树分类器的投票决定最终分类结果；在回归问题中，则由多棵树预测值的均值决定最终预测结果。\n原理：随机森林基于Bagging（Bootstrap Aggregating）策略，通过结合多个决策树的预测结果来提高整个模型的准确性和稳定性。在构建随机森林的过程中，每个决策树都是基于随机选取的样本和特征子集进行训练的，这使得每棵树都具有一定的差异性，从而增加了模型的多样性。\n二、构建过程 数据抽样：使用自助采样法（bootstrap sampling）从原始数据集中随机抽取多个样本，形成多个子数据集。这些子数据集之间可能存在重叠，但每个子数据集都用于训练一棵决策树。 特征选择：在每个节点上，随机选择一部分特征进行分裂。这些特征是从所有特征中随机选取的，而不是每次都使用全部特征。这种特征随机性有助于增加模型的多样性。 构建决策树：对于每个子数据集，使用选定的特征构建一棵决策树。决策树的构建过程遵循标准的决策树算法，如ID3、C4.5或CART等。 集成预测：将多棵决策树的预测结果进行投票或平均，以得到最终的预测结果。对于分类问题，通常采用投票方式；对于回归问题，则采用平均值方式。 三、特征重要性评估 随机森林模型可以评估各个特征在分类或回归中所起的作用，即特征重要性。特征重要性通常通过以下两种方式计算：\n平均不纯度减少：在构建随机森林的每棵树时，每个特征的分裂都会导致某种程度的不纯度减少（如基尼不纯度或信息增益）。一个特征的重要性可以被定义为它在所有树中减少的不纯度的平均值。 平均精度下降：另一种计算特征重要性的方法是通过随机排列特征值并测量这种排列对模型精度的影响。一个特征的重要性可以被定义为它被随机排列时模型精度下降的平均值。 四、优缺点 优点：\n高精度：通过集成多个决策树的预测结果，随机森林通常具有较高的预测准确性。 抗过拟合：由于引入了随机性（样本随机和特征随机），随机森林能够有效地降低模型的方差，从而抑制过拟合。 处理大量数据：随机森林能够有效地处理具有大量特征和数据的问题，且不需要进行复杂的特征选择。 处理缺失值：随机森林能够自然地处理数据中的缺失值，无需进行额外的缺失值处理。 易于使用和调优：随机森林的参数相对较少，且对参数的选择不敏感，因此在实际应用中比较容易使用和调优。 缺点：\n模型复杂度高：由于构建了许多棵决策树，随机森林的模型可能会相对复杂，需要更多的计算资源。 预测过程较慢：当森林中树木数量很多时，每次进行预测所需的时间会增加。但可以通过并行计算来优化。 可解释性不佳：虽然随机森林可以通过特征重要性来评估各个特征的影响，但整体上作为一个集成模型，其预测过程不如单一决策树那样直观易懂。 五、应用领域 随机森林因其强大的性能和广泛适应性，被广泛应用于多个领域，包括但不限于：\n医学诊断：通过分析患者的各种特征（如年龄、性别、症状指标等），帮助医生准确地诊断疾病。 图像分类：在特定领域的图像分类任务中表现出色，特别是当特征明显且数据量有限时。 房价预测：通过分析房屋的各种属性（如面积、地点、卧室数量等），帮助预测房屋的市场价格。 农业产量预测：根据气候、土壤以及种子类型等特征，预测农作物的年产量。 信用卡欺诈检测：分析用户的交易行为特征，识别出异常的交易模式，帮助银行检测和预防信用卡欺诈。 工业设备故障预测：通过监控设备运行参数和历史数据，检测出异常情况，并预测设备可能的故障。 总之，随机森林是一种强大而灵活的机器学习算法，适用于各种复杂的数据分析问题。\n集成学习 集成学习（Ensemble Learning）是一种强大的机器学习策略，它将多个弱学习器（也称为基学习器或基估计器）组合起来，以构建一个具有更强性能的机器学习模型。这种策略基于“三个臭皮匠，顶个诸葛亮”的思想，即多个个体学习器的结合可以产生超过单一学习器的整体性能。\n一、集成学习的分类 集成学习可以根据基学习器的类型是否相同，分为同质集成和异质集成两种方法：\n同质集成：使用相同类型的学习算法构建多个基学习器。例如，所有的基学习器都是决策树或都是神经网络。 异质集成：使用不同类型的学习算法构建基学习器。例如，可以同时使用支持向量机、逻辑回归和朴素贝叶斯等算法作为基学习器。 二、集成学习的基本原理 集成学习的基本原理基于两个关键假设：\n基学习器的准确性：基学习器的预测准确性应高于随机猜测。 基学习器的差异性：基学习器之间应具有一定的差异性，这样它们的预测结果才能互补，从而提高整体模型的性能。 三、集成学习的常用算法 集成学习有多种实现算法，其中一些最著名的算法包括：\nBagging（Bootstrap Aggregating）\n原理：通过自助采样（Bootstrap Sampling）方式，从原始数据集中有放回地采样得到多个子数据集，然后使用相同的学习算法在这些子数据集上构建多个基学习器，最后通过投票或平均的方式得到最终结果。 特点：能够有效降低模型的方差，提高模型的鲁棒性。 代表算法：随机森林（Random Forest）是Bagging的一个变体，通过随机特征选择和样本采样构建多颗决策树，并通过投票机制进行预测。 Boosting\n原理：通过迭代的方式构建基学习器。每一轮迭代中，Boosting算法会根据上一轮的学习结果调整样本的权重，使得模型更关注错误分类的样本。然后将这些基学习器进行线性组合，得到最终的强学习器。 特点：能够有效降低模型的偏差，提高模型的准确性。 代表算法：AdaBoost（Adaptive Boosting）和梯度提升（Gradient Boosting）是Boosting的两个重要算法。 Stacking\n原理：将多个基学习器的预测结果作为输入，再通过一个元学习器（Meta Learner）进行结合，得到最终的预测结果。 特点：能够充分利用基学习器之间的差异性，提高模型的泛化能力。 四、集成学习的应用 集成学习在多个领域都有广泛的应用，包括但不限于：\n金融风控：用于信用评估、欺诈检测等风控任务，提高风险识别能力。 医疗诊断：用于疾病诊断、药物预测等任务，提高诊断的准确性和可靠性。 图像识别：在计算机视觉领域，用于图像分类、目标检测等任务，提高图像识别的准确率。 自然语言处理：在自然语言处理领域，用于文本分类、情感分析等任务，提高文本处理的效果。 五、集成学习的优势和挑战 优势：\n提高准确性：通过多个基学习器的结合，可以提高整体模型的准确性。 提高鲁棒性：通过投票或平均等方式，可以减少模型的方差，提高鲁棒性。 充分利用信息：能够充分利用基学习器之间的差异性，提高模型的泛化能力。 挑战：\n计算复杂度高：需要构建多个基学习器并进行结合，因此计算复杂度较高。 数据不平衡问题：在某些情况下，数据可能存在不平衡的情况，这会影响集成学习的性能。 可解释性较差：由于集成了多个基学习器的预测结果，因此整体模型的可解释性较差。 综上所述，集成学习是一种强大的机器学习策略，它通过结合多个基学习器的预测结果来提高整体模型的性能。在实际应用中，可以根据具体问题的需求选择合适的集成学习算法和基学习器。\n神经网络基础 神经网络基础涉及多个方面，以下是对其的详细阐述：\n一、定义与起源 定义：神经网络是机器学习中的一种模型，是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。它最初是受生物神经系统的启发，为了模拟生物神经系统而出现的。 起源：神经网络模型最初是基于生物神经系统的结构和功能进行构建的，尤其是模拟神经元之间的连接和信号传递方式。 二、基本结构与组成 神经元：神经网络的基本组成单元是神经元（或称为节点）。每个神经元接收来自其他神经元的输入信号，通过加权求和、激活函数等步骤产生输出信号。 层次结构：典型的神经网络结构包括输入层、隐藏层和输出层。输入层负责接收外部输入数据；隐藏层是神经网络的核心部分，负责处理输入数据并提取特征；输出层则根据处理结果产生最终的输出。 连接与权重：神经元之间通过连接（也称为边）相互连接，每个连接都有一个权重值，表示该连接对信号传递的影响程度。 三、工作原理 信号传递：在神经网络中，输入信号首先进入输入层神经元，然后通过加权求和和激活函数处理传递到隐藏层神经元。隐藏层神经元对输入信号进行进一步处理并提取特征后，再将处理结果传递到输出层神经元。输出层神经元最终产生输出结果。 学习与优化：神经网络通过训练过程来学习输入与输出之间的映射关系。在训练过程中，神经网络会根据实际输出与期望输出之间的误差来调整连接权重和偏置项等参数，以减小误差并提高模型的准确性。 四、应用领域 图像识别：神经网络在图像识别领域具有广泛应用，如人脸识别、物体检测等。 自然语言处理：神经网络也被用于自然语言处理任务中，如文本分类、情感分析等。 推荐系统：在推荐系统中，神经网络可以根据用户的历史行为和其他信息来预测用户的兴趣偏好并推荐相关物品。 五、发展趋势 深度学习：随着计算能力的提升和大数据的兴起，深度学习成为神经网络发展的重要方向。深度学习通过构建更深层次的神经网络结构来提取更加抽象和复杂的特征表示，从而进一步提高模型的准确性和泛化能力。 模型压缩与优化：为了降低神经网络的计算复杂度和提高实时性能，研究者们开始关注模型压缩与优化技术。这些技术包括剪枝、量化、知识蒸馏等方法，可以在保证模型性能的前提下减少模型参数数量和计算量。 跨学科融合：神经网络的发展也促进了与其他学科的融合。例如，与脑科学的结合有助于更好地理解神经网络的工作机制和优化方法；与医学的结合则可以推动医疗诊断技术的创新和发展。 综上所述，神经网络作为一种重要的机器学习模型具有广泛的应用前景和发展潜力。随着技术的不断进步和应用领域的不断拓展，相信神经网络将在未来发挥更加重要的作用。\n理解神经网络是端到端的系统 神经网络，特别是深度神经网络，被视为一种端到端（End-to-End）的系统，这一理解主要基于以下几个方面：\n直接映射：端到端的神经网络直接学习从输入到输出的映射关系，而不需要人为地将问题分解成多个子问题或中间步骤。这意味着网络能够自动发现数据中的复杂特征和规律，而不需要人工设计特征提取器或规则。\n黑箱模型：在端到端的系统中，神经网络内部的具体工作机制（即如何从输入转换到输出）对于用户来说通常是不可见的，或者说是一个“黑箱”。用户只需要关心输入和输出，而不需要深入了解网络内部的复杂计算过程。这种特性使得神经网络在处理复杂问题时具有更高的灵活性和泛化能力。\n整体优化：由于神经网络是端到端的，因此可以对整个系统进行整体优化，而不是分别优化各个子模块。在训练过程中，网络会根据输出与期望结果之间的误差来调整其内部参数（如权重和偏置），以最小化这个误差。这种整体优化的方式有助于提高模型的性能和稳定性。\n减少人工干预：端到端的神经网络减少了人工干预的需要。在传统的机器学习方法中，通常需要人工设计特征提取器、选择分类器、调整参数等步骤。而在端到端的神经网络中，这些步骤都被自动化了，大大减轻了人工负担，并提高了模型的泛化能力。\n应用广泛：由于神经网络能够自动学习复杂的映射关系，并且具有高度的灵活性和泛化能力，因此被广泛应用于各种领域，如图像识别、语音识别、自然语言处理、推荐系统等。在这些应用中，神经网络都表现出了端到端系统的优势。\n综上所述，理解神经网络是一个端到端的系统，就是认识到它能够直接学习从输入到输出的映射关系，而不需要人工设计中间步骤或特征提取器；同时，它能够对整个系统进行整体优化，减少人工干预的需要，并广泛应用于各种领域。这种端到端的特性使得神经网络在处理复杂问题时具有更高的效率和准确性。\n感知机 感知机（Perceptron），也被称为感知器，是Frank Rosenblatt在1957年提出的一种人工神经网络模型。以下是对感知机的详细解读：\n一、定义与基础 定义：感知机是一种二分类的线性模型，其输入是实例的特征向量，输出是实例的类别，取值为+1和-1。它属于判别模型，是神经网络和支持向量机的基础。 基础概念：感知机的学习目标是求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面。这个超平面将特征空间划分为两部分，分别对应正类和负类。 二、工作原理 输入与输出：感知机的输入是特征向量，每个特征都对应一个权重，这些权重的和加上一个偏置项，经过符号函数（如sign函数）处理后，得到输出类别（+1或-1）。 学习过程：感知机的学习过程是误分类驱动的，通过不断调整权重和偏置项来减少误分类点的数量。具体来说，当某个实例点被误分类时，即其类别与通过当前模型计算得到的类别不符时，就调整权重和偏置项，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离。 三、优点与缺点 优点： 简单易懂：感知机模型结构简单，易于理解和实现。 训练速度快：由于感知机模型简单，因此训练速度相对较快。 适用于大规模数据集：感知机在处理大规模数据集时表现出色。 缺点： 只能解决线性可分问题：感知机只能处理线性可分的数据集，对于非线性问题表现不佳。 只能进行二分类：感知机只能进行二分类任务，对于多分类问题需要进行改进。 对噪声和异常点敏感：感知机对噪声和异常点比较敏感，需要进行特殊处理以提高模型的鲁棒性。 四、应用领域 感知机模型虽然简单，但在实际应用中仍然具有广泛的应用领域，包括但不限于：\n图像识别与分类：感知机可以用于图像的分类任务，如将图片分为不同的类别。 自然语言处理：在自然语言处理领域，感知机可以用于文本分类、情感分析等任务。 信号处理：在信号处理领域，感知机可以用于信号分类和噪声识别等任务。 数据挖掘与预测：在数据挖掘和预测领域，感知机可以用于分类、回归等任务。 五、发展历史与现状 历史：感知机由Frank Rosenblatt在1957年提出，是神经网络和支持向量机的基础。它的出现标志着人工神经网络研究的开始。 现状：随着人工智能和机器学习技术的不断发展，感知机作为最基础的人工神经网络模型之一，仍然具有一定的研究价值和应用前景。同时，更复杂的神经网络模型如深度神经网络（DNN）、卷积神经网络（CNN）等已经得到了广泛的应用和发展。 综上所述，感知机是一种简单而有效的二分类线性模型，在多个领域具有广泛的应用价值。然而，由于其自身的局限性，如只能处理线性可分问题和二分类任务等，因此在实际应用中需要根据具体问题的特点进行选择和改进。\n多层感知机 多层感知机（Multilayer Perceptron，简称MLP）是深度学习中的一种基础且广泛使用的神经网络模型。以下是多层感知机的简略介绍：\n一、定义与结构 定义：多层感知机是一种前馈神经网络，由多个神经元层组成，包括输入层、一个或多个隐藏层以及输出层。每一层的神经元都与前一层全连接，通过权重和激活函数实现非线性映射。 结构：多层感知机的基本结构包括输入层、隐藏层（可能有多层）和输出层。输入层接收外部数据，隐藏层对数据进行处理和特征提取，输出层则给出最终的预测结果。 二、工作原理 前向传播：输入数据通过输入层进入网络，经过隐藏层的加权求和与激活函数处理后，最终由输出层输出预测结果。 反向传播：在训练过程中，通过比较输出层的预测结果与实际标签的误差，利用反向传播算法调整网络中的权重和偏置项，以最小化误差。 三、优点与缺点 优点： 强大的表征能力：通过多个隐藏层的组合，可以学习到复杂的数据特征和表示。 灵活的非线性映射：激活函数的引入使得多层感知机能够处理非线性问题。 广泛的应用场景：适用于分类、回归、聚类等多种机器学习任务。 缺点： 训练时间长：多层感知机的训练需要大量的计算资源和时间。 易于过拟合：复杂的网络结构可能导致过拟合问题，需要通过正则化等技术进行缓解。 可解释性差：多层感知机的决策过程相对复杂，不如一些传统机器学习模型易于解释。 四、应用场景 多层感知机因其强大的表征能力和广泛的应用场景，在多个领域得到了广泛应用，包括但不限于：\n计算机视觉：图像分类、目标检测、图像分割等。 自然语言处理：文本分类、情感分析、机器翻译等。 推荐系统：个性化推荐、广告推荐等。 金融风控：信用评分、欺诈检测等。 医疗健康：疾病诊断、药物预测、基因分类等。 工业制造：质量控制、故障诊断、预测维护等。 五、实现工具与框架 在Python中，可以使用多种深度学习框架来实现多层感知机，如TensorFlow、PyTorch、Keras等。这些框架提供了丰富的API和工具，使得多层感知机的构建和训练变得更加便捷和高效。\n综上所述，多层感知机作为一种基础且强大的神经网络模型，在深度学习和机器学习领域具有广泛的应用前景。然而，在实际应用中需要注意其训练时间长、易于过拟合以及可解释性差等缺点，并结合具体问题和数据情况选择合适的模型和算法。\nBP神经网络 BP神经网络，全称为反向传播神经网络（Back Propagation Neural Network），是1986年由Rumelhart和McClelland为首的科学家提出的一种多层前馈神经网络模型。该网络通过误差逆向传播算法进行训练，是应用最广泛的神经网络模型之一。以下是对BP神经网络的详细解析：\n一、定义与结构 定义：BP神经网络是一种按照误差逆向传播算法训练的多层前馈神经网络。 结构：BP神经网络通常由输入层、隐藏层（可有多个）和输出层组成。每一层的神经元都与前一层全连接，通过加权和的方式传递信号，并经过激活函数进行非线性变换。 二、工作原理 BP神经网络的训练过程包括两个阶段：前向传播和反向传播。\n前向传播：\n输入信号从输入层开始，逐层向前传播，直到输出层。 在每一层，神经元的输入是前一层神经元输出的加权和，经过激活函数处理后得到该层神经元的输出。 反向传播：\n计算输出层的实际输出与期望输出之间的误差。 将误差信号反向传播回输入层，通过调整各层神经元之间的连接权重和偏置项，使误差逐步减小。 权重和偏置项的调整依据是梯度下降法，即沿着误差梯度下降的方向调整权重和偏置项，以最小化误差函数。 三、特点与优势 强大的非线性映射能力：BP神经网络能够逼近复杂的非线性函数关系，适用于解决各种复杂的非线性问题。 并行处理能力：BP神经网络的各个神经元之间是并行计算的，适合于结构化并行处理，能够快速处理大量数据。 良好的泛化能力：经过训练的BP神经网络能够对未见过的样本进行较好的预测和分类。 四、应用领域 BP神经网络因其强大的建模能力和广泛的应用场景，在多个领域得到了广泛应用，包括但不限于：\n函数逼近：用输入向量和相应的输出向量训练一个网络逼近一个函数。 模式识别：用一个待定的输出向量将它与输入向量联系起来，实现图像识别、语音识别等功能。 分类：把输入向量所定义的合适方式进行分类，如文本分类、图像分类等。 数据压缩：减少输出向量维数以便于传输或存储。 五、存在的问题与改进 尽管BP神经网络具有许多优点，但也存在一些问题和局限性：\n训练时间长：BP神经网络的训练过程需要大量的迭代计算，因此训练时间较长。 易陷入局部最优解：BP神经网络对初始权重和偏置项敏感，容易陷入局部最优解而非全局最优解。 网络结构选择困难：BP神经网络的网络结构选择需要经验和试错，网络的过拟合和欠拟合问题需要仔细调整。 针对这些问题，研究者们提出了许多改进措施，如引入动量项、学习率自适应调整、使用更复杂的激活函数等，以提高BP神经网络的训练效率和性能。\n六、总结 BP神经网络作为一种经典且广泛应用的神经网络模型，在多个领域发挥着重要作用。通过不断优化和改进，BP神经网络将继续在人工智能和机器学习领域发挥更大的潜力。\n径向基函数网络 径向基函数网络（Radial Basis Function Network，简称RBF网络）是一种使用径向基函数作为激活函数的人工神经网络。这种网络在多个领域都有广泛的应用，包括函数近似、时间序列预测、分类和系统控制等。以下是对径向基函数网络的详细解析：\n一、定义与结构 定义：径向基函数网络是一种三层前向网络，包括输入层、隐含层和输出层。其中，隐含层使用径向基函数作为激活函数，实现输入到输出的非线性映射。 结构： 输入层：由信号源节点组成，仅起到传输信号的作用，对输入信息不做任何变换。 隐含层：节点数视所描述问题的需要而定，隐单元的变换函数是径向基函数，通常是对中心点径向对称且衰减的非负非线性函数，如高斯函数。 输出层：对输入模式作出响应，是隐含层输出的线性组合。 二、工作原理 径向基函数：径向基函数是某种沿径向对称的标量函数，通常定义为空间中任一点x到某一中心c之间欧氏距离的单调函数，可记作k(||x-c||)。其作用往往是局部的，即当x远离c时函数取值很小。 前向传播：输入信号通过输入层进入网络，经过隐含层的径向基函数变换后，再传递到输出层。输出层将隐含层的输出加权求和得到最终的输出结果。 训练过程： 无监督学习：首先通过无监督学习确定输入层与隐含层间的参数（如基函数的中心和宽度）。 有监督学习：然后利用有监督学习确定隐含层与输出层间的权值。 三、特点与优势 逼近能力：径向基函数网络能够逼近任意非线性函数，具有强大的非线性映射能力。 学习速度：由于参数初始化具有一定的方法，并非随机初始化，且隐含层到输出层的变换是线性的，因此学习收敛速度快。 结构简单：相比其他神经网络模型，径向基函数网络的结构相对简单，训练过程也较为简洁。 可解释性强：由于网络结构清晰，各层功能明确，因此可解释性较强。 可在线学习：径向基函数网络支持在线学习，即在新数据到达时能够动态调整模型的权值。 四、应用领域 模式分类：如图像分类、语音识别等，通过将输入样本映射到高维空间来提高分类的准确性。 数据挖掘：用于聚类分析、回归分析等任务，帮助挖掘数据中的潜在规律和趋势。 时间序列预测：如股票价格预测、天气预测等，通过学习历史数据的模式和规律来预测未来的趋势和变化。 控制系统：如自适应控制、机器人控制等，通过学习环境的状态和反馈信号来实现智能化的控制策略。 五、总结 径向基函数网络作为一种常用的人工神经网络模型，通过径向基函数的非线性映射和局部响应特性，实现了输入到输出的高效转换。其在多个领域的广泛应用和独特优势使其成为人工智能和机器学习领域的重要工具之一。\nHopfield网络 Hopfield网络，也被称为Hopfield神经网络或浩斯菲尔德网络，是由物理学家约翰·霍普菲尔德（John Hopfield）在1982年发明的一种递归神经网络。以下是对Hopfield网络的详细解析：\n一、定义与结构 定义：Hopfield网络是一种结合存储系统和二元系统的神经网络，用于解决模式识别问题和提供一类组合优化问题的近似解。 结构：Hopfield网络由多个神经元组成，每个神经元可以取两个值（通常是0或1，或者-1和1），表示神经元的兴奋状态。神经元之间通过权重相连，且权重是对称的，即神经元i和神经元j之间的权重w(i,j)等于神经元j和神经元i之间的权重w(j,i)。 二、工作原理 能量函数：Hopfield网络的工作原理基于Lyapunov稳定性定理和LaSalle不变性定理，其内部状态可以通过能量函数来描述。网络的目标是使能量函数最小化，从而达到稳定状态。 联想记忆：Hopfield网络具有联想记忆功能，即能够通过部分或损坏的输入信息回忆起完整的记忆模式。这是通过网络的权重矩阵和神经元的激活状态共同作用实现的。 三、特点与优势 递归性：Hopfield网络是一种递归神经网络，其神经元之间的连接形成了一个闭环系统。 收敛性：网络保证了向局部极小的收敛，但也可能收敛到错误的局部极小值而非全局最小值。 记忆模型：Hopfield网络提供了模拟人类记忆的模型，通过神经元的连接和激活状态来存储和恢复记忆。 灵活性：网络可以存储多个记忆模式，并通过学习算法调整权重矩阵来适应新的记忆模式。 四、应用领域 模式识别：Hopfield网络可以用于存储和识别特定的模式，如人脸识别、指纹识别等。 优化问题：网络还可以用于解决优化问题，如最短路径问题、旅行商问题等。 数据压缩：通过压缩数据中的冗余信息，Hopfield网络可以用于数据压缩以提高存储和传输效率。 异常检测：在数据分析中，网络能够检测数据中的异常值或异常模式。 自组织映射：实现高维数据到低维空间的映射，便于数据的可视化和分析。 五、局限性与改进 记忆容量有限：Hopfield网络的记忆容量受到神经元数量和连接方式的限制。 可能收敛到错误的局部极小值：如前所述，网络在优化过程中可能陷入局部最优解而非全局最优解。 改进方向：为了克服这些局限性，研究人员对Hopfield网络进行了多种改进，如引入更复杂的神经元结构、改变连接方式以及结合其他优化算法等。 综上所述，Hopfield网络作为一种重要的神经网络模型，在模式识别、优化问题、数据压缩等领域具有广泛的应用前景。随着研究的不断深入和技术的不断发展，Hopfield网络的性能和应用范围将不断扩大和完善。\nBoltmann机 Boltzmann机（Boltzmann Machine，简称BM）是一种由二值随机神经元构成的两层对称连接神经网络，该定义于2018年由全国科学技术名词审定委员会公布。以下是对Boltzmann机的详细解析：\n一、定义与结构 定义：Boltzmann机是一种生成模型，它由二值随机神经元组成，并通过对称连接形成网络。这种网络通过优化玻尔兹曼能量函数来训练其权重。 结构：Boltzmann机通常包含可见层（visible layer）和隐藏层（hidden layer）。可见层包含输入数据的节点，而隐藏层包含用于学习的节点。两层之间通过权重进行连接，且这些连接是双向的，即每个节点都与其他层的节点相连。 二、工作原理 初始化：在训练开始时，隐藏层的状态被随机初始化。 迭代更新：随后，根据输入数据、连接权重和激活函数，网络会进行迭代更新。这包括计算输入层和隐藏层之间的激活值，以及更新连接权重以最小化能量函数。 生成与分类：在生成模型中，Boltzmann机会根据输入数据生成一组符合输入数据特征的隐藏状态；在判别模型中，它则根据输入数据判断其所属的类别。 三、数学模型 Boltzmann机的数学模型可以表示为概率分布，其中$P(x,h)$表示数据$x$在隐藏状态$h$下的概率。这个概率分布通过玻尔兹曼能量函数来定义，通常使用sigmoid函数作为激活函数来模拟神经元的激活行为。\n四、应用与前景 应用：Boltzmann机在多个领域都有潜在的应用，包括图像处理、自然语言处理、机器学习等。然而，由于其训练过程的复杂性和计算资源的消耗，实际应用中可能面临一些挑战。 深度玻尔兹曼机：为了克服这些挑战，研究者们提出了深度玻尔兹曼机（Deep Boltzmann Machine，DBM），它包含多个隐藏层，能够学习更复杂的表示和抽象。DBM在图像识别、自然语言处理等领域展现出了良好的性能。 五、未来发展趋势与挑战 发展趋势：随着计算能力的提升和算法的优化，Boltzmann机及其变体有望在更多领域得到应用。特别是在自动驾驶、智能机器人等前沿技术中，Boltzmann机可能发挥重要作用。 挑战：然而，Boltzmann机的训练过程仍然是一个挑战。由于其连接权重是双向的且需要优化能量函数，这导致训练过程可能非常耗时且容易陷入局部最优解。因此，未来的研究需要关注如何改进训练算法以提高效率和性能。 综上所述，Boltzmann机作为一种重要的神经网络模型，在多个领域都具有潜在的应用价值。然而，其训练过程的复杂性和计算资源的消耗仍然是实际应用中需要克服的难题。随着技术的不断进步和发展，我们有理由相信Boltzmann机将在更多领域展现出其独特的优势和价值。\n自组织映射网络 自组织映射网络（Self-Organizing Map，简称SOM或SOFM）是一种基于无监督学习方法的神经网络，由芬兰神经网络专家Kohonen于1981年提出。该网络通过模拟人脑中神经细胞的自组织特性，对输入数据进行学习和映射，生成一个低维的拓扑表示。以下是关于自组织映射网络的详细解析：\n一、定义与结构 定义：自组织映射网络是一种竞争学习网络，通过神经元之间的竞争实现大脑神经系统中的“近兴奋远抑制”功能，并具有把高维输入映射到低维的能力（拓扑保形特性）。 结构：自组织映射网络通常包含输入层和输出层（竞争层）。输入层负责接收原始数据，输出层则通过竞争学习机制对输入数据进行分类和映射。输出层的神经元被放置在一维、二维甚至多维的网格节点中，最常见的是二维拓扑结构。 二、工作原理 竞争学习：当输入数据进入网络时，输出层的神经元会进行竞争，以争夺对输入数据的响应权。竞争过程通常通过计算输入数据与每个神经元之间的距离（如欧氏距离）来实现，距离最小的神经元获胜并更新其权值。 合作过程：获胜神经元周围的神经元也会受到一定程度的刺激，这被称为侧向相互作用或合作过程。这种合作机制有助于保持输出层神经元的拓扑结构，并使得相似的输入数据在输出层上能够形成连续的映射区域。 三、主要特性 拓扑保形特性：自组织映射网络能够保持输入数据在降维过程中的拓扑结构不变，即相似的输入数据在输出层上仍然保持相近的位置关系。 自组织性：网络能够自动地根据输入数据的特征进行学习和调整，而不需要外部的监督信息。 鲁棒性和泛化性能：网络对噪声和异常值具有较好的处理能力，能够提取输入数据的主要特征并进行有效的分类和映射。 四、应用领域 自组织映射网络已广泛应用于多个领域，包括但不限于：\n样本分类与排序：通过自组织映射网络可以对样本数据进行有效的分类和排序，提高数据处理的效率和准确性。 样本检测：在图像处理、信号处理等领域中，自组织映射网络可以用于检测异常样本或目标对象。 模式识别：在生物信息学、医学图像处理等领域中，自组织映射网络可以用于识别特定的模式或结构。 系统分析与优化：在工程、金融、军事等领域中，自组织映射网络可以用于系统分析和优化决策过程。 五、未来发展 随着人工智能技术的不断发展和进步，自组织映射网络将在更多领域发挥其独特的优势。未来的研究方向可能包括以下几个方面：\n算法优化：通过改进学习算法和竞争规则，提高自组织映射网络的训练效率和性能。 多模态学习：结合图像、文本、语音等多种模态的数据进行学习和映射，提高网络的综合处理能力。 跨领域应用：将自组织映射网络应用于更多的实际场景中，如自动驾驶、智能机器人等领域。 总之，自组织映射网络作为一种重要的神经网络模型，在多个领域都展现出了广泛的应用前景和巨大的发展潜力。\n深度神经网络 卷积神经网络(空间共享参数)与循环神经网络(时间共享参数) 基本思想 局部连接 参数共享 卷积操作 卷积操作是一种数学运算，广泛应用于信号处理、图像处理和深度学习等领域。以下是对卷积操作的详细解释：\n一、定义与原理 卷积操作是通过将一个函数（或称为卷积核、滤波器）在另一个函数（通常是输入信号或图像）上进行滑动，并在每个位置上计算两个函数的乘积之和，从而得到一个新的函数（或称为输出信号、特征图）。在图像处理中，卷积核通常是一个小的二维矩阵，用于提取图像中的局部特征，如边缘、纹理等。\n二、数学表达 卷积操作的数学表达可以表示为：\n[ (f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) , d\\tau ]\n其中，$f(t)$ 是输入函数，$g(t)$ 是卷积核，$t$ 和 $\\tau$ 是变量，$*$ 表示卷积操作。在离散情况下（如图像处理），积分变为求和，公式变为：\n[ (I * K){i,j} = \\sum{m,n} I_{i-m,j-n} \\cdot K_{m,n} ]\n其中，$I$ 是输入图像（二维矩阵），$K$ 是卷积核，$(I * K){i,j}$ 表示输出特征图中第 $i, j$ 个元素的值，$I{i-m,j-n}$ 表示输入图像中第 $i-m, j-n$ 个元素的值，$K_{m,n}$ 表示卷积核中第 $m, n$ 个元素的值。\n三、操作过程 在图像处理中，卷积操作的具体过程如下：\n输入图像：将输入的图像表示为一个二维矩阵，矩阵的每个元素表示图像中的一个像素点。 定义卷积核：定义一个小的二维矩阵作为卷积核，卷积核的大小和其中的权重值根据需要提取的特征来确定。 滑动卷积核：将卷积核在输入图像上按照设定的步长进行滑动，每次滑动到一个新位置时，将卷积核与输入图像中对应位置的元素进行逐元素乘积运算。 求和：将乘积运算的结果进行求和，得到输出特征图中对应位置的元素值。 遍历图像：重复步骤3和4，直到卷积核遍历完整个输入图像，最终得到完整的输出特征图。 四、应用领域 卷积操作在多个领域都有广泛的应用，包括但不限于：\n图像处理：用于边缘检测、图像滤波、特征提取等任务。 音频处理：用于音频信号的滤波和降噪，如实现音频信号的均衡器、混响效果或噪声消除。 自然语言处理：在卷积神经网络（CNN）中，通过将文本转化为向量表示，并利用卷积核进行卷积操作，实现文本分类、情感分析和语义理解等任务。 深度学习：卷积层是卷积神经网络（CNN）的核心组件，通过应用多个卷积核对输入数据进行特征提取，实现对图像、音频或文本等复杂数据的高级特征学习和表示。 五、特点与优势 卷积操作具有以下几个特点和优势：\n局部连接：卷积核只与输入图像中的局部区域相连接，减少了网络的参数数量和计算量。 参数共享：同一个卷积核在输入图像的不同位置共享相同的参数，进一步减少了网络的参数量，提高了模型的泛化能力。 平移不变性：卷积操作对输入信号的平移变换具有不变性，即输入信号发生平移时，输出信号也相应平移，但内容保持不变。 综上所述，卷积操作是一种强大的数学工具，在多个领域都有广泛的应用和重要的作用。\n池化层 池化层（Pooling Layer）是深度学习神经网络中常用的一种层级结构，尤其在卷积神经网络（CNN）中扮演着重要角色。以下是关于池化层的详细解释：\n一、定义与功能 池化层主要用于减小输入数据的空间尺寸（即宽度和高度），从而降低模型的计算复杂度，减少过拟合，并在一定程度上提取输入数据的重要特征。通过池化操作，CNN能够在保持模型表达能力的同时，有效降低计算成本和过拟合风险。\n二、主要类型 池化层根据所采用的池化函数不同，主要分为以下几种类型：\n最大池化（Max Pooling）：\n定义：在每个池化窗口（通常是一个小的二维区域，如2x2或3x3）中选择最大值作为输出。 优点：保留了信号的最大强度信息，有助于增强模型对局部细节的敏感性。 局限：可能会丢失一些次要但仍然重要的信息。 平均池化（Average Pooling）：\n定义：在每个池化窗口中计算所有值的平均值作为输出。 优点：降低了输出的方差，增加了预测的稳定性。 局限：可能导致细节模糊。 其他池化方法：如随机池化（Stochastic Pooling）、中值池化（Median Pooling）等，这些方法提供了额外的信息层次，用于更复杂的特征提取，但计算相对复杂，可能增加训练时间。\n三、作用与优势 降维：通过减少特征图的空间尺寸，可以减少模型的参数数量和计算量，从而加速模型的训练和推理过程。 特征不变性：池化操作能够提取特征的局部不变性，即使输入数据发生轻微的平移或变形，池化层仍然能够识别出相同的特征。 提高泛化能力：通过减少模型的复杂度，池化层有助于防止过拟合，提高模型的泛化能力。 增强鲁棒性：池化层增强了模型对输入数据变化的鲁棒性，使其能够更好地应对实际应用中的噪声和干扰。 四、应用领域 池化层广泛应用于各种深度学习框架中，尤其在计算机视觉任务中发挥核心作用，如图像分类、目标检测、语义分割等领域。它们还被用于自然语言处理和音频分析等其他领域的相关任务。\n五、实现方式 在深度学习框架中，如PyTorch和TensorFlow，都提供了实现池化层的函数或层。例如，在PyTorch中，可以使用nn.MaxPool2d和nn.AvgPool2d等函数来实现最大池化和平均池化操作。\n综上所述，池化层是深度学习神经网络中不可或缺的一部分，它通过减小输入数据的空间尺寸、提取重要特征、降低计算复杂度和提高模型泛化能力等方式，为深度学习模型的性能提升做出了重要贡献。\n循环单元 循环单元（Recurrent Units）是循环神经网络（Recurrent Neural Network, RNN）中的核心组成部分，它们负责在序列数据中传递信息，并具有记忆功能，能够捕捉序列数据中的长期依赖关系。以下是对循环单元的详细解析：\n一、基本概念 循环单元是RNN中的基本构件，它通过不断更新隐藏状态来实现信息的传递和记忆。在每个时间步，循环单元接收当前的输入和前一时间步的隐藏状态作为输入，然后输出一个新的隐藏状态，该隐藏状态随后被传递给下一个时间步的循环单元，或者用于生成输出。\n二、设计与功能 循环单元的设计使其能够处理序列数据中的时间依赖性。它们通过以下方式实现其功能：\n状态更新：在每个时间步，循环单元根据当前输入和前一时间步的隐藏状态计算新的隐藏状态。这个计算过程通常包括线性变换（如矩阵乘法）和非线性激活（如tanh或sigmoid函数）。 记忆功能：通过保留前一时间步的隐藏状态，循环单元能够“记住”序列中的历史信息，并在处理当前输入时考虑这些信息。这种记忆功能使得RNN能够捕捉序列数据中的长期依赖关系。 信息传递：循环单元之间的连接形成了一个循环，使得信息可以在序列中沿时间轴传递。这种信息传递机制是RNN处理序列数据的关键所在。 三、常见类型 在实际应用中，循环单元有多种变体，其中最常见的是长短期记忆网络（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU）。\nLSTM：LSTM通过引入三个“门”结构（遗忘门、输入门和输出门）来控制信息的传递和遗忘。这些门结构使得LSTM能够更好地处理长期依赖问题，并在许多任务上表现出色。 GRU：GRU是LSTM的一种简化版本，它只有两个门结构（更新门和重置门）。相比于LSTM，GRU的参数数量更少，计算速度更快，同时在许多任务上的表现也相当不错。 四、应用领域 循环单元及其变体广泛应用于自然语言处理（如文本生成、机器翻译、情感分析等）、语音识别、时间序列预测等领域。它们能够处理序列数据中的时间依赖性，并提取出有用的特征和信息，为这些任务提供有力的支持。\n五、总结 循环单元是RNN中的核心组成部分，它们通过不断更新隐藏状态来实现信息的传递和记忆。在实际应用中，循环单元有多种变体，其中LSTM和GRU是最常见的两种。这些变体通过引入不同的门结构来控制信息的传递和遗忘，从而更好地处理序列数据中的长期依赖问题。\n搭建训练神经网络的项目（待做） 数据预处理模块 数据准备模块 工具函数 神经网络中各组件 神经网络模型 参数优化模块 训练过程定义 启动脚本 聚类算法 K-means K-means（K-均值）聚类算法是一种经典的无监督学习算法，用于将数据集中的样本点划分为K个簇（Cluster），使得每个簇内的样本点尽可能相似，而不同簇之间的样本点尽可能不同。以下是关于K-means算法的详细介绍：\n一、算法原理 K-means算法的核心思想是通过迭代的方式，将数据集中的样本点分配到K个簇中，使得每个簇内的样本点到该簇的质心（即簇内所有样本点的均值）的距离之和最小。算法的具体步骤如下：\n初始化：从数据集中随机选择K个样本点作为初始的簇中心（质心）。 分配簇：对于数据集中的每一个样本点，计算其与各个簇中心的距离，并将其分配到距离最近的簇中心所在的簇中。 更新质心：对于每个簇，重新计算该簇内所有样本点的均值，作为新的簇中心（质心）。 迭代：重复步骤2和步骤3，直到簇中心不再发生变化或达到预设的迭代次数为止。 二、算法特点 无监督学习：K-means算法不需要事先知道数据集的标签信息，能够自动将数据划分为多个簇。 基于划分的聚类：K-means算法通过划分的方式将数据集划分为K个簇，每个簇内的样本点具有较高的相似度。 迭代优化：K-means算法采用迭代的方式不断优化簇的划分和簇中心的位置，以最小化簇内样本点到簇中心的距离之和。 需要预先指定K值：K-means算法需要事先指定要划分的簇的个数K，这个值的选择对聚类结果有很大影响。 三、算法应用 K-means算法在各个领域都有广泛的应用，包括但不限于：\n市场分析：通过聚类分析，企业可以将客户划分为不同的群体，以便制定更加精准的营销策略。 图像处理：在图像处理中，K-means算法可以用于图像分割、颜色量化等任务。 生物信息学：在生物信息学中，K-means算法可以用于基因表达数据的分析，揭示基因之间的相互作用关系。 文本挖掘：在文本挖掘中，K-means算法可以用于文档聚类，将相似的文档划分到同一个簇中。 四、算法优化 为了提高K-means算法的性能和聚类效果，可以采取以下优化措施：\n选择合适的K值：可以通过手肘法（Elbow Method）、轮廓系数法（Silhouette Coefficient）等方法来确定合适的K值。 选择初始质心：初始质心的选择对聚类结果有很大影响，可以采用K-means++等算法来优化初始质心的选择。 使用距离度量：在计算样本点到簇中心的距离时，可以选择不同的距离度量方式，如欧氏距离、曼哈顿距离等。 并行化计算：对于大规模数据集，可以采用并行化计算来提高K-means算法的执行效率。 五、注意事项 对异常值敏感：K-means算法对异常值（离群点）比较敏感，可能会导致聚类结果不准确。 可能陷入局部最优解：K-means算法采用贪心策略进行迭代优化，可能会陷入局部最优解而无法达到全局最优。 需要预先指定K值：K值的选择对聚类结果有很大影响，需要根据实际情况进行选择。 总之，K-means算法是一种简单而有效的聚类算法，在各个领域都有广泛的应用。然而，在使用时需要注意其特点和限制条件，并采取适当的优化措施来提高聚类效果。\n寻优算法之遗传算法 遗传算法（Genetic Algorithm, GA）是一种模拟自然界生物进化过程的优化搜索方法，它基于达尔文进化论中的自然选择和遗传学原理。遗传算法通过模拟生物进化过程中的选择、交叉（杂交）和变异等操作，对问题的解进行迭代更新，从而搜索最优解或近似最优解。以下是对遗传算法的详细介绍：\n一、起源与发展 起源：遗传算法最早由美国计算机科学家John H. Holland于20世纪60年代提出，并于1975年详细阐述了遗传算法的基本理论和方法。 发展：自20世纪80年代以来，遗传算法进入兴盛发展时期，被广泛应用于自动控制、生产计划、图像处理、机器人等研究领域。 二、基本概念 种群：遗传算法从一个代表问题可能潜在解集的种群开始，种群由多个个体组成，每个个体表示一个解。 染色体：在遗传算法中，每个个体由代表基因集合的染色体构成，染色体可以是二进制串、实数向量或其他形式的编码。 适应度函数：用于评估种群中个体的优劣程度，根据问题的目标函数来确定。适应度得分高的个体更有可能被选中进行繁殖。 三、基本操作 选择（Selection）：根据适应度函数，从当前种群中选择适应度高的个体作为父代，用于繁殖下一代。常用的选择策略有轮盘赌选择、锦标赛选择等。 交叉（Crossover）：随机选择两个父代个体，并交换它们染色体的一部分，以生成新的后代个体。交叉操作有助于保持种群的多样性。 变异（Mutation）：以一定的概率随机改变后代个体染色体中的某些基因，以引入新的遗传信息。变异操作有助于避免算法陷入局部最优解。 四、算法流程 初始化种群：随机生成一定数量的个体作为初始种群。 评估适应度：计算种群中每个个体的适应度值。 选择操作：根据适应度值选择父代个体。 交叉操作：对选中的父代个体进行交叉，生成新的后代个体。 变异操作：对后代个体的染色体进行变异。 更新种群：用新生成的后代个体替换种群中的部分个体，形成新的种群。 终止条件：重复执行步骤2至步骤6，直到满足预设的迭代次数或达到其他停止条件。 五、优缺点 优点：\n自适应性：能够处理复杂的非线性、非凸优化问题。 全局搜索能力：具有较好的全局搜索能力，可以避免陷入局部最优解。 鲁棒性：对问题的依赖性较小，易于实现和应用。 缺点：\n收敛速度可能较慢：需要较多的迭代次数才能找到最优解。 对参数敏感：如种群大小、交叉概率、变异概率等参数的选择对算法性能有较大影响。 早熟收敛：在某些情况下，算法可能过早地收敛到局部最优解。 六、应用领域 遗传算法已被广泛应用于多个领域，包括但不限于：\n函数优化：求解各种复杂形式的优化问题。 组合优化：解决背包问题、装载问题、选址问题等组合优化问题。 机器学习：优化机器学习模型的参数，提高模型的性能。 控制系统：优化控制系统的设计，如控制器的参数调节。 信号处理：优化信号处理问题，如图像压缩、音频处理等。 生物信息学：解决生物信息学中的问题，如基因编码、蛋白质结构预测等。 总之，遗传算法作为一种模拟自然进化过程的优化搜索方法，在多个领域都具有广泛的应用前景。\n算法拓展 精英主义思想 每次产生新种群时，把父代种群中的部分最优解直接复制到子代群体里或按概率选择方法保留一部分个体\n灾变 如果找不到最优解，或陷入局部最优，杀死一定比例的最优个体，给其他远离最优的个体一个机会\n","date":"2024-03-30T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/097ac305590341a9a724bf7be454397f9b1719b3_hu11689926663444899714.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","title":"机器学习基础笔记"},{"content":"Window与Linux双系统安装与卸载Linux 如果你事先在window上安装的ubuntu,但是因为种种原因需要重装，那么你需要先卸载装在你电脑上的ubuntu系统才能安装新的Ubuntu\n重装unbuntu记得先保存好ubuntu想保存的文件\nUbuntu卸载 删除ubuntu所在卷 进入windows系统，右键此电脑-管理-磁盘管理-删除ubuntu所在的卷，使这些区域成为“可用空间”\n删除unbuntu EFI分区 Win + R 输入cmd打开终端，输入 diskpart进入磁盘工具\n输入 list disk 查看磁盘，输入 select disk 1 （我的Ubuntu EFI分区在磁盘1，根据自己的情况选择）\n输入 list partition ，输入 select partition * （*为Ubuntu EFI分区号）\n删除Ubuntu系统启动项 Win + R 输入cmd打开终端，输入 diskpart 进入磁盘工具\n输入 list disk 查看磁盘，输入 select disk 1\n输入 list partition ，输入 select partition * （*为Windows EFI分区，一般为260M）\n输入 assign letter=J（分配盘符）\n管理员模式打开记事本，记事本选择文件-打开-选中磁盘J\n打开 EFI 文件夹，删除Ubuntu文件夹\n返回 Distpart 界面，输入 remove letter=J\n删除时一定要看仔细，不如会导致很严重的后果\n磁盘分区（如果你是第一次装Ubuntu） 打开计算机管理 找到磁盘管理 选择你要压缩的盘，你压缩掉的空间将作为新系统的空间 右键打开你要压缩的盘符，点击压缩卷 安装ubuntu系统 强烈建议安装最新lts版本，我写这篇博客时是22.04版 我就是因为第一次装的是18.04版所以才重装的，QAQ\n下载Linux镜像 1.下载 Ubuntu 镜像，这里可以去官方下载，但是官方在国外，默认外网链接可能网速有点小慢。（科技玩家例外） 更好的选择是国内的资源镜像网站，比如说清华大学开源软件镜像站\n清华大学开源软件镜像站\n准备u盘刻盘工具 我用的是UltralSO\n下载后打开，找到你下载的镜像文件，找到后，点击右上角的启动-\u0026gt;写入硬盘映像\n写入时记得插入u盘，这个u盘将作为启动盘 写入时，这个u盘的数据全会被格式化，谨慎操作\n然后点击写入就可以了\n开机引导界面 将写好的映像文件的U盘插入你需要的电脑，在开始时按f12，本人用的是联想，不同电脑可能不同，正常都是f12\n在引导界面可以看到几个选项，选择你的u盘作为启动项，反正不是window和network就是了\n启动后你会进入系统选择界面，使用方向键选择 ubuntu 后回车就进入了 Ubuntu 的安装引导界面。在侧边栏中选择系统语言，English、Chinese都可，看自己喜好，然后点击 Install Ubuntu 进入安装，选择安装方式，选择正常安装就行，会默认安装火狐浏览器等软件。或者选择最小安装的话可以在安装完成后自行安装需要的软件，两种方式影响不大。下面的安装第三方软件选项也可以选上，也可以不选，后面再根据需要手动安装。我这里就只选择了正常安装，然后点击继续，在安装类型选择时，建议自己手动分区，说一下分区情况吧，我找了大部分教程都是分为四个区：\n分区方案 /boot : 1G（最好） 主分区。系统的boot启动引导项安装位置\n/ : 随意（尽量大） 主分区。根目录，所有目录的根节点，其下包含很多子目录，如/usr /tmp等\n/home : 自定义（尽量大，一般最后分） 逻辑分区。一般放置自己的数据\nswap : 16G 逻辑分区。交换空间，一般是物理内存的1~2倍就行了\n具体操作，首先找到 free space 空间，如下，选中该空间，点击左下角的加号+，进行内存分配\n安装后-\u0026gt;选择地区（上海或香港都可以，我选的是上海）-\u0026gt;设置账户密码-\u0026gt;重启-\u0026gt;输入密码，ok\n我和同学在重启时都出现，黑屏跳一堆数据的情况，我们当时都很慌，但是我们最后强制关机重启后，发现没有大问题，起码我们没发现问题\n以后你每次开机时都可以选择windo和Linux之一的系统进行启动\nTip 安装后可能会出现很多问题，比如，Linux熄屏黑屏启动不了，这个是好像是显卡冲突问题，内置键盘输入不了等等\u0026hellip;\n可以去网上找对应解决方法\n","date":"2024-03-25T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/linux-win%E5%8F%8C%E7%B3%BB%E7%BB%9F/f5fe3c72432cf9bace81d3d424f1a6a5_hu16972317326397920307.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/linux-win%E5%8F%8C%E7%B3%BB%E7%BB%9F/","title":"Linux+Win双系统"},{"content":"MySQL基本操作命令汇总 基本操作 对数据库以及表的一些基本操作\n关于数据库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 创建数据库 create database h_test; // 查看数据库 show databases; //查看数据库信息 show create database h_test; //修改数据库的编码，可使用上一条语句查看是否修改成功 alter database h_test default character set gbk collate gbk_bin; //删除数据库 drop database h_test; //综上，可以直接创建数据库且设置编码方式 create database h_test default character set utf8 collate utf_general_ci; 关于数据表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 //首先选定操作的数据库 use h_test; //create table student( id int(11), name varchar(20), age int(11) ); //查看数据表 show tables; //查看数据表信息，后面加上参数/G可使结果更加美观 show create table student; //查看表的字段信息 desc student; //修改表名 alter table student rename [to] h_student; //修改字段名 alter table h_student change name stu_name varchar(20); //修改字段的数据类型 alter table h_sutdent modify id int(20); //添加字段 alter table h_student add grade float; //删除字段 alter table h_student drop grade; //修改字段的位置 alter table h_student modify stu_name varchar(20) first; alter table h_student modify id int(11) after age; //删除数据表 drop table h_student;S 表的约束 PRIMARY KEY 主键约束，用于唯一标识对应的记录 FOREIGN KEY 外键约束 NOT NULL 非空约束 UNIQUE 唯一性约束 DEFAULT 默认值约束，用于设置字段的默认值\n索引 作用：提高表中数据的查询速度\n普通索引 唯一性索引 全文索引 单列索引 多列索引 空间索引 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 //创建索引 //一.创建表的时候创建索引 create table 表名( 字段名 数据类型[完整性约束条件], ... 字段名 数据类型, [UNIQUE|FULLTEXT|SPATIAL] INDEX|KEY ); //1-1.创建普通索引 create table test1( id INT, name VARCHAR(20), age INT, INDEX (id) ); //可以插入一条数据,查看索引是否被使用 explain select * from test1 where id=1 \\G; //1-2.创建唯一性索引 create table test2( id INT, name VARCHAR(20), age INT, UNIQUE INDEX unique_id(id asc) ); //1-3.创建全文索引 create table test3( id INT, name VARCHAR(20), age INT, FULLTEXT INDEX fulltext_name(name) )ENGINE=MyISAM; //1-4.创建单列索引 create table test4( id INT, name VARCHAR(20), age INT, INDEX single_name(name(20)) ); //1-5.创建多列索引 create table test5( id INT, name VARCHAR(20), age INT, INDEX multi(id,name(20)) ); //1-6.创建空间索引 create table test6( id INT, space GEOMETRY NOT NULL, SPATIAL INDEX sp(space) )ENGINE=MyISAM; --------------------------------------------------- //二.使用create index语句在已经存在的表上创建索引 //首先新建一个表,这个表没有索引 create table student( id int, age int, name varchar(20), intro varchar(40), g GEOMETRY NOT NULL )ENGINE=MyISAM; //2-1.创建普通索引 create index index_id on student(id); //2-2.创建唯一性索引 create unique index uniqueidx on student(id); //2-3.创建单列索引 create index singleidx on student(age); //2-4.创建多列索引 create index mulitidx on student(name(20),intro(40)); //2-5.创建全文索引 create fulltext index fulltextidx on student(name); //2-6.创建空间索引 create spatial index spatidx on student(g); //下图是第二种方法创建索引演示后的所有索引 //三.使用alter table语句在已经存在的表上创建索引 //删除student表，重新创建 drop table student; create table student( id int, age int, name varchar(20), intro varchar(40), space GEOMETRY NOT NULL )ENGINE=MyISAM; //3-1.创建普通索引 alter table student add index index_id(id); //3-2.创建唯一性索引 alter table student add unique uniqueidx(id); //3-3.创建单列索引 alter table student add index singleidx (age); //3-4.创建多列索引 alter table student add index multidx(name(20),intro(40)); //3-5.创建全文索引 alter table student add fulltext index fulltextidx(name); //3-6.创建空间索引 alter table student add spatial index spatidx(space); //删除索引，有下面两种方式 //1.使用alter table删除索引fulltextidx alter table student drop index fulltextidx; //2.使用drop index删除索引spatidx drop index spatidx on student; //下图可看到删除成功 添加数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 //重新建立表student drop table student; create table student( id int, name varchar(20) not null, grade float ); //插入一条数据，也可以少某个字段的同时也少对应的数据 insert into student(id,name,grade) values(1,\u0026#39;CROW\u0026#39;,70); //也可以不指定字段名，但要注意顺序 insert into student values(2,\u0026#39;CROW\u0026#39;,80); //也可以这样添加数据 insert into student set id=3,name=\u0026#34;CROW\u0026#34;,grade=90; //同时添加多条数据 insert into student values (4,\u0026#39;CROW\u0026#39;,80), (5,\u0026#39;CROW\u0026#39;,80), (6,\u0026#39;CROW\u0026#39;,80); 更新数据 1 2 3 4 //更新id=1的数据 update student set name=\u0026#34;abab\u0026#34;,grade=60 where id=1; //批量更新，如果没有where子句，会更新表中所有对应数据 update student set grade=100 where id\u0026lt;4; 删除数据 1 2 3 4 5 6 //删除id=6的数据 delete from student where id=6; //批量删除数据 delete from student where id\u0026gt;3; //删除所有数据,DDL(数据定义语言)语句 truncate table student也可以删除表内所有数据 delete from student; 单表查询和多表操作 单表查询：如何从数据库中获取你需要的数据 多表查询：实际开发中，需要进行2张表以上进行操作\n单表查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 //建立表student create table student( id int not null auto_increment, name varchar(20) not null, grade float, primary key(id) ); //插入数据 insert into student (name,grade) values (\u0026#34;Crow1\u0026#34;,40), (\u0026#34;Crow1\u0026#34;,50), (\u0026#34;Crow2\u0026#34;,50), (\u0026#34;Crow3\u0026#34;,60), (\u0026#34;Crow4\u0026#34;,70), (\u0026#34;Crow5\u0026#34;,80), (\u0026#34;Crow6\u0026#34;,null); //查询全部 select * from student; //查询某个字段 select name from student; //条件查询,查询id=2学生的信息 select * from student where id=2; //in关键字查询,也可以使用not in select * from student where id IN(1,2,3); //between and关键字查询 select * from student where id between 2 and 5; //空值(NULL)查询，使用IS NULL来判断 select * from student where grade is null; //distinct关键字查询 select distinct name from student; //like关键字查询,查询以h开头，e结尾的数据 select * from student where name like \u0026#34;h%e\u0026#34;; //and关键字多条件查询,or关键字的使用也是类似 select * from student where id\u0026gt;5 and grade\u0026gt;60; 高级查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 //聚合函数 //count()函数,sum()函数,avg()函数,max()函数,min()函数 select count(*) from student; select sum(grade) from student; select avg(grade) from student; select max(grade) from student; select min(grade) from student; //对查询结果进行排序 select * from student order by grade; //分组查询 //1.单独使用group by分组 select * from student group by grade; //2.和聚合函数一起使用 select count(*),grade from student group by grade; //3.和having关键字一起使用 select sum(grade),name from student group by grade having sum(grade) \u0026gt;100; //使用limit限制查询结果的数量 select * from student limit 5; select * from student limit 2,2; select * from student order by grade desc limit 2,2; //函数,mysql提供了许多函数 select concat(id,\u0026#39;:\u0026#39;,name,\u0026#39;:\u0026#39;,grade) from student; //为表取别名 select * from student as stu where stu.name=\u0026#34;CROW\u0026#34;; //为字段取别名,as关键字也可以不写 select name as stu_name,grade stu_grade from student; 多表操作 1.了解外键 2.了解关联关系 3.了解各种连接查询多表的数据 4.了解子查询，会使用各种关键字以及比较运算符查询多表中的数据\n外键 外键是指引用另一个表中的一列或者多列，被引用的列应该具有主键约束或者唯一性约束，用于建立和加强两个数据表之间的连接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //创建表class,student create table class( id int not null primary key, classname varchar(20) not null )ENGINE=InnoDB; create table student( stu_id int not null primary key, stu_name varchar(20) not null, cid int not null -- 表示班级id，它就是class表的外键 )ENGINE=InnoDB; //添加外键约束 alter table student add constraint FK_ID foreign key(cid) references class(id); //删除外键约束 alter table student drop foreign key FK_ID; 操作关联表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 //数据表有三种关联关系，多对一、多对多、一对一 //学生(student)和班级(class)是多对一关系，添加数据 //首选添加外键约束 alter table student add constraint FK_ID foreign key(cid) references class(id); //添加数据,这两个表便有了关联若插入中文在终端显示空白，可设置set names \u0026#39;gbk\u0026#39;; insert into class values(1,\u0026#34;软件一班\u0026#34;),(2,\u0026#34;软件二班\u0026#34;); insert into student values(1,\u0026#34;CROW\u0026#34;,1),(2,\u0026#34;Crow1\u0026#34;,2),(3,\u0026#34;Crow2\u0026#34;,1),(4,\u0026#34;Crow3\u0026#34;,2); //交叉连接 select * from student cross join class; //内连接，该功能也可以使用where语句实现 select student.stu_name,class.classname from student join class on class.id=student.cid; //外连接 //首先在student,class表中插入数据 insert into class values(3,\u0026#34;软件三班\u0026#34;); //左连接，右连接 select s.stu_id,s.stu_name,c.classname from student s left join class c on c.id=s.cid; select s.stu_id,s.stu_name,c.classname from student s right join class c on c.id=s.cid; //复合条件连接查询就是添加过滤条件 //子查询 //in关键字子查询跟上面的in关键字查询类似 select * from student where cid in(select id from class where id=2); //exists关键字查询,相当于测试，不产生数据，只返回true或者false，只有返回true，外层才会执行，具体看下图 select * from student where exists(select id from class where id=12); -- 外层不会执行 select * from student where exists(select id from class where id=1); -- 外层会执行 //any关键字查询 select * from student where cid\u0026gt;any(select id from class); //all关键字查询 select * from student where cid=all(select id from class); 事务于存储过程 事务的概念，会开启、提交和回滚事务 事务的四种隔离级别 创建存储过程 调用、查看、修改和删除存储过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 start transaction; -- 开启事务 commit; -- 提交事务 rollback; -- 取消事务(回滚) //创建表account，插入数据 create table account( id int primary key auto_increment, name varchar(40), money float ); insert into account(name,money) values(\u0026#39;a\u0026#39;,1000),(\u0026#39;b\u0026#39;,2000),(\u0026#39;c\u0026#39;,3000); //利用事务实现转账功能，首先开启事务，然后执行语句，提交事务 start transaction; update account set money=money-100 where name=\u0026#39;a\u0026#39;; update account set money=money+100 where name=\u0026#39;b\u0026#39;; commit; //事务的提交，通过这个命令查看mysql提交方式 select @@autocommit; -- 若为1，表示自动提交，为0，就要手动提交 //若事务的提交方式为手动提交 set @@autocommit = 0; -- 设置为手动提交 start transaction; update account set money=money+100 where name=\u0026#39;a\u0026#39;; update account set money=money-100 where name=\u0026#39;b\u0026#39;; //现在执行select * from account 可以看到转账成功，若此时退出数据库重新登录，会看到各账户余额没有改变，所以一定要用commit语句提交事务，否则会失败 //事务的回滚，别忘记设置为手动提交的模式 start transaction; update account set money=money-100 where name=\u0026#39;a\u0026#39;; update account set money=money+100 where name=\u0026#39;b\u0026#39;; //若此时a不想转账给b，可以使用事务的回滚 rollback; //事务的隔离级别 read uncommitted; read committed; repeatable read; serializable; 存储过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //创建查看student表的存储过程 //创建student表 create table student( id int not null primary key auto_increment, name varchar(4), grade float )ENGINE=InnoDB default character set utf8; delimiter // -- 将mysql的结束符设置为// create procedure Proc() begin select * from student; end // delimiter ; -- 将mysql的结束符设置为; call Proc(); -- 这样就可以调用该存储过程 //变量的使用,mysql中变量不用事前申明，在用的时候直接用“@变量名”使用就可以 set @number=100; -- 或set @num:=1; //定义条件和处理程序 //光标的使用 //1.声明光标 DECLARE * cursor_name* CURSOR FOR select_statement 2. 光标OPEN语句 OPEN cursor_name 3. 光标FETCH语句 FETCH cursor_name INTO var_name [, var_name] ... 4. 光标CLOSE语句 CLOSE cursor_name //流程控制的使用 不做介绍 调用存储过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //定义存储过程 delimiter // create procedure proc1(in name varchar(4),out num int) begin select count(*) into num from student where name=name; end// delimiter ; //调用存储过程 call proc1(\u0026#34;tom\u0026#34;,@num) -- 查找名为tom学生人数 //查看结果 select @num; //查看存储过程 show procedure status like \u0026#39;p%\u0026#39; \\G -- 获得以p开头的存储过程信息 //修改存储过程 alter {procedure|function} sp_name[characteristic...] //删除存储过程 drop procedure proc1; 视图 如何创建视图 查看、修改、更新、删除视图\n视图的基本操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 //在单表上创建视图,重新创建student表，插入数据 create table student( id int not null primary key auto_increment, name varchar(10) not null, math float, chinese float ); insert into student(name,math,chinese) values (\u0026#39;Crow1\u0026#39;,66,77), (\u0026#39;Crow2\u0026#39;,66,77), (\u0026#39;Crow3\u0026#39;,66,77); //开始创建视图 create view stu_view as select math,chinese,math+chinese from student; -- 下图可看出创建成功 //也可以创建自定义字段名称的视图 create view stu_view2(math,chin,sum) as select math,chinese,math+chinese from student; //在多表上创建视图，创建表stu_info，插入数据 create table stu_info( id int not null primary key auto_increment, class varchar(10) not null, addr varchar(100) ); insert into stu_info(class,addr) values (\u0026#39;1\u0026#39;,\u0026#39;anhui\u0026#39;), (\u0026#39;2\u0026#39;,\u0026#39;fujian\u0026#39;), (\u0026#39;3\u0026#39;,\u0026#39;guangdong\u0026#39;); //创建视图stu_class create view stu_class(id,name,class) as select student.id,student.name,stu_info.class from student,stu_info where student.id=stu_info.id; //查看视图 desc stu_class; show table status like \u0026#39;stu_class\u0026#39;\\G show create view stu_class\\G //修改视图 create or replace view stu_view as select * from student; alter view stu_view as select chinese from student; //更新视图 update stu_view set chinese=100; insert into student values(null,\u0026#39;haha\u0026#39;,100,100); delete from stu_view2 where math=100; //删除视图 drop view if exists stu_view2; ","date":"2024-03-22T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/mysql%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/8c8b581ab1cf8ea5e3aa00cfb897ea4e_hu4213249906190306138.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/mysql%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/","title":"MySQL简单命令"},{"content":"前言 在搭建个人博客时，遇到了各种阴间问题，为此我删了起码七个库，都是泪，归根到底就是两个问题，一是主题配置，二是远程连接\nhugo（静态网站生成器） Hugo是一个快速、灵活且功能强大的静态网站生成器。它使用Go语言开发，旨在帮助用户轻松创建和管理网站\nhugo中文文档\nhugo下载地址\n生成站点 使用Hugo快速生成站点，比如希望生成到 /path/to/site 路径：\nwin+r输入cmd调用命令行\nhugo new site /path/to/site\n这样就在 /path/to/site 目录里生成了初始站点，进去目录：\ncd /path/to/site\n站点目录结构：\narchetypes/:存放内容模板的目录 content/:：存放所有内容文件（如Markdown文件）的目录。 layouts/:存放布局文件的目录。 static/:存放静态资源（如图片、CSS、JavaScript等）的目录 config.toml(yaml):站点的配置文件。 在这个目录下，最主要的是对hugo.toml，themes/的配置。创建的所有.md文件都放置在content/文件下，所有文章的图片都放在static/下面\n创建文章 创建一个 about 页面\nhugo new about.md\nabout.md 自动生成到了 content/about.md ，打开 about.md 看下 \u0026mdash; title: Chinese Test description: 这是一个副标题 date: 2020-09-09 slug: test-chinese image: 文章封面图片 categories: - Test - 测试 \u0026mdash;\ntitle: 页面或文章的标题，这里是 \u0026ldquo;Chinese Test\u0026rdquo;。 description: 页面或文章的描述，这里是 \u0026ldquo;这是一个副标题\u0026rdquo;。 date: 文章的发布日期，这里是 \u0026ldquo;2020-09-09\u0026rdquo;。 slug: 文章的URL别名，这里是 \u0026ldquo;test-chinese\u0026rdquo;。 image: 文章的封面图片，这里是 \u0026ldquo;helena-hertz-wWZzXlDpMog-unsplash.jpg\u0026rdquo;。 categories: 文章的分类，这里有两个分类：\u0026ldquo;Test\u0026rdquo; 和 \u0026ldquo;测试\u0026rdquo;。 draft:草稿，建议直接设为false 正文内容\n创建第一篇文章，放到 post 目录，方便之后生成聚合页面\nhugo new post/first.md\n安装皮肤 一个好看的主题是必要的，但是完全自己去写就太麻烦了，hugo中有很多精美的主题，比较方便的操作是在Hugo项目目录里面使用Git命令来克隆themes：你需要提供主题的 Git 仓库 URL。通常，主题的仓库 URL 可以在 Hugo Themes 官方网站或主题的文档中找到。\ngit clone https://github.com/author/theme-name.git themes/theme-name\n或者直接下载主题的压缩包，将其解压到themes/文件夹下，这样的话，就需要你对hugo.toml文件进行一点过的修改。\n每个主题文件里面都有对应的配置教程和演示网站，比如stack在exampleSite文件夹里面\n配置hugo.toml 好多教程里面直接就说会生成config.yaml文件，但事实上新版的都是生成的hugo.toml，这不免让第一次配置的人感到迷惑，这两种的使用都是可以的，只是在语法结构上会有区别(类似于c++和python)，如果你想要完全自己手搓，按照你喜欢的语言就好，如果像我一样，只想点点鼠标，那就主要观察你下载的themes里面它使用的是什么，跟着用就行。\n配置文件可以理解为对这个项目的总的配置，比如修改网站的标题等\n使用主题前最好看一下主题相应的配置教程，不同的主题是不一样，主题就是别人写好的网站的模板，而你就是去套用别人的模板。\ntheme = \u0026ldquo;my-theme\u0026rdquo; my-theme是你下载的theme的主题名，要和你解压的文件名一致\n配置还有很多，可以结合网上资源或自己专业知识定制自己的网站，类似：\n1 2 3 4 5 6 7 sidebar: emoji: 🍔 subtitle: 所行皆坦途,所愿皆如期 avatar: enabled: true local: true src: img/avatar.jpg 生成网站 需要注意的是，虽然我们做了这么多，但是在这个结构下(在 my-site这个结构下)，我们网站的页面实际上是还没有生成的，要想生成静态网站页面，必须运行如下命令\nhugo\n命令运行后，在上文提到my-site这个结构下会产生一个public/文件夹，里面保存生成的静态页面，后面将其在GitHub上面布置，实际上就是将public/中的内容远程推送到Github仓库中后进行展示。\nhugo server\n可以在本地预览你生成的网站，点击链接就可以在你本地的电脑看到自己搭建的网页的，但是这只是完成了一半，你需要将其托管到github page上，别人才能通过网址找到你的网站，github page是最简单且免费的方法，当然便宜的东西是有缺陷的，如果以后自己想深入，就需要买个域名和服务器了\n推送到GitHub 首先在GitHub上创建一个仓库，名字最好和你自己的名字一样，其实也无所谓，但是好多人都这么做，但我不是\n在pubilc文件夹中创建仓库 记得cd 进入对应的文件夹\n其实远程连接在我的git博客上有详细的教程，这里我大概说一下\ngit init 初始化仓库 git add. 增加所有修改的意思 git commit -m \u0026ldquo;备注\u0026rdquo; 提交到远程仓库“备注随便写，你写我是傻逼都行”\n将两个仓库链接起来 这一步是最恶心我的，不知道为什么，不能用http链接，只能用ssh链接 大概意思就是你需要在你的电脑生成一个ssh链接，然后将其放到GitHub的配置上\n这步是参考别人的\n打开git bash命令窗口 生成ssh key ssh-keygen -t rsa -b 4096 -C \u0026ldquo;your_email@example.com\u0026rdquo; your_email@example.com为github上你注册的email地址。\n然后直接三个enter不管他\n上面默认生成在用户主目录的.ssh目录下，可以自己输入自定义位置\n把ssh key添加到github 复制文件c/Users/Administrator/.ssh/id_rsa.pub内容，把key添加到：github \u0026gt; settings \u0026gt; SSH and GPG keys \u0026gt; New SSH key \u0026gt; 粘贴保存。\n测试SSH连接 ssh -T git@github.com\n如果成功的话你就可以通过SSH方式来clone及提交代码了\ngit branch -M main // 创建一个分支\ngit remote add origin https://github.com/your-username/your-repo.git //使用 git remote add 命令将远程仓库添加到仓库配置中\ngit remote set-url origin git@github.com:your-username/your-repo.git //使用 SSH 连接后，可以使用 SSH URL 推送到 GitHub 仓库\ngit push -u origin main //推送文件\n后续提交 hugo //在站点中运行\ncd public/\ngit add . //提交文件\ngit commit -m \u0026ldquo;备注\u0026rdquo; //推送到远程：在ssh已连接的情况下\ngit push -u origin main\n","date":"2024-03-20T00:00:00Z","image":"https://a-b-ab.github.io/hugo-dev/p/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/1636c128aa9d846a9aa395599c438a041d4f56b41dff52-PmgBaF_hu14021243640980924465.jpg","permalink":"https://a-b-ab.github.io/hugo-dev/p/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","title":"个人博客搭建"}]